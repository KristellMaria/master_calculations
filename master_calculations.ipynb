{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fa28fec-d5d7-425f-80f7-b07158469676",
   "metadata": {},
   "source": [
    "### Does Bilingualism affect the Inhibition Deficit and the Aging effect on the elderly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a74d5-1e0e-417c-8446-ab5cbf4cef26",
   "metadata": {},
   "source": [
    "#### Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e04fc9-c0dc-438f-8d5e-9edc2d21775e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcc55234-81bf-4231-a8a4-c5ec996673e6",
   "metadata": {},
   "source": [
    "#### Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6669df-fbf8-44cb-9679-fc4aa71342a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9b09496-be7b-4efc-ba7b-a2dfc70066af",
   "metadata": {},
   "source": [
    "##### Pacages Imported to perform the different Statistical tests, analyses, and graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99e7ea39-73a2-4850-a3f4-a8ecfd36318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install package to open and read excel sheets\n",
    "from pathlib import Path\n",
    "#!pip install openpyxl\n",
    "\n",
    "# Installing package to perform Normality check (Shapiro-Wilk test)\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Installing package to perform other Statistical analysis like Kruskal-Wallis test, and hypothesis testing \n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Installing Scikit-posthocs package to perform post-hoc test\n",
    "#!pip install scikit-posthocs\n",
    "\n",
    "# Installing to perform specific post-hoc test, Dunn's post-hoc test.\n",
    "import scikit_posthocs as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Installing pingouin Library package to perform mixed-design ANOVA, and other Statistical analysis:\n",
    "#!pip install pingouin\n",
    "import pingouin as pg\n",
    "import itertools\n",
    "\n",
    "# Installing package to create some visuals like error-bars.\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce4ef9-449c-4554-8317-497258ab1978",
   "metadata": {},
   "source": [
    "##### Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dad669-4ce2-4c49-982c-7e524f8cffd3",
   "metadata": {},
   "source": [
    "In this pipeline we use mostly Pandas packaging. Pandas DataFrames offer a powerful and flexible structure for data manipulation and analysis. They enable efficient data cleaning, merging, reshaping, and summarization with concise, readable code—making complex data workflows accessible and reproducible, which is essential for robust scientific research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc1ef5-b9f4-4bdb-ba89-ed3531cd49d3",
   "metadata": {},
   "source": [
    "##### Calculation Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c6ade8-3b5c-4586-a90f-bb274a319b63",
   "metadata": {},
   "source": [
    "- Upload the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe9cf25-6d7b-4e49-8f9a-93128675c08d",
   "metadata": {},
   "source": [
    "1. Upload data from csv.\n",
    "   It was detected when we uploaded the data that the tables was showing separation by ; not ,\n",
    "   * had to define new sep as it was separated by ; not ,\n",
    "   * Data values were written with comma rather than dot. Therefore we needed to replace the commas with dots. Then python could automatically convert to float values we also used slicing to get the columns needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48955073-141a-4225-87ea-815816f0e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "473cda06-e8d3-499b-99be-8c832b2017c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your files (note the extension is now .csv)\n",
    "emo_df = pd.read_csv(Path(\"C:/github/master_calculations/data/emotional_wide.csv\"), sep=\";\", decimal=\",\")\n",
    "stroop_df = pd.read_csv(Path(\"C:/github/master_calculations/data/stroop_wide.csv\"), sep=\";\", decimal=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efed7090-655d-4231-8636-00616d62f95b",
   "metadata": {},
   "source": [
    "- Open files:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e21c00d-49f0-4c12-a32c-1efac2c7b109",
   "metadata": {},
   "source": [
    "Classical Stroop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48320315-0950-4ba8-915a-b232b5b4af01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>respc.corr_mean.incongruent</th>\n",
       "      <th>respc.corr_std.incongruent</th>\n",
       "      <th>respc.rt_mean.incongruent</th>\n",
       "      <th>respc.rt_std.incongruent</th>\n",
       "      <th>respc.corr_mean.congruent</th>\n",
       "      <th>respc.corr_std.congruent</th>\n",
       "      <th>respc.rt_mean.congruent</th>\n",
       "      <th>respc.rt_std.congruent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.715111</td>\n",
       "      <td>0.254540</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.074536</td>\n",
       "      <td>0.773110</td>\n",
       "      <td>0.257307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1204</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.606518</td>\n",
       "      <td>0.126010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.677501</td>\n",
       "      <td>0.173684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1205</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.995745</td>\n",
       "      <td>0.265899</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.074536</td>\n",
       "      <td>1.118971</td>\n",
       "      <td>0.252094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1206</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.091287</td>\n",
       "      <td>0.704573</td>\n",
       "      <td>0.155570</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.074536</td>\n",
       "      <td>0.816686</td>\n",
       "      <td>0.219771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1207</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.683636</td>\n",
       "      <td>0.224203</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.223607</td>\n",
       "      <td>0.797774</td>\n",
       "      <td>0.224723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1319</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.725578</td>\n",
       "      <td>0.237461</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.751577</td>\n",
       "      <td>0.212407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1326</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.149071</td>\n",
       "      <td>0.937385</td>\n",
       "      <td>0.420897</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.886707</td>\n",
       "      <td>0.254680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1337</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.074536</td>\n",
       "      <td>0.672102</td>\n",
       "      <td>0.261308</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.074536</td>\n",
       "      <td>0.654819</td>\n",
       "      <td>0.181993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1344</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.074536</td>\n",
       "      <td>1.105055</td>\n",
       "      <td>0.271146</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.240358</td>\n",
       "      <td>1.289605</td>\n",
       "      <td>0.350826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1345</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.074536</td>\n",
       "      <td>0.810246</td>\n",
       "      <td>0.194319</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.074536</td>\n",
       "      <td>1.032755</td>\n",
       "      <td>0.272284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    participant_id  respc.corr_mean.incongruent  respc.corr_std.incongruent  \\\n",
       "0             1203                     1.000000                    0.000000   \n",
       "1             1204                     1.000000                    0.000000   \n",
       "2             1205                     1.000000                    0.000000   \n",
       "3             1206                     0.933333                    0.091287   \n",
       "4             1207                     1.000000                    0.000000   \n",
       "..             ...                          ...                         ...   \n",
       "83            1319                     1.000000                    0.000000   \n",
       "84            1326                     0.933333                    0.149071   \n",
       "85            1337                     0.966667                    0.074536   \n",
       "86            1344                     0.966667                    0.074536   \n",
       "87            1345                     0.966667                    0.074536   \n",
       "\n",
       "    respc.rt_mean.incongruent  respc.rt_std.incongruent  \\\n",
       "0                    0.715111                  0.254540   \n",
       "1                    0.606518                  0.126010   \n",
       "2                    0.995745                  0.265899   \n",
       "3                    0.704573                  0.155570   \n",
       "4                    0.683636                  0.224203   \n",
       "..                        ...                       ...   \n",
       "83                   0.725578                  0.237461   \n",
       "84                   0.937385                  0.420897   \n",
       "85                   0.672102                  0.261308   \n",
       "86                   1.105055                  0.271146   \n",
       "87                   0.810246                  0.194319   \n",
       "\n",
       "    respc.corr_mean.congruent  respc.corr_std.congruent  \\\n",
       "0                    0.966667                  0.074536   \n",
       "1                    1.000000                  0.000000   \n",
       "2                    0.966667                  0.074536   \n",
       "3                    0.966667                  0.074536   \n",
       "4                    0.900000                  0.223607   \n",
       "..                        ...                       ...   \n",
       "83                   1.000000                  0.000000   \n",
       "84                   1.000000                  0.000000   \n",
       "85                   0.966667                  0.074536   \n",
       "86                   0.866667                  0.240358   \n",
       "87                   0.966667                  0.074536   \n",
       "\n",
       "    respc.rt_mean.congruent  respc.rt_std.congruent  \n",
       "0                  0.773110                0.257307  \n",
       "1                  0.677501                0.173684  \n",
       "2                  1.118971                0.252094  \n",
       "3                  0.816686                0.219771  \n",
       "4                  0.797774                0.224723  \n",
       "..                      ...                     ...  \n",
       "83                 0.751577                0.212407  \n",
       "84                 0.886707                0.254680  \n",
       "85                 0.654819                0.181993  \n",
       "86                 1.289605                0.350826  \n",
       "87                 1.032755                0.272284  \n",
       "\n",
       "[88 rows x 9 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stroop_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74b0be6-ebca-4e4b-a095-e8be26c12f7a",
   "metadata": {},
   "source": [
    "Emotional Stroop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38e42ce9-3e9f-44d9-845d-b27e2e216140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>resp.corr_mean.incongruent.glad.adult</th>\n",
       "      <th>resp.corr_std.incongruent.glad.adult</th>\n",
       "      <th>resp.rt_mean.incongruent.glad.adult</th>\n",
       "      <th>resp.rt_std.incongruent.glad.adult</th>\n",
       "      <th>resp.corr_mean.incongruent.trist.adult</th>\n",
       "      <th>resp.corr_std.incongruent.trist.adult</th>\n",
       "      <th>resp.rt_mean.incongruent.trist.adult</th>\n",
       "      <th>resp.rt_std.incongruent.trist.adult</th>\n",
       "      <th>resp.corr_mean.incongruent.neu.adult</th>\n",
       "      <th>...</th>\n",
       "      <th>resp.rt_mean.congruent.glad.baby</th>\n",
       "      <th>resp.rt_std.congruent.glad.baby</th>\n",
       "      <th>resp.corr_mean.congruent.trist.baby</th>\n",
       "      <th>resp.corr_std.congruent.trist.baby</th>\n",
       "      <th>resp.rt_mean.congruent.trist.baby</th>\n",
       "      <th>resp.rt_std.congruent.trist.baby</th>\n",
       "      <th>resp.corr_mean.congruent.neu.baby</th>\n",
       "      <th>resp.corr_std.congruent.neu.baby</th>\n",
       "      <th>resp.rt_mean.congruent.neu.baby</th>\n",
       "      <th>resp.rt_std.congruent.neu.baby</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.834702</td>\n",
       "      <td>0.143169</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.141421</td>\n",
       "      <td>0.925454</td>\n",
       "      <td>0.278187</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.854535</td>\n",
       "      <td>0.232593</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>0.924575</td>\n",
       "      <td>0.271926</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>1.000787</td>\n",
       "      <td>0.318872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1204</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.141421</td>\n",
       "      <td>0.631355</td>\n",
       "      <td>0.123766</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.651777</td>\n",
       "      <td>0.033001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704160</td>\n",
       "      <td>0.173085</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>0.581735</td>\n",
       "      <td>0.123698</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.580491</td>\n",
       "      <td>0.083795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1205</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.855834</td>\n",
       "      <td>0.194823</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.771844</td>\n",
       "      <td>0.109109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.850434</td>\n",
       "      <td>0.135730</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.117851</td>\n",
       "      <td>0.863746</td>\n",
       "      <td>0.138510</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>0.794245</td>\n",
       "      <td>0.176831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1206</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.734624</td>\n",
       "      <td>0.079265</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.852039</td>\n",
       "      <td>0.151549</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.820891</td>\n",
       "      <td>0.107333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.830719</td>\n",
       "      <td>0.146121</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.744530</td>\n",
       "      <td>0.085321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1207</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.936832</td>\n",
       "      <td>0.195431</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.804329</td>\n",
       "      <td>0.208366</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.935145</td>\n",
       "      <td>0.187571</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>0.885868</td>\n",
       "      <td>0.133503</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>0.743572</td>\n",
       "      <td>0.095836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1319</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.968130</td>\n",
       "      <td>0.235274</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.879301</td>\n",
       "      <td>0.067024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.073240</td>\n",
       "      <td>0.315628</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>1.108588</td>\n",
       "      <td>0.372499</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.797543</td>\n",
       "      <td>0.135086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1326</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.844349</td>\n",
       "      <td>0.111004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.978748</td>\n",
       "      <td>0.102694</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.852385</td>\n",
       "      <td>0.072896</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>0.949973</td>\n",
       "      <td>0.147841</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>0.858571</td>\n",
       "      <td>0.120270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1337</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.744503</td>\n",
       "      <td>0.144672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.839017</td>\n",
       "      <td>0.099145</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.760030</td>\n",
       "      <td>0.141914</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>0.851125</td>\n",
       "      <td>0.221673</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.845381</td>\n",
       "      <td>0.143963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1344</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.886098</td>\n",
       "      <td>0.155842</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.760499</td>\n",
       "      <td>0.151753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.824691</td>\n",
       "      <td>0.182646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.868993</td>\n",
       "      <td>0.095461</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.872308</td>\n",
       "      <td>0.172360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1345</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.961006</td>\n",
       "      <td>0.336347</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098052</td>\n",
       "      <td>0.225527</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.983500</td>\n",
       "      <td>0.274123</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>1.088604</td>\n",
       "      <td>0.220189</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.729422</td>\n",
       "      <td>0.147840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    participant_id  resp.corr_mean.incongruent.glad.adult  \\\n",
       "0             1203                                    1.0   \n",
       "1             1204                                    0.9   \n",
       "2             1205                                    1.0   \n",
       "3             1206                                    1.0   \n",
       "4             1207                                    1.0   \n",
       "..             ...                                    ...   \n",
       "83            1319                                    1.0   \n",
       "84            1326                                    1.0   \n",
       "85            1337                                    1.0   \n",
       "86            1344                                    1.0   \n",
       "87            1345                                    1.0   \n",
       "\n",
       "    resp.corr_std.incongruent.glad.adult  resp.rt_mean.incongruent.glad.adult  \\\n",
       "0                               0.000000                             0.834702   \n",
       "1                               0.141421                             0.631355   \n",
       "2                               0.000000                             0.855834   \n",
       "3                               0.000000                             0.734624   \n",
       "4                               0.000000                             0.936832   \n",
       "..                                   ...                                  ...   \n",
       "83                              0.000000                             0.968130   \n",
       "84                              0.000000                             0.844349   \n",
       "85                              0.000000                             0.744503   \n",
       "86                              0.000000                             0.886098   \n",
       "87                              0.000000                             0.961006   \n",
       "\n",
       "    resp.rt_std.incongruent.glad.adult  \\\n",
       "0                             0.143169   \n",
       "1                             0.123766   \n",
       "2                             0.194823   \n",
       "3                             0.079265   \n",
       "4                             0.195431   \n",
       "..                                 ...   \n",
       "83                            0.235274   \n",
       "84                            0.111004   \n",
       "85                            0.144672   \n",
       "86                            0.155842   \n",
       "87                            0.336347   \n",
       "\n",
       "    resp.corr_mean.incongruent.trist.adult  \\\n",
       "0                                      0.9   \n",
       "1                                      1.0   \n",
       "2                                      1.0   \n",
       "3                                      1.0   \n",
       "4                                      1.0   \n",
       "..                                     ...   \n",
       "83                                     1.0   \n",
       "84                                     1.0   \n",
       "85                                     1.0   \n",
       "86                                     1.0   \n",
       "87                                     1.0   \n",
       "\n",
       "    resp.corr_std.incongruent.trist.adult  \\\n",
       "0                                0.141421   \n",
       "1                                0.000000   \n",
       "2                                0.000000   \n",
       "3                                0.000000   \n",
       "4                                0.000000   \n",
       "..                                    ...   \n",
       "83                               0.000000   \n",
       "84                               0.000000   \n",
       "85                               0.000000   \n",
       "86                               0.000000   \n",
       "87                               0.000000   \n",
       "\n",
       "    resp.rt_mean.incongruent.trist.adult  resp.rt_std.incongruent.trist.adult  \\\n",
       "0                               0.925454                             0.278187   \n",
       "1                               0.651777                             0.033001   \n",
       "2                               0.771844                             0.109109   \n",
       "3                               0.852039                             0.151549   \n",
       "4                               0.804329                             0.208366   \n",
       "..                                   ...                                  ...   \n",
       "83                              0.879301                             0.067024   \n",
       "84                              0.978748                             0.102694   \n",
       "85                              0.839017                             0.099145   \n",
       "86                              0.760499                             0.151753   \n",
       "87                              1.098052                             0.225527   \n",
       "\n",
       "    resp.corr_mean.incongruent.neu.adult  ...  \\\n",
       "0                                    1.0  ...   \n",
       "1                                    1.0  ...   \n",
       "2                                    1.0  ...   \n",
       "3                                    1.0  ...   \n",
       "4                                    1.0  ...   \n",
       "..                                   ...  ...   \n",
       "83                                   1.0  ...   \n",
       "84                                   1.0  ...   \n",
       "85                                   1.0  ...   \n",
       "86                                   1.0  ...   \n",
       "87                                   1.0  ...   \n",
       "\n",
       "    resp.rt_mean.congruent.glad.baby  resp.rt_std.congruent.glad.baby  \\\n",
       "0                           0.854535                         0.232593   \n",
       "1                           0.704160                         0.173085   \n",
       "2                           0.850434                         0.135730   \n",
       "3                           0.820891                         0.107333   \n",
       "4                           0.935145                         0.187571   \n",
       "..                               ...                              ...   \n",
       "83                          1.073240                         0.315628   \n",
       "84                          0.852385                         0.072896   \n",
       "85                          0.760030                         0.141914   \n",
       "86                          0.824691                         0.182646   \n",
       "87                          0.983500                         0.274123   \n",
       "\n",
       "    resp.corr_mean.congruent.trist.baby  resp.corr_std.congruent.trist.baby  \\\n",
       "0                              0.958333                            0.058926   \n",
       "1                              0.958333                            0.058926   \n",
       "2                              0.916667                            0.117851   \n",
       "3                              1.000000                            0.000000   \n",
       "4                              0.958333                            0.058926   \n",
       "..                                  ...                                 ...   \n",
       "83                             0.875000                            0.176777   \n",
       "84                             0.958333                            0.058926   \n",
       "85                             0.958333                            0.058926   \n",
       "86                             1.000000                            0.000000   \n",
       "87                             0.958333                            0.058926   \n",
       "\n",
       "    resp.rt_mean.congruent.trist.baby  resp.rt_std.congruent.trist.baby  \\\n",
       "0                            0.924575                          0.271926   \n",
       "1                            0.581735                          0.123698   \n",
       "2                            0.863746                          0.138510   \n",
       "3                            0.830719                          0.146121   \n",
       "4                            0.885868                          0.133503   \n",
       "..                                ...                               ...   \n",
       "83                           1.108588                          0.372499   \n",
       "84                           0.949973                          0.147841   \n",
       "85                           0.851125                          0.221673   \n",
       "86                           0.868993                          0.095461   \n",
       "87                           1.088604                          0.220189   \n",
       "\n",
       "    resp.corr_mean.congruent.neu.baby  resp.corr_std.congruent.neu.baby  \\\n",
       "0                            0.958333                          0.058926   \n",
       "1                            1.000000                          0.000000   \n",
       "2                            0.958333                          0.058926   \n",
       "3                            1.000000                          0.000000   \n",
       "4                            0.875000                          0.058926   \n",
       "..                                ...                               ...   \n",
       "83                           1.000000                          0.000000   \n",
       "84                           0.958333                          0.058926   \n",
       "85                           1.000000                          0.000000   \n",
       "86                           1.000000                          0.000000   \n",
       "87                           1.000000                          0.000000   \n",
       "\n",
       "    resp.rt_mean.congruent.neu.baby  resp.rt_std.congruent.neu.baby  \n",
       "0                          1.000787                        0.318872  \n",
       "1                          0.580491                        0.083795  \n",
       "2                          0.794245                        0.176831  \n",
       "3                          0.744530                        0.085321  \n",
       "4                          0.743572                        0.095836  \n",
       "..                              ...                             ...  \n",
       "83                         0.797543                        0.135086  \n",
       "84                         0.858571                        0.120270  \n",
       "85                         0.845381                        0.143963  \n",
       "86                         0.872308                        0.172360  \n",
       "87                         0.729422                        0.147840  \n",
       "\n",
       "[88 rows x 49 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2cc0893-d7d9-46c4-9e0c-c6a5aa953b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "participant_id                              int64\n",
       "resp.corr_mean.incongruent.glad.adult     float64\n",
       "resp.corr_std.incongruent.glad.adult      float64\n",
       "resp.rt_mean.incongruent.glad.adult       float64\n",
       "resp.rt_std.incongruent.glad.adult        float64\n",
       "resp.corr_mean.incongruent.trist.adult    float64\n",
       "resp.corr_std.incongruent.trist.adult     float64\n",
       "resp.rt_mean.incongruent.trist.adult      float64\n",
       "resp.rt_std.incongruent.trist.adult       float64\n",
       "resp.corr_mean.incongruent.neu.adult      float64\n",
       "resp.corr_std.incongruent.neu.adult       float64\n",
       "resp.rt_mean.incongruent.neu.adult        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm types look right\n",
    "stroop_df.dtypes.head(12)\n",
    "emo_df.dtypes.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "128bab90-c1ab-479a-8ce6-fcef9923013c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "participant_id                           0\n",
       "resp.corr_mean.incongruent.glad.baby     0\n",
       "resp.rt_mean.incongruent.glad.baby       0\n",
       "resp.rt_std.incongruent.glad.baby        0\n",
       "resp.corr_mean.incongruent.trist.baby    0\n",
       "resp.corr_std.incongruent.trist.baby     0\n",
       "resp.rt_mean.incongruent.trist.baby      0\n",
       "resp.rt_std.incongruent.trist.baby       0\n",
       "resp.corr_mean.incongruent.neu.baby      0\n",
       "resp.corr_std.incongruent.neu.baby       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See if any NaNs appeared after conversion (e.g., non-numeric text)\n",
    "stroop_df.isna().sum().sort_values(ascending=False).head(10)\n",
    "emo_df.isna().sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "979e1a5c-0fcf-48a9-9aea-a0de54b7478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroop_df[\"participant_id\"] = stroop_df[\"participant_id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e63f222-e87b-444f-bdc0-2fd2d2cf2470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>resp.corr_mean.incongruent.glad.adult</th>\n",
       "      <th>resp.corr_std.incongruent.glad.adult</th>\n",
       "      <th>resp.rt_mean.incongruent.glad.adult</th>\n",
       "      <th>resp.rt_std.incongruent.glad.adult</th>\n",
       "      <th>resp.corr_mean.incongruent.trist.adult</th>\n",
       "      <th>resp.corr_std.incongruent.trist.adult</th>\n",
       "      <th>resp.rt_mean.incongruent.trist.adult</th>\n",
       "      <th>resp.rt_std.incongruent.trist.adult</th>\n",
       "      <th>resp.corr_mean.incongruent.neu.adult</th>\n",
       "      <th>...</th>\n",
       "      <th>resp.rt_mean.congruent.glad.baby</th>\n",
       "      <th>resp.rt_std.congruent.glad.baby</th>\n",
       "      <th>resp.corr_mean.congruent.trist.baby</th>\n",
       "      <th>resp.corr_std.congruent.trist.baby</th>\n",
       "      <th>resp.rt_mean.congruent.trist.baby</th>\n",
       "      <th>resp.rt_std.congruent.trist.baby</th>\n",
       "      <th>resp.corr_mean.congruent.neu.baby</th>\n",
       "      <th>resp.corr_std.congruent.neu.baby</th>\n",
       "      <th>resp.rt_mean.congruent.neu.baby</th>\n",
       "      <th>resp.rt_std.congruent.neu.baby</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1266.170455</td>\n",
       "      <td>0.965909</td>\n",
       "      <td>0.032141</td>\n",
       "      <td>0.839050</td>\n",
       "      <td>0.162535</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>0.038569</td>\n",
       "      <td>0.876550</td>\n",
       "      <td>0.181659</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.839282</td>\n",
       "      <td>0.170302</td>\n",
       "      <td>0.936080</td>\n",
       "      <td>0.040846</td>\n",
       "      <td>0.905969</td>\n",
       "      <td>0.200276</td>\n",
       "      <td>0.974432</td>\n",
       "      <td>0.020088</td>\n",
       "      <td>0.799207</td>\n",
       "      <td>0.145589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>41.831688</td>\n",
       "      <td>0.116349</td>\n",
       "      <td>0.073429</td>\n",
       "      <td>0.118615</td>\n",
       "      <td>0.087137</td>\n",
       "      <td>0.110570</td>\n",
       "      <td>0.085035</td>\n",
       "      <td>0.148745</td>\n",
       "      <td>0.116965</td>\n",
       "      <td>0.038428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124196</td>\n",
       "      <td>0.087086</td>\n",
       "      <td>0.176545</td>\n",
       "      <td>0.054607</td>\n",
       "      <td>0.138716</td>\n",
       "      <td>0.084498</td>\n",
       "      <td>0.096522</td>\n",
       "      <td>0.038827</td>\n",
       "      <td>0.124248</td>\n",
       "      <td>0.072272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1203.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.628502</td>\n",
       "      <td>0.036224</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575645</td>\n",
       "      <td>0.033001</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629927</td>\n",
       "      <td>0.062028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.581735</td>\n",
       "      <td>0.076558</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.580491</td>\n",
       "      <td>0.051065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1228.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755970</td>\n",
       "      <td>0.099702</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.781008</td>\n",
       "      <td>0.099415</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.749487</td>\n",
       "      <td>0.112040</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.814037</td>\n",
       "      <td>0.133147</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.715115</td>\n",
       "      <td>0.102361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1262.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.832294</td>\n",
       "      <td>0.143920</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.845678</td>\n",
       "      <td>0.150347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823834</td>\n",
       "      <td>0.148653</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.898520</td>\n",
       "      <td>0.188464</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.782664</td>\n",
       "      <td>0.130587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1301.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.898917</td>\n",
       "      <td>0.200575</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.952084</td>\n",
       "      <td>0.240777</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899481</td>\n",
       "      <td>0.201692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>0.991394</td>\n",
       "      <td>0.248895</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>0.863237</td>\n",
       "      <td>0.168149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1345.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.424264</td>\n",
       "      <td>1.159541</td>\n",
       "      <td>0.501599</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.424264</td>\n",
       "      <td>1.334006</td>\n",
       "      <td>0.780579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.258131</td>\n",
       "      <td>0.657062</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.235702</td>\n",
       "      <td>1.396446</td>\n",
       "      <td>0.487515</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.235702</td>\n",
       "      <td>1.241493</td>\n",
       "      <td>0.490676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       participant_id  resp.corr_mean.incongruent.glad.adult  \\\n",
       "count       88.000000                              88.000000   \n",
       "mean      1266.170455                               0.965909   \n",
       "std         41.831688                               0.116349   \n",
       "min       1203.000000                               0.000000   \n",
       "25%       1228.750000                               1.000000   \n",
       "50%       1262.500000                               1.000000   \n",
       "75%       1301.250000                               1.000000   \n",
       "max       1345.000000                               1.000000   \n",
       "\n",
       "       resp.corr_std.incongruent.glad.adult  \\\n",
       "count                             88.000000   \n",
       "mean                               0.032141   \n",
       "std                                0.073429   \n",
       "min                                0.000000   \n",
       "25%                                0.000000   \n",
       "50%                                0.000000   \n",
       "75%                                0.000000   \n",
       "max                                0.424264   \n",
       "\n",
       "       resp.rt_mean.incongruent.glad.adult  \\\n",
       "count                            88.000000   \n",
       "mean                              0.839050   \n",
       "std                               0.118615   \n",
       "min                               0.628502   \n",
       "25%                               0.755970   \n",
       "50%                               0.832294   \n",
       "75%                               0.898917   \n",
       "max                               1.159541   \n",
       "\n",
       "       resp.rt_std.incongruent.glad.adult  \\\n",
       "count                           88.000000   \n",
       "mean                             0.162535   \n",
       "std                              0.087137   \n",
       "min                              0.036224   \n",
       "25%                              0.099702   \n",
       "50%                              0.143920   \n",
       "75%                              0.200575   \n",
       "max                              0.501599   \n",
       "\n",
       "       resp.corr_mean.incongruent.trist.adult  \\\n",
       "count                               88.000000   \n",
       "mean                                 0.963636   \n",
       "std                                  0.110570   \n",
       "min                                  0.100000   \n",
       "25%                                  1.000000   \n",
       "50%                                  1.000000   \n",
       "75%                                  1.000000   \n",
       "max                                  1.000000   \n",
       "\n",
       "       resp.corr_std.incongruent.trist.adult  \\\n",
       "count                              88.000000   \n",
       "mean                                0.038569   \n",
       "std                                 0.085035   \n",
       "min                                 0.000000   \n",
       "25%                                 0.000000   \n",
       "50%                                 0.000000   \n",
       "75%                                 0.000000   \n",
       "max                                 0.424264   \n",
       "\n",
       "       resp.rt_mean.incongruent.trist.adult  \\\n",
       "count                             88.000000   \n",
       "mean                               0.876550   \n",
       "std                                0.148745   \n",
       "min                                0.575645   \n",
       "25%                                0.781008   \n",
       "50%                                0.845678   \n",
       "75%                                0.952084   \n",
       "max                                1.334006   \n",
       "\n",
       "       resp.rt_std.incongruent.trist.adult  \\\n",
       "count                            88.000000   \n",
       "mean                              0.181659   \n",
       "std                               0.116965   \n",
       "min                               0.033001   \n",
       "25%                               0.099415   \n",
       "50%                               0.150347   \n",
       "75%                               0.240777   \n",
       "max                               0.780579   \n",
       "\n",
       "       resp.corr_mean.incongruent.neu.adult  ...  \\\n",
       "count                             88.000000  ...   \n",
       "mean                               0.979167  ...   \n",
       "std                                0.038428  ...   \n",
       "min                                0.833333  ...   \n",
       "25%                                1.000000  ...   \n",
       "50%                                1.000000  ...   \n",
       "75%                                1.000000  ...   \n",
       "max                                1.000000  ...   \n",
       "\n",
       "       resp.rt_mean.congruent.glad.baby  resp.rt_std.congruent.glad.baby  \\\n",
       "count                         88.000000                        88.000000   \n",
       "mean                           0.839282                         0.170302   \n",
       "std                            0.124196                         0.087086   \n",
       "min                            0.629927                         0.062028   \n",
       "25%                            0.749487                         0.112040   \n",
       "50%                            0.823834                         0.148653   \n",
       "75%                            0.899481                         0.201692   \n",
       "max                            1.258131                         0.657062   \n",
       "\n",
       "       resp.corr_mean.congruent.trist.baby  \\\n",
       "count                            88.000000   \n",
       "mean                              0.936080   \n",
       "std                               0.176545   \n",
       "min                               0.000000   \n",
       "25%                               0.916667   \n",
       "50%                               1.000000   \n",
       "75%                               1.000000   \n",
       "max                               1.000000   \n",
       "\n",
       "       resp.corr_std.congruent.trist.baby  resp.rt_mean.congruent.trist.baby  \\\n",
       "count                           88.000000                          88.000000   \n",
       "mean                             0.040846                           0.905969   \n",
       "std                              0.054607                           0.138716   \n",
       "min                              0.000000                           0.581735   \n",
       "25%                              0.000000                           0.814037   \n",
       "50%                              0.000000                           0.898520   \n",
       "75%                              0.058926                           0.991394   \n",
       "max                              0.235702                           1.396446   \n",
       "\n",
       "       resp.rt_std.congruent.trist.baby  resp.corr_mean.congruent.neu.baby  \\\n",
       "count                         88.000000                          88.000000   \n",
       "mean                           0.200276                           0.974432   \n",
       "std                            0.084498                           0.096522   \n",
       "min                            0.076558                           0.125000   \n",
       "25%                            0.133147                           0.958333   \n",
       "50%                            0.188464                           1.000000   \n",
       "75%                            0.248895                           1.000000   \n",
       "max                            0.487515                           1.000000   \n",
       "\n",
       "       resp.corr_std.congruent.neu.baby  resp.rt_mean.congruent.neu.baby  \\\n",
       "count                         88.000000                        88.000000   \n",
       "mean                           0.020088                         0.799207   \n",
       "std                            0.038827                         0.124248   \n",
       "min                            0.000000                         0.580491   \n",
       "25%                            0.000000                         0.715115   \n",
       "50%                            0.000000                         0.782664   \n",
       "75%                            0.058926                         0.863237   \n",
       "max                            0.235702                         1.241493   \n",
       "\n",
       "       resp.rt_std.congruent.neu.baby  \n",
       "count                       88.000000  \n",
       "mean                         0.145589  \n",
       "std                          0.072272  \n",
       "min                          0.051065  \n",
       "25%                          0.102361  \n",
       "50%                          0.130587  \n",
       "75%                          0.168149  \n",
       "max                          0.490676  \n",
       "\n",
       "[8 rows x 49 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07980200-d825-4260-afec-c416306e6622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respc.corr_mean.incongruent</th>\n",
       "      <th>respc.corr_std.incongruent</th>\n",
       "      <th>respc.rt_mean.incongruent</th>\n",
       "      <th>respc.rt_std.incongruent</th>\n",
       "      <th>respc.corr_mean.congruent</th>\n",
       "      <th>respc.corr_std.congruent</th>\n",
       "      <th>respc.rt_mean.congruent</th>\n",
       "      <th>respc.rt_std.congruent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.976894</td>\n",
       "      <td>0.048384</td>\n",
       "      <td>0.853074</td>\n",
       "      <td>0.251353</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.090096</td>\n",
       "      <td>0.992118</td>\n",
       "      <td>0.310842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.035145</td>\n",
       "      <td>0.072897</td>\n",
       "      <td>0.212774</td>\n",
       "      <td>0.145106</td>\n",
       "      <td>0.152543</td>\n",
       "      <td>0.109454</td>\n",
       "      <td>0.291179</td>\n",
       "      <td>0.164527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.481819</td>\n",
       "      <td>0.070943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.596278</td>\n",
       "      <td>0.095487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.703588</td>\n",
       "      <td>0.154486</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.782203</td>\n",
       "      <td>0.207822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.786450</td>\n",
       "      <td>0.227217</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.074536</td>\n",
       "      <td>0.896302</td>\n",
       "      <td>0.255542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.074536</td>\n",
       "      <td>0.965770</td>\n",
       "      <td>0.297830</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.149071</td>\n",
       "      <td>1.177996</td>\n",
       "      <td>0.363376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.314894</td>\n",
       "      <td>1.598872</td>\n",
       "      <td>1.007927</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.514220</td>\n",
       "      <td>2.110163</td>\n",
       "      <td>1.002553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       respc.corr_mean.incongruent  respc.corr_std.incongruent  \\\n",
       "count                    88.000000                   88.000000   \n",
       "mean                      0.976894                    0.048384   \n",
       "std                       0.035145                    0.072897   \n",
       "min                       0.833333                    0.000000   \n",
       "25%                       0.966667                    0.000000   \n",
       "50%                       1.000000                    0.000000   \n",
       "75%                       1.000000                    0.074536   \n",
       "max                       1.000000                    0.314894   \n",
       "\n",
       "       respc.rt_mean.incongruent  respc.rt_std.incongruent  \\\n",
       "count                  88.000000                 88.000000   \n",
       "mean                    0.853074                  0.251353   \n",
       "std                     0.212774                  0.145106   \n",
       "min                     0.481819                  0.070943   \n",
       "25%                     0.703588                  0.154486   \n",
       "50%                     0.786450                  0.227217   \n",
       "75%                     0.965770                  0.297830   \n",
       "max                     1.598872                  1.007927   \n",
       "\n",
       "       respc.corr_mean.congruent  respc.corr_std.congruent  \\\n",
       "count                  88.000000                 88.000000   \n",
       "mean                    0.933333                  0.090096   \n",
       "std                     0.152543                  0.109454   \n",
       "min                     0.000000                  0.000000   \n",
       "25%                     0.933333                  0.000000   \n",
       "50%                     0.966667                  0.074536   \n",
       "75%                     1.000000                  0.149071   \n",
       "max                     1.000000                  0.514220   \n",
       "\n",
       "       respc.rt_mean.congruent  respc.rt_std.congruent  \n",
       "count                88.000000               88.000000  \n",
       "mean                  0.992118                0.310842  \n",
       "std                   0.291179                0.164527  \n",
       "min                   0.596278                0.095487  \n",
       "25%                   0.782203                0.207822  \n",
       "50%                   0.896302                0.255542  \n",
       "75%                   1.177996                0.363376  \n",
       "max                   2.110163                1.002553  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stroop_df.describe ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bffc9f63-2964-4728-a24f-a0b3776f9443",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"C:/github/master_calculations/data/p_details.xlsx\")  # sjekk at navnet og endelsen stemmer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cb7b58d-9c8b-4fa5-9903-a965ca6c052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "details_df = pd.read_excel(path, engine=\"openpyxl\", header=0, decimal=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d37bda17-2b57-43df-a4ff-15fa3d11078f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Session 1-Interview</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Session 2- EEG</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>Session 3- MRI (as applicable)</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>participant_ID</td>\n",
       "      <td>email</td>\n",
       "      <td>age</td>\n",
       "      <td>sex</td>\n",
       "      <td>Interview_date</td>\n",
       "      <td>Attended (y/n)</td>\n",
       "      <td>tester_initials</td>\n",
       "      <td>session_notes</td>\n",
       "      <td>EEG_date</td>\n",
       "      <td>Attended (y/n)</td>\n",
       "      <td>Tester Initials</td>\n",
       "      <td>cap_size</td>\n",
       "      <td>distance_from_screen_cm</td>\n",
       "      <td>MMSE_score</td>\n",
       "      <td>session_notes</td>\n",
       "      <td>MRI_date</td>\n",
       "      <td>Notes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>eirik.m.rosoy@gmail.com</td>\n",
       "      <td>49</td>\n",
       "      <td>m</td>\n",
       "      <td>2022-02-01 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>JR/VD</td>\n",
       "      <td>Q20: accent in French must be only moderate, n...</td>\n",
       "      <td>2022-02-01 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>GB7JR/VD</td>\n",
       "      <td>59</td>\n",
       "      <td>-</td>\n",
       "      <td>30</td>\n",
       "      <td>70 minutes ENC to RET</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>gro.kiil.larsen@gmail.com</td>\n",
       "      <td>45</td>\n",
       "      <td>f</td>\n",
       "      <td>2022-02-04 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>TV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-07 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>HT/SZ/TV</td>\n",
       "      <td>59</td>\n",
       "      <td>-</td>\n",
       "      <td>29</td>\n",
       "      <td>Isssue with triggers during ENC portion of epi...</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003</td>\n",
       "      <td>beathe.thomsen@tffk.no</td>\n",
       "      <td>48</td>\n",
       "      <td>f</td>\n",
       "      <td>2022-02-10 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>HT</td>\n",
       "      <td>Q20: accents should be: English - very strong,...</td>\n",
       "      <td>2022-02-17 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>HT/SZ</td>\n",
       "      <td>55</td>\n",
       "      <td>-</td>\n",
       "      <td>30</td>\n",
       "      <td>87 minutes ENC to RET Had issues with datamous...</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>sapmi77@yahoo.no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-17 00:00:00</td>\n",
       "      <td>n</td>\n",
       "      <td>JR</td>\n",
       "      <td>NO SHOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>1346</td>\n",
       "      <td>k-lien@live.no</td>\n",
       "      <td>44</td>\n",
       "      <td>m</td>\n",
       "      <td>2024-11-06 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>KE</td>\n",
       "      <td>noisy background (UiT common area), still student</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>1347</td>\n",
       "      <td>siwmb@online.no</td>\n",
       "      <td>56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-11-13 00:00:00</td>\n",
       "      <td>n</td>\n",
       "      <td>KE</td>\n",
       "      <td>rescheduled</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>1348</td>\n",
       "      <td>lenelias82@gmail.com</td>\n",
       "      <td>42</td>\n",
       "      <td>f</td>\n",
       "      <td>2024-11-14 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>KE</td>\n",
       "      <td>metal object in the hip (?)</td>\n",
       "      <td>26.11.24</td>\n",
       "      <td>Y</td>\n",
       "      <td>AG/FG</td>\n",
       "      <td>53</td>\n",
       "      <td>59</td>\n",
       "      <td>28</td>\n",
       "      <td>65min enc to ret</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>1349</td>\n",
       "      <td>torbjorn.nordmo@gmail.com</td>\n",
       "      <td>59</td>\n",
       "      <td>m</td>\n",
       "      <td>2024-12-03 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>KE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>1350</td>\n",
       "      <td>tove_kristine@hotmail.com</td>\n",
       "      <td>52</td>\n",
       "      <td>f</td>\n",
       "      <td>2024-12-05 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>KE</td>\n",
       "      <td>metallic object in the shoulder - also, someho...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>352 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0                 Unnamed: 1 Unnamed: 2 Unnamed: 3  \\\n",
       "0    participant_ID                      email        age        sex   \n",
       "1              1001    eirik.m.rosoy@gmail.com         49          m   \n",
       "2              1002  gro.kiil.larsen@gmail.com         45          f   \n",
       "3              1003     beathe.thomsen@tffk.no         48          f   \n",
       "4              1004           sapmi77@yahoo.no        NaN        NaN   \n",
       "..              ...                        ...        ...        ...   \n",
       "347            1346             k-lien@live.no         44          m   \n",
       "348            1347            siwmb@online.no         56        NaN   \n",
       "349            1348       lenelias82@gmail.com         42          f   \n",
       "350            1349  torbjorn.nordmo@gmail.com         59          m   \n",
       "351            1350  tove_kristine@hotmail.com         52          f   \n",
       "\n",
       "     Session 1-Interview      Unnamed: 5       Unnamed: 6  \\\n",
       "0         Interview_date  Attended (y/n)  tester_initials   \n",
       "1    2022-02-01 00:00:00               y            JR/VD   \n",
       "2    2022-02-04 00:00:00               y               TV   \n",
       "3    2022-02-10 00:00:00               y               HT   \n",
       "4    2022-02-17 00:00:00               n              JR    \n",
       "..                   ...             ...              ...   \n",
       "347  2024-11-06 00:00:00               y               KE   \n",
       "348  2024-11-13 00:00:00               n               KE   \n",
       "349  2024-11-14 00:00:00               y               KE   \n",
       "350  2024-12-03 00:00:00               y               KE   \n",
       "351  2024-12-05 00:00:00               y               KE   \n",
       "\n",
       "                                            Unnamed: 7      Session 2- EEG   \\\n",
       "0                                        session_notes             EEG_date   \n",
       "1    Q20: accent in French must be only moderate, n...  2022-02-01 00:00:00   \n",
       "2                                                  NaN  2022-02-07 00:00:00   \n",
       "3    Q20: accents should be: English - very strong,...  2022-02-17 00:00:00   \n",
       "4                                              NO SHOW                  NaN   \n",
       "..                                                 ...                  ...   \n",
       "347  noisy background (UiT common area), still student                  NaN   \n",
       "348                                        rescheduled                  NaN   \n",
       "349                        metal object in the hip (?)             26.11.24   \n",
       "350                                                NaN                  NaN   \n",
       "351  metallic object in the shoulder - also, someho...                  NaN   \n",
       "\n",
       "         Unnamed: 9      Unnamed: 10 Unnamed: 11              Unnamed: 12  \\\n",
       "0    Attended (y/n)  Tester Initials    cap_size  distance_from_screen_cm   \n",
       "1                 y         GB7JR/VD          59                        -   \n",
       "2                 y         HT/SZ/TV          59                        -   \n",
       "3                 y            HT/SZ          55                        -   \n",
       "4               NaN              NaN         NaN                      NaN   \n",
       "..              ...              ...         ...                      ...   \n",
       "347             NaN              NaN         NaN                      NaN   \n",
       "348             NaN              NaN         NaN                      NaN   \n",
       "349               Y            AG/FG          53                       59   \n",
       "350             NaN              NaN         NaN                      NaN   \n",
       "351             NaN              NaN         NaN                      NaN   \n",
       "\n",
       "    Unnamed: 13                                        Unnamed: 14  \\\n",
       "0    MMSE_score                                      session_notes   \n",
       "1            30                              70 minutes ENC to RET   \n",
       "2            29  Isssue with triggers during ENC portion of epi...   \n",
       "3            30  87 minutes ENC to RET Had issues with datamous...   \n",
       "4           NaN                                                NaN   \n",
       "..          ...                                                ...   \n",
       "347         NaN                                                NaN   \n",
       "348         NaN                                                NaN   \n",
       "349          28                                  65min enc to ret    \n",
       "350         NaN                                                NaN   \n",
       "351         NaN                                                NaN   \n",
       "\n",
       "    Session 3- MRI (as applicable) Unnamed: 16  \n",
       "0                         MRI_date       Notes  \n",
       "1                           TBC(?)         NaN  \n",
       "2                           TBC(?)         NaN  \n",
       "3                           TBC(?)         NaN  \n",
       "4                           TBC(?)         NaN  \n",
       "..                             ...         ...  \n",
       "347                            NaN         NaN  \n",
       "348                            NaN         NaN  \n",
       "349                            NaN         NaN  \n",
       "350                            NaN         NaN  \n",
       "351                            NaN         NaN  \n",
       "\n",
       "[352 rows x 17 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "181a6842-993e-40da-80a4-f6a64239dbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Session 1-Interview', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Session 2- EEG ', 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14', 'Session 3- MRI (as applicable)', 'Unnamed: 16']\n"
     ]
    }
   ],
   "source": [
    "print(details_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "deb758fc-a1fb-486a-b6c7-d098c5ff39b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Session 1-Interview</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Session 2- EEG</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>Session 3- MRI (as applicable)</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>participant_ID</td>\n",
       "      <td>email</td>\n",
       "      <td>age</td>\n",
       "      <td>sex</td>\n",
       "      <td>Interview_date</td>\n",
       "      <td>Attended (y/n)</td>\n",
       "      <td>tester_initials</td>\n",
       "      <td>session_notes</td>\n",
       "      <td>EEG_date</td>\n",
       "      <td>Attended (y/n)</td>\n",
       "      <td>Tester Initials</td>\n",
       "      <td>cap_size</td>\n",
       "      <td>distance_from_screen_cm</td>\n",
       "      <td>MMSE_score</td>\n",
       "      <td>session_notes</td>\n",
       "      <td>MRI_date</td>\n",
       "      <td>Notes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>eirik.m.rosoy@gmail.com</td>\n",
       "      <td>49</td>\n",
       "      <td>m</td>\n",
       "      <td>2022-02-01 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>JR/VD</td>\n",
       "      <td>Q20: accent in French must be only moderate, n...</td>\n",
       "      <td>2022-02-01 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>GB7JR/VD</td>\n",
       "      <td>59</td>\n",
       "      <td>-</td>\n",
       "      <td>30</td>\n",
       "      <td>70 minutes ENC to RET</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>gro.kiil.larsen@gmail.com</td>\n",
       "      <td>45</td>\n",
       "      <td>f</td>\n",
       "      <td>2022-02-04 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>TV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-07 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>HT/SZ/TV</td>\n",
       "      <td>59</td>\n",
       "      <td>-</td>\n",
       "      <td>29</td>\n",
       "      <td>Isssue with triggers during ENC portion of epi...</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003</td>\n",
       "      <td>beathe.thomsen@tffk.no</td>\n",
       "      <td>48</td>\n",
       "      <td>f</td>\n",
       "      <td>2022-02-10 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>HT</td>\n",
       "      <td>Q20: accents should be: English - very strong,...</td>\n",
       "      <td>2022-02-17 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>HT/SZ</td>\n",
       "      <td>55</td>\n",
       "      <td>-</td>\n",
       "      <td>30</td>\n",
       "      <td>87 minutes ENC to RET Had issues with datamous...</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>sapmi77@yahoo.no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-17 00:00:00</td>\n",
       "      <td>n</td>\n",
       "      <td>JR</td>\n",
       "      <td>NO SHOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1005</td>\n",
       "      <td>eline.holdo@gmail.com</td>\n",
       "      <td>46</td>\n",
       "      <td>f</td>\n",
       "      <td>2022-02-18 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>JR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-21 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>HT/JR</td>\n",
       "      <td>57</td>\n",
       "      <td>-</td>\n",
       "      <td>30</td>\n",
       "      <td>55 minutes ENC to RET</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1006</td>\n",
       "      <td>bjorg.hestvik@gmail.com</td>\n",
       "      <td>65</td>\n",
       "      <td>f</td>\n",
       "      <td>2022-02-21 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>TV/SZ</td>\n",
       "      <td>SZ Training session</td>\n",
       "      <td>2022-02-28 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>HT/JR</td>\n",
       "      <td>55</td>\n",
       "      <td>-</td>\n",
       "      <td>29</td>\n",
       "      <td>Stopped during practice ENC, because EEG was v...</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1007</td>\n",
       "      <td>stine@fam-barlindhaug.no</td>\n",
       "      <td>54</td>\n",
       "      <td>f</td>\n",
       "      <td>2022-02-22 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>TV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-05-18 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>HT/AK/MF</td>\n",
       "      <td>57</td>\n",
       "      <td>56</td>\n",
       "      <td>30</td>\n",
       "      <td>62 minutes ENC to RET</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1008</td>\n",
       "      <td>helge.matland@gmail.com</td>\n",
       "      <td>67</td>\n",
       "      <td>m</td>\n",
       "      <td>2022-02-22 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>JR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-25 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>HT/VD</td>\n",
       "      <td>59</td>\n",
       "      <td>-</td>\n",
       "      <td>29</td>\n",
       "      <td>59 minutes ENC to RET. Ppt said he forgot whic...</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1009</td>\n",
       "      <td>i-hemmin@online.no</td>\n",
       "      <td>56</td>\n",
       "      <td>f</td>\n",
       "      <td>2022-02-22 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>JR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-03-08 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>AG/VD</td>\n",
       "      <td>57</td>\n",
       "      <td>61</td>\n",
       "      <td>29</td>\n",
       "      <td>66 minutes from ENC to RET. P8 &amp; P10 might hav...</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                 Unnamed: 1 Unnamed: 2 Unnamed: 3  \\\n",
       "0  participant_ID                      email        age        sex   \n",
       "1            1001    eirik.m.rosoy@gmail.com         49          m   \n",
       "2            1002  gro.kiil.larsen@gmail.com         45          f   \n",
       "3            1003     beathe.thomsen@tffk.no         48          f   \n",
       "4            1004           sapmi77@yahoo.no        NaN        NaN   \n",
       "5            1005      eline.holdo@gmail.com         46          f   \n",
       "6            1006    bjorg.hestvik@gmail.com         65          f   \n",
       "7            1007   stine@fam-barlindhaug.no         54          f   \n",
       "8            1008    helge.matland@gmail.com         67          m   \n",
       "9            1009         i-hemmin@online.no         56          f   \n",
       "\n",
       "   Session 1-Interview      Unnamed: 5       Unnamed: 6  \\\n",
       "0       Interview_date  Attended (y/n)  tester_initials   \n",
       "1  2022-02-01 00:00:00               y            JR/VD   \n",
       "2  2022-02-04 00:00:00               y               TV   \n",
       "3  2022-02-10 00:00:00               y               HT   \n",
       "4  2022-02-17 00:00:00               n              JR    \n",
       "5  2022-02-18 00:00:00               y               JR   \n",
       "6  2022-02-21 00:00:00               y            TV/SZ   \n",
       "7  2022-02-22 00:00:00               y               TV   \n",
       "8  2022-02-22 00:00:00               y               JR   \n",
       "9  2022-02-22 00:00:00               y               JR   \n",
       "\n",
       "                                          Unnamed: 7      Session 2- EEG   \\\n",
       "0                                      session_notes             EEG_date   \n",
       "1  Q20: accent in French must be only moderate, n...  2022-02-01 00:00:00   \n",
       "2                                                NaN  2022-02-07 00:00:00   \n",
       "3  Q20: accents should be: English - very strong,...  2022-02-17 00:00:00   \n",
       "4                                            NO SHOW                  NaN   \n",
       "5                                                NaN  2022-02-21 00:00:00   \n",
       "6                                SZ Training session  2022-02-28 00:00:00   \n",
       "7                                                NaN  2022-05-18 00:00:00   \n",
       "8                                                NaN  2022-02-25 00:00:00   \n",
       "9                                                NaN  2022-03-08 00:00:00   \n",
       "\n",
       "       Unnamed: 9      Unnamed: 10 Unnamed: 11              Unnamed: 12  \\\n",
       "0  Attended (y/n)  Tester Initials    cap_size  distance_from_screen_cm   \n",
       "1               y         GB7JR/VD          59                        -   \n",
       "2               y         HT/SZ/TV          59                        -   \n",
       "3               y            HT/SZ          55                        -   \n",
       "4             NaN              NaN         NaN                      NaN   \n",
       "5               y            HT/JR          57                        -   \n",
       "6               y            HT/JR          55                        -   \n",
       "7               y         HT/AK/MF          57                       56   \n",
       "8               y            HT/VD          59                        -   \n",
       "9               y            AG/VD          57                       61   \n",
       "\n",
       "  Unnamed: 13                                        Unnamed: 14  \\\n",
       "0  MMSE_score                                      session_notes   \n",
       "1          30                              70 minutes ENC to RET   \n",
       "2          29  Isssue with triggers during ENC portion of epi...   \n",
       "3          30  87 minutes ENC to RET Had issues with datamous...   \n",
       "4         NaN                                                NaN   \n",
       "5          30                              55 minutes ENC to RET   \n",
       "6          29  Stopped during practice ENC, because EEG was v...   \n",
       "7          30                              62 minutes ENC to RET   \n",
       "8          29  59 minutes ENC to RET. Ppt said he forgot whic...   \n",
       "9          29  66 minutes from ENC to RET. P8 & P10 might hav...   \n",
       "\n",
       "  Session 3- MRI (as applicable) Unnamed: 16  \n",
       "0                       MRI_date       Notes  \n",
       "1                         TBC(?)         NaN  \n",
       "2                         TBC(?)         NaN  \n",
       "3                         TBC(?)         NaN  \n",
       "4                         TBC(?)         NaN  \n",
       "5                         TBC(?)         NaN  \n",
       "6                         TBC(?)         NaN  \n",
       "7                         TBC(?)         NaN  \n",
       "8                         TBC(?)         NaN  \n",
       "9                         TBC(?)         NaN  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3918470b-228e-4818-a765-f60d8a555e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "details_df = pd.read_excel(\n",
    "    path,\n",
    "    engine=\"openpyxl\",\n",
    "    header=1,   # Bruk rad 2 som kolonnenavn (0 = første rad)\n",
    "    decimal=\",\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d680e07-ef7d-4f98-b6c9-97b1e5d55a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   participant_ID  sex  age\n",
      "0            1001    m   49\n",
      "1            1002    f   45\n",
      "2            1003    f   48\n",
      "3            1004  NaN  NaN\n",
      "4            1005    f   46\n"
     ]
    }
   ],
   "source": [
    "demo_df = details_df[[\"participant_ID\", \"sex\", \"age\"]]\n",
    "print(demo_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a367a9f3-8fad-4cc8-a34c-e6cc7c2d994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging participant details and emotional dataframe, and participant details and stroop dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a26c8a56-a036-4681-bc70-093936ae5810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging participant details and stroop dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be8bc628-70b4-4837-82a3-cdce0886860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first standardize the ID in both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ef85c9d-75eb-44a3-9733-024d769be7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_id(s: pd.Series) -> pd.Series:\n",
    "    return (s.astype(str).str.replace(r\"\\.0$\", \"\", regex=True).str.strip())\n",
    "\n",
    "details_df[\"participant_id\"] = normalize_id(details_df[\"participant_ID\"])\n",
    "stroop_df[\"participant_id\"]  = normalize_id(stroop_df[\"participant_id\"])\n",
    "emo_df[\"participant_id\"]     = normalize_id(emo_df[\"participant_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "495091b1-2e76-4264-969a-62c19e487af0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'details_keep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m conflicts \u001b[38;5;241m=\u001b[39m (details_keep\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparticipant_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39magg(\n\u001b[0;32m      4\u001b[0m         n_age\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m,    \u001b[38;5;28;01mlambda\u001b[39;00m s: s\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mnunique()),\n\u001b[0;32m      5\u001b[0m         ages\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m,     \u001b[38;5;28;01mlambda\u001b[39;00m s: \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28msorted\u001b[39m(s\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39munique()))),\n\u001b[0;32m      6\u001b[0m         n_sex\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m,    \u001b[38;5;28;01mlambda\u001b[39;00m s: s\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mnunique()),\n\u001b[0;32m      7\u001b[0m         sexes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m,    \u001b[38;5;28;01mlambda\u001b[39;00m s: \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28msorted\u001b[39m(s\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39munique())))\n\u001b[0;32m      8\u001b[0m     )\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_age > 1 or n_sex > 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m conflicts\n",
      "\u001b[1;31mNameError\u001b[0m: name 'details_keep' is not defined"
     ]
    }
   ],
   "source": [
    "conflicts = (details_keep\n",
    "    .groupby(\"participant_id\")\n",
    "    .agg(\n",
    "        n_age=(\"age\",    lambda s: s.dropna().nunique()),\n",
    "        ages=(\"age\",     lambda s: tuple(sorted(s.dropna().unique()))),\n",
    "        n_sex=(\"sex\",    lambda s: s.dropna().nunique()),\n",
    "        sexes=(\"sex\",    lambda s: tuple(sorted(s.dropna().unique())))\n",
    "    )\n",
    "    .query(\"n_age > 1 or n_sex > 1\")\n",
    ")\n",
    "conflicts  # <- manually check these later (e.g., 1224: ages (32, 41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "85bcedaf-a28c-4c41-8c74-16486ace7a25",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'details_keep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s) \u001b[38;5;28;01melse\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mNA\n\u001b[1;32m----> 6\u001b[0m details_unique \u001b[38;5;241m=\u001b[39m (details_keep\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparticipant_id\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparticipant_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, as_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39magg(sex\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m, first_nonnull),\n\u001b[0;32m     10\u001b[0m          age\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m, first_nonnull))\n\u001b[0;32m     11\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'details_keep' is not defined"
     ]
    }
   ],
   "source": [
    "# First, de-duplicate using \"first non-null\" rule\n",
    "def first_nonnull(s):\n",
    "    s = s.dropna()\n",
    "    return s.iloc[0] if len(s) else pd.NA\n",
    "\n",
    "details_unique = (details_keep\n",
    "    .sort_values([\"participant_id\"])\n",
    "    .groupby(\"participant_id\", as_index=False)\n",
    "    .agg(sex=(\"sex\", first_nonnull),\n",
    "         age=(\"age\", first_nonnull))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2a995a54-c135-4633-af5b-749214e228dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'details_unique' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Then manually fix participant 1224\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m details_unique\u001b[38;5;241m.\u001b[39mloc[details_unique[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparticipant_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1224\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m41\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'details_unique' is not defined"
     ]
    }
   ],
   "source": [
    "# Then manually fix participant 1224\n",
    "details_unique.loc[details_unique[\"participant_id\"] == \"1224\", \"age\"] = 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e12b6c72-4203-4fcd-9253-7c0d11a99f5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'details_unique' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stroop_merged \u001b[38;5;241m=\u001b[39m stroop_df\u001b[38;5;241m.\u001b[39mmerge(details_unique, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparticipant_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m, validate\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm:1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m emo_merged    \u001b[38;5;241m=\u001b[39m emo_df\u001b[38;5;241m.\u001b[39mmerge(details_unique,    on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparticipant_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m, validate\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm:1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'details_unique' is not defined"
     ]
    }
   ],
   "source": [
    "stroop_merged = stroop_df.merge(details_unique, on=\"participant_id\", how=\"left\", validate=\"m:1\")\n",
    "emo_merged    = emo_df.merge(details_unique,    on=\"participant_id\", how=\"left\", validate=\"m:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "704ab209-7b4f-4408-bd80-c996c2fdb6d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stroop_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# check that everyone got matched\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStroop unmatched:\u001b[39m\u001b[38;5;124m\"\u001b[39m, stroop_merged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum())\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmo unmatched:   \u001b[39m\u001b[38;5;124m\"\u001b[39m, emo_merged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stroop_merged' is not defined"
     ]
    }
   ],
   "source": [
    "# check that everyone got matched\n",
    "print(\"Stroop unmatched:\", stroop_merged[\"sex\"].isna().sum())\n",
    "print(\"Emo unmatched:   \", emo_merged[\"sex\"].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "efb8bcf0-09a7-4a0c-8bf5-c00bdf26bffe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stroop_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stroop_unmatched \u001b[38;5;241m=\u001b[39m stroop_merged[stroop_merged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misna()][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparticipant_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m      2\u001b[0m emo_unmatched    \u001b[38;5;241m=\u001b[39m emo_merged[emo_merged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misna()][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparticipant_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnmatched in Stroop:\u001b[39m\u001b[38;5;124m\"\u001b[39m, stroop_unmatched)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stroop_merged' is not defined"
     ]
    }
   ],
   "source": [
    "stroop_unmatched = stroop_merged[stroop_merged[\"sex\"].isna()][\"participant_id\"].unique()\n",
    "emo_unmatched    = emo_merged[emo_merged[\"sex\"].isna()][\"participant_id\"].unique()\n",
    "\n",
    "print(\"Unmatched in Stroop:\", stroop_unmatched)\n",
    "print(\"Unmatched in Emo:   \", emo_unmatched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "52de2868-14a1-42d6-81f9-8f3dd247fe93",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'details_unique' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m details_unique\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparticipant_id == \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1284\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'details_unique' is not defined"
     ]
    }
   ],
   "source": [
    "details_unique.query(\"participant_id == '1284'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f9439b2a-2c90-45e6-95d7-acee6c51b4d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'details_unique' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fill in missing sex for participant 1284\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m details_unique\u001b[38;5;241m.\u001b[39mloc[details_unique[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparticipant_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1284\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'details_unique' is not defined"
     ]
    }
   ],
   "source": [
    "# Fill in missing sex for participant 1284\n",
    "details_unique.loc[details_unique[\"participant_id\"] == \"1284\", \"sex\"] = \"f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "75bcce09-4976-47e9-9497-71ffcadb36e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'details_unique' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Re-merge so the correction is included\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m stroop_merged \u001b[38;5;241m=\u001b[39m stroop_df\u001b[38;5;241m.\u001b[39mmerge(details_unique, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparticipant_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m, validate\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm:1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m emo_merged    \u001b[38;5;241m=\u001b[39m emo_df\u001b[38;5;241m.\u001b[39mmerge(details_unique,    on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparticipant_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m, validate\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm:1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'details_unique' is not defined"
     ]
    }
   ],
   "source": [
    "# Re-merge so the correction is included\n",
    "stroop_merged = stroop_df.merge(details_unique, on=\"participant_id\", how=\"left\", validate=\"m:1\")\n",
    "emo_merged    = emo_df.merge(details_unique,    on=\"participant_id\", how=\"left\", validate=\"m:1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "67461bbd-3036-4d02-9733-cd832ff00f0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stroop_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Double-check: no unmatched left\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStroop unmatched:\u001b[39m\u001b[38;5;124m\"\u001b[39m, stroop_merged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum())\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmo unmatched:   \u001b[39m\u001b[38;5;124m\"\u001b[39m, emo_merged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stroop_merged' is not defined"
     ]
    }
   ],
   "source": [
    "# Double-check: no unmatched left\n",
    "print(\"Stroop unmatched:\", stroop_merged[\"sex\"].isna().sum())\n",
    "print(\"Emo unmatched:   \", emo_merged[\"sex\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "940f0e18-6cf8-4fa4-b3e3-8a1583fdde46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a7a95630-ba73-4c82-a30a-4fc51767293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_demo(df, label):\n",
    "    \"\"\"Summarize demographics for a dataset\"\"\"\n",
    "    # drop duplicates in case multiple rows per participant\n",
    "    df_unique = df.drop_duplicates(subset=\"participant_id\")\n",
    "\n",
    "    n = len(df_unique)\n",
    "    sex_counts = df_unique[\"sex\"].value_counts(dropna=False).to_dict()\n",
    "    age_mean = df_unique[\"age\"].mean()\n",
    "    age_sd   = df_unique[\"age\"].std()\n",
    "    age_min  = df_unique[\"age\"].min()\n",
    "    age_max  = df_unique[\"age\"].max()\n",
    "\n",
    "    summary = {\n",
    "        \"Dataset\": label,\n",
    "        \"N\": n,\n",
    "        \"Sex counts\": sex_counts,\n",
    "        \"Age (mean ± SD)\": f\"{age_mean:.2f} ± {age_sd:.2f}\",\n",
    "        \"Age range\": f\"{age_min} – {age_max}\"\n",
    "    }\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dd7afc0e-3c48-4fe9-87ac-eeebcc7a2de1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'details_unique' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#We will make sure age is numeric to continue on to make some information tables\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# ensure age is numeric in the three dataframes we’ll summarize from\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _df \u001b[38;5;129;01min\u001b[39;00m (details_unique, stroop_merged, emo_merged):\n\u001b[0;32m      4\u001b[0m     _df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'details_unique' is not defined"
     ]
    }
   ],
   "source": [
    "#We will make sure age is numeric to continue on to make some information tables\n",
    "# ensure age is numeric in the three dataframes we’ll summarize from\n",
    "for _df in (details_unique, stroop_merged, emo_merged):\n",
    "    _df[\"age\"] = pd.to_numeric(_df[\"age\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9eac913b-57f1-4752-ac68-9850e587c914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_demo(df, label):\n",
    "    # one row per participant\n",
    "    cols = [c for c in [\"participant_id\", \"sex\", \"age\"] if c in df.columns]\n",
    "    dfu = df[cols].drop_duplicates(subset=\"participant_id\").copy()\n",
    "\n",
    "    # standardize sex strings a bit\n",
    "    dfu[\"sex\"] = dfu[\"sex\"].astype(str).str.strip().str.lower().replace({\"nan\": np.nan})\n",
    "\n",
    "    # numeric age (already coerced, but safe)\n",
    "    dfu[\"age\"] = pd.to_numeric(dfu[\"age\"], errors=\"coerce\")\n",
    "\n",
    "    # counts\n",
    "    n_total = len(dfu)\n",
    "    sex_counts = dfu[\"sex\"].value_counts(dropna=False).to_dict()\n",
    "\n",
    "    # age stats\n",
    "    age_mean = dfu[\"age\"].mean()\n",
    "    age_sd   = dfu[\"age\"].std()\n",
    "    age_min  = dfu[\"age\"].min()\n",
    "    age_max  = dfu[\"age\"].max()\n",
    "\n",
    "    return {\n",
    "        \"Dataset\": label,\n",
    "        \"N\": n_total,\n",
    "        \"Sex counts\": sex_counts,                 # e.g. {'f': 45, 'm': 43, nan: 0}\n",
    "        \"Age (mean ± SD)\": f\"{age_mean:.2f} ± {age_sd:.2f}\",\n",
    "        \"Age range\": f\"{int(age_min)} – {int(age_max)}\" if pd.notna(age_min) else \"—\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2791566a-10da-4baa-afd6-3814e1931ff6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stroop_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#building the three tables: one for stroop, one for emotional, and one where both are combined\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m stroop_demo   \u001b[38;5;241m=\u001b[39m summarize_demo(stroop_merged,   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassical Stroop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m emo_demo      \u001b[38;5;241m=\u001b[39m summarize_demo(emo_merged,      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmotional Stroop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m combined_demo \u001b[38;5;241m=\u001b[39m summarize_demo(details_unique,  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll Participants\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stroop_merged' is not defined"
     ]
    }
   ],
   "source": [
    "#building the three tables: one for stroop, one for emotional, and one where both are combined\n",
    "stroop_demo   = summarize_demo(stroop_merged,   \"Classical Stroop\")\n",
    "emo_demo      = summarize_demo(emo_merged,      \"Emotional Stroop\")\n",
    "combined_demo = summarize_demo(details_unique,  \"All Participants\")\n",
    "\n",
    "demo_table = pd.DataFrame([stroop_demo, emo_demo, combined_demo])\n",
    "print(demo_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e7a06c8e-cfd0-4eed-abb3-78096ee481bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'details_unique' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#lets check for the 'non-binary group'. We don't have any non-binary group. Let's fix this wil willing in manually sex for the ones that are in this category.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m details_unique[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts(dropna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'details_unique' is not defined"
     ]
    }
   ],
   "source": [
    "#lets check for the 'non-binary group'. We don't have any non-binary group. Let's fix this wil willing in manually sex for the ones that are in this category.\n",
    "details_unique[\"sex\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cc28963f-e71d-47a4-a70c-5cb9762f17e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'details_unique' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#lets see which participants are tagged this way\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 1) Make a normalized helper series (lowercase, trimmed; keep NaN as NaN)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m sex_norm \u001b[38;5;241m=\u001b[39m (details_unique[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      4\u001b[0m             \u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m      5\u001b[0m             \u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m      6\u001b[0m             \u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m      7\u001b[0m             \u001b[38;5;241m.\u001b[39mreplace({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan}))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'details_unique' is not defined"
     ]
    }
   ],
   "source": [
    "#lets see which participants are tagged this way\n",
    "# 1) Make a normalized helper series (lowercase, trimmed; keep NaN as NaN)\n",
    "sex_norm = (details_unique[\"sex\"]\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .str.lower()\n",
    "            .replace({\"nan\": np.nan}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c036387b-b978-4f6c-962f-73a2d6725158",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sex_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 2) See ALL unique values currently present (helps spot typos)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnique sex values:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msorted\u001b[39m(sex_norm\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39munique()))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sex_norm' is not defined"
     ]
    }
   ],
   "source": [
    "# 2) See ALL unique values currently present (helps spot typos)\n",
    "print(\"Unique sex values:\", sorted(sex_norm.dropna().unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "46b02e78-5082-4cb0-8701-22316521358b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sex_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 3) Find anything NOT in {'f','m'}  (potential typos / unwanted categories)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m weird_mask \u001b[38;5;241m=\u001b[39m sex_norm\u001b[38;5;241m.\u001b[39mnotna() \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m~\u001b[39msex_norm\u001b[38;5;241m.\u001b[39misin({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m      3\u001b[0m weird_rows \u001b[38;5;241m=\u001b[39m details_unique\u001b[38;5;241m.\u001b[39mloc[weird_mask, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparticipant_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-standard sex entries:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, weird_rows)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sex_norm' is not defined"
     ]
    }
   ],
   "source": [
    "# 3) Find anything NOT in {'f','m'}  (potential typos / unwanted categories)\n",
    "weird_mask = sex_norm.notna() & ~sex_norm.isin({\"f\", \"m\"})\n",
    "weird_rows = details_unique.loc[weird_mask, [\"participant_id\", \"sex\", \"age\"]]\n",
    "print(\"Non-standard sex entries:\\n\", weird_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96820a3d-a5c9-4812-9f85-2be07c79bfd7",
   "metadata": {},
   "source": [
    "So there is age missing, and sex missing. Let's check all the participants in two groups: the ones that are missing sex, and the ones that are missing age. Let's start with the ones missing age. Scan all these and then manually fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d693b92a-b657-43c5-aa93-3e84ac192043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find all participants with missing age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1198ad11-0b07-4caa-9694-22c18e568352",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'details_unique' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[108], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# make sure age is numeric so we can reliably find missing\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m details_unique[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(details_unique[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'details_unique' is not defined"
     ]
    }
   ],
   "source": [
    "# make sure age is numeric so we can reliably find missing\n",
    "details_unique[\"age\"] = pd.to_numeric(details_unique[\"age\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "40e365ab-ddb1-421f-9c5c-9d6a889b3a19",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'details_unique' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# list all with missing age\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m missing_age \u001b[38;5;241m=\u001b[39m (details_unique\n\u001b[0;32m      3\u001b[0m                \u001b[38;5;241m.\u001b[39mloc[details_unique[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misna(), [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparticipant_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m      4\u001b[0m                \u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparticipant_id\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParticipants with missing age: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(missing_age)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m missing_age\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m22\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'details_unique' is not defined"
     ]
    }
   ],
   "source": [
    "# list all with missing age\n",
    "missing_age = (details_unique\n",
    "               .loc[details_unique[\"age\"].isna(), [\"participant_id\", \"sex\", \"age\"]]\n",
    "               .sort_values(\"participant_id\"))\n",
    "print(f\"Participants with missing age: {len(missing_age)}\")\n",
    "missing_age.head(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f4fb9d3f-99ae-4280-971a-8540d09823a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checked the following participants age, and the following did not show up:\n",
    "    # \"1004\": no show,\n",
    "    # \"1010\": no show,\n",
    "    # \"1014\": no show,\n",
    "    # \"1017\": no show\n",
    "    # \"1041\": no show\n",
    "    # \"1044: no show, \n",
    "    # \"1048\": spurte alder\n",
    "    # \"1054\": no show\n",
    "    # \"1060\": excluded due to head trauma\n",
    "    # \"1065\": no show\n",
    "    # \"1068\": no show\n",
    "    # \"1072\": no show\n",
    "    # \"1073\": no show\n",
    "    # \"1079\": no show\n",
    "    # \"1080\": no show\n",
    "    # \"1082\": no show\n",
    "    # \"1086\": no show\n",
    "    # \"1093\": no show\n",
    "    # \"1094\": no show\n",
    "    # \"1107\": excluded due to hist. with antidepressants and other psychotripic medicines. Suffered a serious fall as well. \n",
    "    # \"1312\": no show\n",
    "    # \"1338\": no show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c56c811c-2e65-4593-a2b8-b6824111f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclusions: participant_id (as strings) -> reason\n",
    "exclusions = {\n",
    "    \"1004\": \"no show\",\n",
    "    \"1010\": \"no show\",\n",
    "    \"1014\": \"no show\",\n",
    "    \"1017\": \"no show\",\n",
    "    \"1041\": \"no show\",\n",
    "    \"1044\": \"no show\",  # <- you missed a quote earlier; fixed here\n",
    "    \"1048\": \"asked age / missing age\",\n",
    "    \"1054\": \"no show\",\n",
    "    \"1060\": \"head trauma\",\n",
    "    \"1065\": \"no show\",\n",
    "    \"1068\": \"no show\",\n",
    "    \"1072\": \"no show\",\n",
    "    \"1073\": \"no show\",\n",
    "    \"1079\": \"no show\",\n",
    "    \"1080\": \"no show\",\n",
    "    \"1082\": \"no show\",\n",
    "    \"1086\": \"no show\",\n",
    "    \"1093\": \"no show\",\n",
    "    \"1094\": \"no show\",\n",
    "    \"1107\": \"psychotropic meds + serious fall\",\n",
    "    \"1312\": \"no show\",\n",
    "    \"1338\": \"no show\",\n",
    "}\n",
    "exclude_ids = set(exclusions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c756aabd-9756-4770-bf4f-0d7f983051e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'details_unique' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Drop these participants from details + raw instrument data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# details_unique, stroop_df, emo_df should already exist and have normalized string IDs\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Remove from details\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m details_unique \u001b[38;5;241m=\u001b[39m details_unique[\u001b[38;5;241m~\u001b[39mdetails_unique[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparticipant_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misin(exclude_ids)]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'details_unique' is not defined"
     ]
    }
   ],
   "source": [
    "# Drop these participants from details + raw instrument data\n",
    "# details_unique, stroop_df, emo_df should already exist and have normalized string IDs\n",
    "# Remove from details\n",
    "details_unique = details_unique[~details_unique[\"participant_id\"].isin(exclude_ids)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e75584-23f5-4d01-b35c-5509a65102c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also remove from instrument data (in case any slipped in)\n",
    "stroop_df = stroop_df[~stroop_df[\"participant_id\"].isin(exclude_ids)].copy()\n",
    "emo_df    = emo_df[~emo_df[\"participant_id\"].isin(exclude_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5910f747-7693-4b31-aeb9-78c3f446e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-merge demographics to the instruments\n",
    "stroop_merged = stroop_df.merge(details_unique, on=\"participant_id\", how=\"left\", validate=\"m:1\")\n",
    "emo_merged    = emo_df.merge(details_unique,    on=\"participant_id\", how=\"left\", validate=\"m:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a216f2c2-c2b1-4f50-a7da-08166f73ce79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the data \n",
    "print(\"Remaining details participants:\", details_unique[\"participant_id\"].nunique())\n",
    "print(\"Unmatched Stroop (missing sex):\", stroop_merged[\"sex\"].isna().sum())\n",
    "print(\"Unmatched Emo (missing sex):   \", emo_merged[\"sex\"].isna().sum())\n",
    "\n",
    "# Verify none of the excluded IDs remain anywhere\n",
    "def any_left(df, name):\n",
    "    left = sorted(set(df[\"participant_id\"]) & exclude_ids)\n",
    "    print(f\"Excluded IDs still in {name}: {left}\")\n",
    "\n",
    "any_left(details_unique, \"details_unique\")\n",
    "any_left(stroop_df,     \"stroop_df\")\n",
    "any_left(emo_df,        \"emo_df\")\n",
    "any_left(stroop_merged, \"stroop_merged\")\n",
    "any_left(emo_merged,    \"emo_merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bac8a3-8df4-49cd-91a7-c4ec0554da18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-build the demographic table\n",
    "# ensure numeric age\n",
    "for _df in (details_unique, stroop_merged, emo_merged):\n",
    "    _df[\"age\"] = pd.to_numeric(_df[\"age\"], errors=\"coerce\")\n",
    "\n",
    "stroop_demo   = summarize_demo(stroop_merged,   \"Classical Stroop\")\n",
    "emo_demo      = summarize_demo(emo_merged,      \"Emotional Stroop\")\n",
    "combined_demo = summarize_demo(details_unique,  \"All Participants\")\n",
    "\n",
    "import pandas as pd\n",
    "demo_table = pd.DataFrame([stroop_demo, emo_demo, combined_demo])\n",
    "print(demo_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d69042b-e43c-4cd9-89f4-6abb9cc4dc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Got answer from the participant 1048, age 65 years old. SO now I have to re-merge the data and update the participants age. \n",
    "details_unique.loc[details_unique[\"participant_id\"] == \"1048\", \"age\"] = 65\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aacdcdb-176d-471c-9ba9-ad9dc20334b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-emerge so stroop and emotional stroop have the same data update of the participant\n",
    "stroop_merged = stroop_df.merge(details_unique, on=\"participant_id\", how=\"left\", validate=\"m:1\")\n",
    "emo_merged    = emo_df.merge(details_unique,    on=\"participant_id\", how=\"left\", validate=\"m:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428397d2-c617-4769-bd18-6b347b27f08a",
   "metadata": {},
   "source": [
    "update the raw file with the new participants update age. BUT first! find the missing sex participants and try to fill in sex before touching the raw datafile and merging everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa5221f-5f49-482b-9560-6a3e3cc76f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all missing sex-participants\n",
    "# normalize sex (so \"<na>\" and weird spellings don't slip through)\n",
    "sex_norm = (details_unique[\"sex\"].astype(str).str.strip().str.lower()\n",
    "            .replace({\"nan\": pd.NA, \"<na>\": pd.NA, \"female\":\"f\", \"male\":\"m\"}))\n",
    "\n",
    "missing_sex = details_unique.loc[sex_norm.isna(), [\"participant_id\",\"sex\",\"age\"]].sort_values(\"participant_id\")\n",
    "print(f\"Participants with missing sex: {len(missing_sex)}\")\n",
    "missing_sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821ae266-38f8-4973-9625-e625f769f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sex information of participants, manually checking:\n",
    "    # \"1103\": female,\n",
    "    # \"1143\": no show,\n",
    "    # \"1225\": excluded because of age 23,\n",
    "    # \"1233\": female, coded as 1223,\n",
    "    # \"1238\": no show,\n",
    "    # \"1239\": 51, but tested 2 years ago so not included in this sample,\n",
    "    # \"1275\": male,\n",
    "    # \"1247\": female, is 1192 so we can substitue these two,\n",
    "    # \"1288\": no show,\n",
    "    # \"1329\": male,\n",
    "    # \"1347\": female,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fafde8c-1792-4183-9b97-4c85fc597e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- apply sex corrections ---\n",
    "sex_corrections = {\n",
    "    \"1103\": \"f\",\n",
    "    \"1233\": \"f\",  \n",
    "    \"1275\": \"m\",\n",
    "    \"1329\": \"m\",\n",
    "    \"1347\": \"f\",\n",
    "    \"1275\": \"m\",\n",
    "    \"1247\": \"f\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316333e3-6ced-438f-9c0b-eca52644676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid, sex_val in sex_corrections.items():\n",
    "    details_unique.loc[details_unique[\"participant_id\"] == pid, \"sex\"] = sex_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1137ae-202e-4321-9a30-3f815c243b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize sex to f/m/NaN\n",
    "details_unique[\"sex\"] = (details_unique[\"sex\"].astype(str).str.strip().str.lower()\n",
    "                         .replace({\"female\":\"f\", \"male\":\"m\", \"nan\": pd.NA, \"<na>\": pd.NA}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b842c4-2c25-41ed-8912-3355c0129dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- exclusions (do NOT exclude 1247) ---\n",
    "extra_exclusions = {\"1143\", \"1225\", \"1238\", \"1239\", \"1288\"}\n",
    "details_unique = details_unique[~details_unique[\"participant_id\"].isin(extra_exclusions)].copy()\n",
    "stroop_df      = stroop_df[~stroop_df[\"participant_id\"].isin(extra_exclusions)].copy()\n",
    "emo_df         = emo_df[~emo_df[\"participant_id\"].isin(extra_exclusions)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dc5c6c-adc9-48b9-a9ae-9233222019f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-emerge demographocs to stroop and emo\n",
    "stroop_merged = stroop_df.merge(details_unique, on=\"participant_id\", how=\"left\", validate=\"m:1\")\n",
    "emo_merged    = emo_df.merge(details_unique,    on=\"participant_id\", how=\"left\", validate=\"m:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c039a8fe-6ddf-4ea4-bd4d-1fb40f211a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-build the three summaries again (emo, stroop, rawfile\n",
    "# ensure numeric age\n",
    "for _df in (details_unique, stroop_merged, emo_merged):\n",
    "    _df[\"age\"] = pd.to_numeric(_df[\"age\"], errors=\"coerce\")\n",
    "\n",
    "def summarize_demo(df, label):\n",
    "    cols = [c for c in [\"participant_id\",\"sex\",\"age\"] if c in df.columns]\n",
    "    dfu = df[cols].drop_duplicates(subset=\"participant_id\").copy()\n",
    "    dfu[\"sex\"] = dfu[\"sex\"].astype(str).str.strip().str.lower().replace({\"nan\": pd.NA})\n",
    "    n = len(dfu)\n",
    "    sex_counts = dfu[\"sex\"].value_counts(dropna=False).to_dict()\n",
    "    age_mean = dfu[\"age\"].mean(); age_sd = dfu[\"age\"].std()\n",
    "    age_min  = dfu[\"age\"].min();  age_max = dfu[\"age\"].max()\n",
    "    return {\n",
    "        \"Dataset\": label,\n",
    "        \"N\": n,\n",
    "        \"Sex counts\": sex_counts,\n",
    "        \"Age (mean ± SD)\": f\"{age_mean:.2f} ± {age_sd:.2f}\",\n",
    "        \"Age range\": f\"{int(age_min)} – {int(age_max)}\" if pd.notna(age_min) else \"—\",\n",
    "    }\n",
    "\n",
    "stroop_demo   = summarize_demo(stroop_merged,   \"Classical Stroop\")\n",
    "emo_demo      = summarize_demo(emo_merged,      \"Emotional Stroop\")\n",
    "combined_demo = summarize_demo(details_unique,  \"All Participants\")\n",
    "\n",
    "demo_table = pd.DataFrame([stroop_demo, emo_demo, combined_demo])\n",
    "print(demo_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761a3080-8165-4451-b25c-8c7c56dbd1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing sex (details):\", details_unique[\"sex\"].isna().sum())\n",
    "print(\"Unmatched Stroop (sex NaN):\", stroop_merged[\"sex\"].isna().sum())\n",
    "print(\"Unmatched Emo (sex NaN):   \", emo_merged[\"sex\"].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bb0d9b-0957-48a8-b562-d07b7b59bd80",
   "metadata": {},
   "source": [
    "finding a non-binary participant. Which is correct according to the dataset a participant did selflabel as binary. But we will have to exclude this participant due to missing number on this specific group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d43701d-7b80-4c25-8c21-7d39240e6144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#locate the non-binary\n",
    "mask_nb = details_unique[\"sex\"].astype(str).str.strip().str.lower() == \"non-binary\"\n",
    "details_unique.loc[mask_nb, [\"participant_id\",\"sex\",\"age\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da19736b-a361-4ddb-89f7-7a3e40b23db4",
   "metadata": {},
   "source": [
    "This information is correct with our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3eb562-309e-4f14-bf96-74a1577b1098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# participant to exclude\n",
    "exclude_nonbinary = [\"1216\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6430c58a-3ad3-4e9c-bd16-170b85b4c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# remove from cleaned demographics\n",
    "details_unique = details_unique[~details_unique[\"participant_id\"].isin(exclude_nonbinary)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded4798e-1acb-4bb2-8686-75d6799cc227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove from raw (if you want consistency)\n",
    "if \"participant_id\" not in details_df.columns:\n",
    "    def normalize_id(s):\n",
    "        return (s.astype(str).str.replace(r\"\\.0$\",\"\",regex=True).str.strip())\n",
    "    details_df[\"participant_id\"] = normalize_id(details_df[\"participant_ID\"])\n",
    "\n",
    "details_df = details_df[~details_df[\"participant_id\"].isin(exclude_nonbinary)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815a3349-76a4-4e7d-9330-d718c952c84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove from instrument data\n",
    "stroop_df = stroop_df[~stroop_df[\"participant_id\"].isin(exclude_nonbinary)].copy()\n",
    "emo_df    = emo_df[~emo_df[\"participant_id\"].isin(exclude_nonbinary)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d48771-50ff-42d3-9c26-92024028d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroop_merged = stroop_df.merge(details_unique, on=\"participant_id\", how=\"left\", validate=\"m:1\")\n",
    "emo_merged    = emo_df.merge(details_unique,    on=\"participant_id\", how=\"left\", validate=\"m:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab12fe7-fb25-4b4e-ad4e-9096ff76fb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Participants left in details:\", details_unique[\"participant_id\"].nunique())\n",
    "print(\"Any 1216 left in details?\", \"1216\" in details_unique[\"participant_id\"].values)\n",
    "print(\"Any 1216 left in Stroop?\", \"1216\" in stroop_df[\"participant_id\"].values)\n",
    "print(\"Any 1216 left in Emo?\", \"1216\" in emo_df[\"participant_id\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a3c24-ba12-4c5f-a70f-9890a13f9443",
   "metadata": {},
   "source": [
    "Those lines are the checks we wrote to confirm that participant 1216 was successfully excluded. \n",
    "Any 1216 left in details? \n",
    "\n",
    "False → means 1216 is not present in details_unique.\n",
    "\n",
    "Any 1216 left in Stroop? False → means 1216 is not present in your Stroop dataset.\n",
    "\n",
    "Any 1216 left in Emo? False → means 1216 is not present in your Emotional Stroop dataset.\n",
    "\n",
    "So all three False values are good news: they confirm that participant 1216 was fully removed from every dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa584bb4-d6f5-48fd-9d49-9e79013e5c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Ensure age is numeric and sex normalized (f/m/NaN only) ---\n",
    "for _df in (details_unique, stroop_merged, emo_merged):\n",
    "    _df[\"age\"] = pd.to_numeric(_df[\"age\"], errors=\"coerce\")\n",
    "    _df[\"sex\"] = (_df[\"sex\"].astype(str).str.strip().str.lower()\n",
    "                  .replace({\"female\": \"f\", \"male\": \"m\", \"nan\": np.nan, \"<na>\": np.nan}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a170c61b-eee0-445d-af1f-43fe8d65c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) Helper to summarize demographics ---\n",
    "def summarize_demo(df, label):\n",
    "    cols = [c for c in [\"participant_id\",\"sex\",\"age\"] if c in df.columns]\n",
    "    dfu = df[cols].drop_duplicates(subset=\"participant_id\").copy()\n",
    "\n",
    "    n = len(dfu)\n",
    "    sex_counts = dfu[\"sex\"].value_counts(dropna=False).to_dict()\n",
    "    age_mean = dfu[\"age\"].mean(); age_sd = dfu[\"age\"].std()\n",
    "    age_min  = dfu[\"age\"].min();  age_max = dfu[\"age\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37711f1f-60ca-4b77-9173-9d9b249241e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_demo(df, label):\n",
    "    cols = [c for c in [\"participant_id\", \"sex\", \"age\"] if c in df.columns]\n",
    "    dfu = df[cols].drop_duplicates(subset=\"participant_id\").copy()\n",
    "\n",
    "    n = len(dfu)\n",
    "    sex_counts = dfu[\"sex\"].value_counts(dropna=False).to_dict()\n",
    "    age_mean = dfu[\"age\"].mean()\n",
    "    age_sd   = dfu[\"age\"].std()\n",
    "    age_min  = dfu[\"age\"].min()\n",
    "    age_max  = dfu[\"age\"].max()\n",
    "\n",
    "    # Pretty-print sex counts: collapse NaN label to '<na>' if present\n",
    "    if any(pd.isna(k) for k in sex_counts.keys()):\n",
    "        sex_counts = {\n",
    "            (\"f\" if k == \"f\" else \"m\" if k == \"m\" else \"<na>\"): v\n",
    "            for k, v in sex_counts.items()\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"Dataset\": label,\n",
    "        \"N\": n,\n",
    "        \"Sex counts\": sex_counts,\n",
    "        \"Age (mean ± SD)\": f\"{age_mean:.2f} ± {age_sd:.2f}\" if n else \"—\",\n",
    "        \"Age range\": f\"{int(age_min)} – {int(age_max)}\" if pd.notna(age_min) else \"—\",\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a59e88-45e1-4e75-a85f-e71aae2df207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) Build summaries ---\n",
    "stroop_demo   = summarize_demo(stroop_merged,   \"Classical Stroop\")\n",
    "emo_demo      = summarize_demo(emo_merged,      \"Emotional Stroop\")\n",
    "combined_demo = summarize_demo(details_unique,  \"All Participants\")\n",
    "\n",
    "demo_table = pd.DataFrame([stroop_demo, emo_demo, combined_demo])\n",
    "print(demo_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369fbb56-fcfa-4bb9-a08c-2beab4a02681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔎 Participants missing sex\n",
    "missing_sex = details_unique.loc[details_unique[\"sex\"].isna(), [\"participant_id\", \"age\"]].sort_values(\"participant_id\")\n",
    "print(\"Participants missing sex:\", len(missing_sex))\n",
    "display(missing_sex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a8c8eb-b39f-4391-8bdf-5a4d1482c020",
   "metadata": {},
   "source": [
    "We will tru to check all the data and the steps in one sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed8953a-6e72-4651-a199-205c7f800d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ONE-CELL REPAIR: tidy up details, apply your real exclusions/corrections, re-merge, and report ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- helpers & config ----\n",
    "def normalize_id(s):\n",
    "    return (s.astype(str).str.replace(r\"\\.0$\", \"\", regex=True).str.strip())\n",
    "\n",
    "def first_nonnull(s):\n",
    "    s = s.dropna()\n",
    "    return s.iloc[0] if len(s) else pd.NA\n",
    "\n",
    "EXCLUDE_IDS = {\n",
    "    # original big list + final decisions (incl. 1216 excluded)\n",
    "    \"1004\",\"1010\",\"1014\",\"1017\",\"1041\",\"1044\",\"1054\",\"1060\",\"1065\",\"1068\",\n",
    "    \"1072\",\"1073\",\"1079\",\"1080\",\"1082\",\"1086\",\"1093\",\"1094\",\"1107\",\"1143\",\n",
    "    \"1225\",\"1238\",\"1239\",\"1288\",\"1312\",\"1338\",\"1216\"\n",
    "}\n",
    "\n",
    "SEX_FIX = {\n",
    "    \"1103\": \"f\",\n",
    "    \"1233\": \"f\",  # corrected from 1223\n",
    "    \"1275\": \"m\",\n",
    "    \"1329\": \"m\",\n",
    "    \"1347\": \"f\",\n",
    "    \"1284\": \"f\",  # you confirmed earlier\n",
    "}\n",
    "\n",
    "# ---- choose a source for details ----\n",
    "if 'details_df' in globals():\n",
    "    src = details_df.copy()\n",
    "elif 'details_unique' in globals():\n",
    "    # fall back to whatever cleaned version you had\n",
    "    src = details_unique.copy()\n",
    "else:\n",
    "    raise NameError(\"I don't see details_df or details_unique in memory. Please run the cell that loads details first.\")\n",
    "\n",
    "# ensure participant_id exists\n",
    "if \"participant_id\" not in src.columns and \"participant_ID\" in src.columns:\n",
    "    src[\"participant_id\"] = normalize_id(src[\"participant_ID\"])\n",
    "elif \"participant_id\" in src.columns:\n",
    "    src[\"participant_id\"] = normalize_id(src[\"participant_id\"])\n",
    "else:\n",
    "    raise KeyError(\"Couldn't find a participant id column (participant_id / participant_ID) in details.\")\n",
    "\n",
    "# keep only needed cols if present\n",
    "keep_cols = [\"participant_id\"] + [c for c in [\"sex\",\"age\"] if c in src.columns]\n",
    "src = src[keep_cols].copy()\n",
    "\n",
    "# normalize fields\n",
    "src[\"sex\"] = (src.get(\"sex\")\n",
    "              .astype(str)\n",
    "              .str.strip().str.lower()\n",
    "              .replace({\"female\":\"f\",\"male\":\"m\",\"nan\":np.nan,\"<na>\":np.nan}))\n",
    "src[\"age\"] = pd.to_numeric(src.get(\"age\"), errors=\"coerce\")\n",
    "\n",
    "# deduplicate to one row per participant (first non-null rule)\n",
    "details_unique = (src.sort_values(\"participant_id\")\n",
    "                    .groupby(\"participant_id\", as_index=False)\n",
    "                    .agg(sex=(\"sex\", first_nonnull),\n",
    "                         age=(\"age\", first_nonnull)))\n",
    "\n",
    "# apply corrections\n",
    "for pid, val in SEX_FIX.items():\n",
    "    details_unique.loc[details_unique[\"participant_id\"] == pid, \"sex\"] = val\n",
    "\n",
    "# exclude participants\n",
    "before_n = details_unique[\"participant_id\"].nunique()\n",
    "details_unique = details_unique[~details_unique[\"participant_id\"].isin(EXCLUDE_IDS)].copy()\n",
    "after_n = details_unique[\"participant_id\"].nunique()\n",
    "\n",
    "# final normalize\n",
    "details_unique[\"sex\"] = (details_unique[\"sex\"].astype(str).str.strip().str.lower()\n",
    "                         .replace({\"female\":\"f\",\"male\":\"m\",\"nan\":np.nan,\"<na>\":np.nan}))\n",
    "details_unique[\"age\"] = pd.to_numeric(details_unique[\"age\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"[details] Kept {after_n} participants (removed {before_n - after_n}).\")\n",
    "\n",
    "# ---- re-merge into stroop/emo if they exist ----\n",
    "def merge_if_present(task_df, name):\n",
    "    if name in globals():\n",
    "        df = globals()[name].copy()\n",
    "        if \"participant_id\" in df.columns:\n",
    "            df[\"participant_id\"] = normalize_id(df[\"participant_id\"])\n",
    "            # drop excluded IDs from task DF too\n",
    "            df = df[~df[\"participant_id\"].isin(EXCLUDE_IDS)].copy()\n",
    "            merged = df.merge(details_unique, on=\"participant_id\", how=\"left\", validate=\"m:1\")\n",
    "            globals()[f\"{name}_merged\"] = merged\n",
    "            print(f\"[merge] {name}: rows={merged.shape[0]} | unmatched sex={merged['sex'].isna().sum()} | unmatched age={merged['age'].isna().sum()}\")\n",
    "        else:\n",
    "            print(f\"[merge] Skipped {name}: no 'participant_id' column.\")\n",
    "    else:\n",
    "        print(f\"[merge] {name} not found in memory; skipping.\")\n",
    "\n",
    "merge_if_present(None, \"stroop_df\")\n",
    "merge_if_present(None, \"emo_df\")\n",
    "\n",
    "# ---- final checks on details ----\n",
    "missing_sex = details_unique.loc[details_unique[\"sex\"].isna(), [\"participant_id\",\"age\"]].sort_values(\"participant_id\")\n",
    "missing_age = details_unique.loc[details_unique[\"age\"].isna(), [\"participant_id\",\"sex\"]].sort_values(\"participant_id\")\n",
    "missing_both = details_unique.loc[details_unique[\"sex\"].isna() & details_unique[\"age\"].isna(), [\"participant_id\"]]\n",
    "\n",
    "print(f\"[checks] Missing sex in details: {len(missing_sex)}\")\n",
    "print(f\"[checks] Missing age in details: {len(missing_age)}\")\n",
    "print(f\"[checks] Missing BOTH in details: {len(missing_both)}\")\n",
    "\n",
    "# display lists (comment out if not needed)\n",
    "display(missing_sex.head(50))\n",
    "display(missing_age.head(50))\n",
    "display(missing_both)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2120a1c3-4cd7-4035-b552-6fcc3e308a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# --- point to your file ---\n",
    "path = Path(\"C:/github/master_calculations/data/p_details.xlsx\")\n",
    "\n",
    "# --- try header row 0, else 1 (your file has sometimes needed header=1) ---\n",
    "tmp0 = pd.read_excel(path, engine=\"openpyxl\", header=0)\n",
    "if any(\"participant\" in str(c).lower() for c in tmp0.columns):\n",
    "    details_df = tmp0.copy()\n",
    "else:\n",
    "    details_df = pd.read_excel(path, engine=\"openpyxl\", header=1)\n",
    "\n",
    "# --- find the participant id column and standardize its name ---\n",
    "id_candidates = [c for c in details_df.columns\n",
    "                 if \"participant\" in str(c).lower() and \"id\" in str(c).lower()]\n",
    "if not id_candidates:\n",
    "    raise KeyError(\"Could not find a participant ID column in details_df.\")\n",
    "pid_col = id_candidates[0]\n",
    "details_df = details_df.rename(columns={pid_col: \"participant_ID\"})\n",
    "\n",
    "print(\"Loaded details_df with columns:\", list(details_df.columns)[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "63440a0d-6a8e-45ae-b9ca-75dd24a447e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FILE CHECKS ===\n",
      "C:\\github\\master_calculations\\data\\p_details.xlsx  ->  OK\n",
      "C:\\github\\master_calculations\\data\\stroop_wide.csv  ->  OK\n",
      "C:\\github\\master_calculations\\data\\emotional_wide.csv  ->  OK\n",
      "\n",
      "=== DATAFRAMES IN MEMORY ===\n",
      "details_df: present\n",
      "details_unique: absent\n",
      "stroop_df: present\n",
      "emo_df: present\n",
      "stroop_merged: absent\n",
      "emo_merged: absent\n",
      "\n",
      "[details_df] type=DataFrame shape=(351, 18)\n",
      "  columns (18): ['participant_ID', 'email', 'age', 'sex', 'Interview_date', 'Attended (y/n)', 'tester_initials', 'session_notes', 'EEG_date', 'Attended (y/n).1', 'Tester Initials', 'cap_size'] ...\n",
      "  ✔ 'participant_id' present | nulls=0 | unique IDs=350\n",
      "  sex: dtype=object | nulls=22\n",
      "  age: dtype=object | nulls=19\n",
      "\n",
      "[stroop_df] type=DataFrame shape=(88, 9)\n",
      "  columns (9): ['participant_id', 'respc.corr_mean.incongruent', 'respc.corr_std.incongruent', 'respc.rt_mean.incongruent', 'respc.rt_std.incongruent', 'respc.corr_mean.congruent', 'respc.corr_std.congruent', 'respc.rt_mean.congruent', 'respc.rt_std.congruent']\n",
      "  ✔ 'participant_id' present | nulls=0 | unique IDs=88\n",
      "  (no 'sex' column)\n",
      "  (no 'age' column)\n",
      "\n",
      "[emo_df] type=DataFrame shape=(88, 49)\n",
      "  columns (49): ['participant_id', 'resp.corr_mean.incongruent.glad.adult', 'resp.corr_std.incongruent.glad.adult', 'resp.rt_mean.incongruent.glad.adult', 'resp.rt_std.incongruent.glad.adult', 'resp.corr_mean.incongruent.trist.adult', 'resp.corr_std.incongruent.trist.adult', 'resp.rt_mean.incongruent.trist.adult', 'resp.rt_std.incongruent.trist.adult', 'resp.corr_mean.incongruent.neu.adult', 'resp.corr_std.incongruent.neu.adult', 'resp.rt_mean.incongruent.neu.adult'] ...\n",
      "  ✔ 'participant_id' present | nulls=0 | unique IDs=88\n",
      "  (no 'sex' column)\n",
      "  (no 'age' column)\n"
     ]
    }
   ],
   "source": [
    "##Checking for faults in code/missing things after kernel restart\n",
    "# ==== DIAGNOSTICS: what exists, shapes, key columns, common pitfalls ====\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# If you use these paths, set them so we can also check file existence (optional)\n",
    "DETAILS_XLSX = Path(\"C:/github/master_calculations/data/p_details.xlsx\")\n",
    "STROOP_CSV   = Path(\"C:/github/master_calculations/data/stroop_wide.csv\")\n",
    "EMO_CSV      = Path(\"C:/github/master_calculations/data/emotional_wide.csv\")\n",
    "\n",
    "def exists(name):\n",
    "    return name in globals()\n",
    "\n",
    "def info_df(name):\n",
    "    df = globals()[name]\n",
    "    print(f\"\\n[{name}] type={type(df).__name__} shape={getattr(df, 'shape', None)}\")\n",
    "    cols = list(df.columns) if hasattr(df, \"columns\") else []\n",
    "    print(f\"  columns ({len(cols)}): {cols[:12]}{' ...' if len(cols)>12 else ''}\")\n",
    "    if \"participant_id\" in cols:\n",
    "        pid_nulls = df[\"participant_id\"].isna().sum()\n",
    "        print(f\"  ✔ 'participant_id' present | nulls={pid_nulls} | unique IDs={df['participant_id'].nunique()}\")\n",
    "    elif any(\"participant\" in str(c).lower() and \"id\" in str(c).lower() for c in cols):\n",
    "        cand = [c for c in cols if \"participant\" in str(c).lower() and \"id\" in str(c).lower()][0]\n",
    "        print(f\"  ⚠ 'participant_id' missing; found candidate column: '{cand}'\")\n",
    "    else:\n",
    "        print(\"  ❌ No participant id-like column found.\")\n",
    "\n",
    "    for c in (\"sex\",\"age\"):\n",
    "        if c in cols:\n",
    "            nnull = df[c].isna().sum()\n",
    "            print(f\"  {c}: dtype={df[c].dtype} | nulls={nnull}\")\n",
    "        else:\n",
    "            print(f\"  (no '{c}' column)\")\n",
    "\n",
    "# 0) Files on disk (optional but helpful)\n",
    "print(\"=== FILE CHECKS ===\")\n",
    "for p in (DETAILS_XLSX, STROOP_CSV, EMO_CSV):\n",
    "    try:\n",
    "        print(f\"{p}  ->  {'OK' if p.exists() else 'MISSING'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{p}: (skipped) {e}\")\n",
    "\n",
    "# 1) What DataFrames are in memory?\n",
    "print(\"\\n=== DATAFRAMES IN MEMORY ===\")\n",
    "for name in (\"details_df\",\"details_unique\",\"stroop_df\",\"emo_df\",\"stroop_merged\",\"emo_merged\"):\n",
    "    print(f\"{name}: {'present' if exists(name) else 'absent'}\")\n",
    "\n",
    "# 2) Quick summaries for those that exist\n",
    "for name in (\"details_df\",\"details_unique\",\"stroop_df\",\"emo_df\",\"stroop_merged\",\"emo_merged\"):\n",
    "    if exists(name):\n",
    "        try:\n",
    "            info_df(name)\n",
    "        except Exception as e:\n",
    "            print(f\"  !! Could not inspect {name}: {e}\")\n",
    "\n",
    "# 3) Common merge blockers (only if both sides exist)\n",
    "def check_merge_ready(left_name, right_name=\"details_unique\"):\n",
    "    if exists(left_name) and exists(right_name):\n",
    "        L = globals()[left_name]; R = globals()[right_name]\n",
    "        print(f\"\\n=== MERGE READINESS: {left_name} ⟷ {right_name} on 'participant_id' ===\")\n",
    "        if \"participant_id\" not in L.columns: print(f\"  ❌ {left_name} missing 'participant_id'\")\n",
    "        if \"participant_id\" not in R.columns: print(f\"  ❌ {right_name} missing 'participant_id'\")\n",
    "        if \"participant_id\" in L.columns and \"participant_id\" in R.columns:\n",
    "            ltype = L[\"participant_id\"].dtype; rtype = R[\"participant_id\"].dtype\n",
    "            print(f\"  dtypes: {left_name}={ltype} | {right_name}={rtype}\")\n",
    "            # uniqueness on right for validate='m:1'\n",
    "            right_dups = R[\"participant_id\"].duplicated().sum()\n",
    "            print(f\"  {right_name} duplicates on participant_id: {right_dups}\")\n",
    "            if right_dups:\n",
    "                print(\"  → Use groupby/drop_duplicates to make right side unique before merge.\")\n",
    "            # coverage\n",
    "            missing_right = sorted(set(L[\"participant_id\"]) - set(R[\"participant_id\"]))\n",
    "            print(f\"  IDs in {left_name} not in {right_name}: {len(missing_right)}\")\n",
    "            if missing_right[:10]:\n",
    "                print(f\"   e.g., {missing_right[:10]}\")\n",
    "\n",
    "check_merge_ready(\"stroop_df\")\n",
    "check_merge_ready(\"emo_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b3eef3c4-4b10-481b-aae2-18a7c1316062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[details_unique] rows=350 | missing sex=21 | missing age=22\n",
      "[merge] stroop_df: rows=88 | unmatched sex=1 | unmatched age=0\n",
      "[merge] emo_df: rows=88 | unmatched sex=1 | unmatched age=0\n"
     ]
    }
   ],
   "source": [
    "# --- REPAIR: build details_keep/details_unique and merge into stroop/emo ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def normalize_id(s: pd.Series) -> pd.Series:\n",
    "    return (s.astype(str).str.replace(r\"\\.0$\", \"\", regex=True).str.strip())\n",
    "\n",
    "def first_nonnull(s: pd.Series):\n",
    "    s = s.dropna()\n",
    "    return s.iloc[0] if len(s) else pd.NA\n",
    "\n",
    "# 1) Build details_keep from your existing details_df\n",
    "if \"details_df\" not in globals():\n",
    "    raise NameError(\"details_df is not in memory. (You do have it, per diagnostics.)\")\n",
    "\n",
    "details_keep = details_df.copy()\n",
    "\n",
    "# ensure we have a normalized participant_id column\n",
    "if \"participant_id\" not in details_keep.columns and \"participant_ID\" in details_keep.columns:\n",
    "    details_keep[\"participant_id\"] = normalize_id(details_keep[\"participant_ID\"])\n",
    "else:\n",
    "    details_keep[\"participant_id\"] = normalize_id(details_keep[\"participant_id\"])\n",
    "\n",
    "# keep only the columns we need for merging\n",
    "cols = [\"participant_id\"] + [c for c in [\"sex\",\"age\"] if c in details_keep.columns]\n",
    "details_keep = details_keep[cols].copy()\n",
    "\n",
    "# tidy types\n",
    "details_keep[\"sex\"] = (details_keep.get(\"sex\")\n",
    "                       .astype(str).str.strip().str.lower()\n",
    "                       .replace({\"female\":\"f\",\"male\":\"m\",\"nan\":np.nan,\"<na>\":np.nan}))\n",
    "details_keep[\"age\"] = pd.to_numeric(details_keep.get(\"age\"), errors=\"coerce\")\n",
    "\n",
    "# 2) De-duplicate to one row per participant (first non-null rule)\n",
    "details_unique = (details_keep\n",
    "                  .sort_values(\"participant_id\")\n",
    "                  .groupby(\"participant_id\", as_index=False)\n",
    "                  .agg(sex=(\"sex\", first_nonnull),\n",
    "                       age=(\"age\", first_nonnull)))\n",
    "\n",
    "print(f\"[details_unique] rows={details_unique.shape[0]} | missing sex={details_unique['sex'].isna().sum()} | missing age={details_unique['age'].isna().sum()}\")\n",
    "\n",
    "# 3) Merge into stroop/emo that you already have\n",
    "def do_merge(left_df, name):\n",
    "    if name not in globals():\n",
    "        print(f\"[merge] {name} not in memory, skipping.\")\n",
    "        return None\n",
    "    df = globals()[name].copy()\n",
    "    if \"participant_id\" not in df.columns:\n",
    "        raise KeyError(f\"{name} is missing 'participant_id'\")\n",
    "    df[\"participant_id\"] = normalize_id(df[\"participant_id\"])\n",
    "    merged = df.merge(details_unique, on=\"participant_id\", how=\"left\", validate=\"m:1\")\n",
    "    globals()[f\"{name}_merged\"] = merged\n",
    "    print(f\"[merge] {name}: rows={merged.shape[0]} | unmatched sex={merged['sex'].isna().sum()} | unmatched age={merged['age'].isna().sum()}\")\n",
    "    return merged\n",
    "\n",
    "stroop_merged = do_merge(stroop_df, \"stroop_df\")\n",
    "emo_merged    = do_merge(emo_df,    \"emo_df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6fc4d34b-2136-4f39-b7ea-b80f6d0819c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Great — that means our base merge works, but we haven’t applied corrections/exclusions yet. Let’s add them on top of your current objects (no restarting, no reloading)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b2669335-aa61-4eea-ad16-358ba4e6759c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[details_unique] kept 323 (removed 27); missing sex=0 | missing age=0\n",
      "[merge] stroop_df: rows=87 | unmatched sex=0 | unmatched age=0\n",
      "[merge] emo_df: rows=87 | unmatched sex=0 | unmatched age=0\n"
     ]
    }
   ],
   "source": [
    "# --- APPLY your exclusions & manual fixes, then re-merge and report ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Exclude participants (incl. the 'non-binary' 1216)\n",
    "EXCLUDE_IDS = {\n",
    "    \"1004\",\"1010\",\"1014\",\"1017\",\"1041\",\"1044\",\"1054\",\"1060\",\"1065\",\"1068\",\n",
    "    \"1072\",\"1073\",\"1079\",\"1080\",\"1082\",\"1086\",\"1093\",\"1094\",\"1107\",\n",
    "    \"1143\",\"1225\",\"1238\",\"1239\",\"1288\",\"1312\",\"1338\",\"1216\"\n",
    "}\n",
    "\n",
    "# 2) Manual corrections you confirmed\n",
    "SEX_FIX = {\n",
    "    \"1103\":\"f\",\n",
    "    \"1233\":\"f\",   # was mistyped as 1223\n",
    "    \"1275\":\"m\",\n",
    "    \"1329\":\"m\",\n",
    "    \"1347\":\"f\",\n",
    "    \"1284\":\"f\",\n",
    "}\n",
    "AGE_FIX = {\n",
    "    \"1048\": 65,   # confirmed\n",
    "    # \"1224\": 41, # uncomment if you want this applied too\n",
    "}\n",
    "\n",
    "# 3) Apply fixes on the details you already built (details_unique)\n",
    "details_unique.loc[\n",
    "    details_unique[\"participant_id\"].isin(SEX_FIX), \"sex\"\n",
    "] = details_unique[\"participant_id\"].map(SEX_FIX)\n",
    "\n",
    "details_unique.loc[\n",
    "    details_unique[\"participant_id\"].isin(AGE_FIX), \"age\"\n",
    "] = details_unique[\"participant_id\"].map(AGE_FIX)\n",
    "\n",
    "# 4) Normalize types again\n",
    "details_unique[\"sex\"] = (details_unique[\"sex\"]\n",
    "                         .astype(str).str.strip().str.lower()\n",
    "                         .replace({\"female\":\"f\",\"male\":\"m\",\"nan\":np.nan,\"<na>\":np.nan}))\n",
    "details_unique[\"age\"] = pd.to_numeric(details_unique[\"age\"], errors=\"coerce\")\n",
    "\n",
    "# 5) Drop excluded IDs\n",
    "before = details_unique[\"participant_id\"].nunique()\n",
    "details_unique = details_unique[~details_unique[\"participant_id\"].isin(EXCLUDE_IDS)].copy()\n",
    "after = details_unique[\"participant_id\"].nunique()\n",
    "print(f\"[details_unique] kept {after} (removed {before-after}); \"\n",
    "      f\"missing sex={details_unique['sex'].isna().sum()} | missing age={details_unique['age'].isna().sum()}\")\n",
    "\n",
    "# 6) Re-merge into your task dataframes already in memory\n",
    "def remerge(name):\n",
    "    df = globals()[name].copy()\n",
    "    df = df[~df[\"participant_id\"].isin(EXCLUDE_IDS)].copy()\n",
    "    merged = df.merge(details_unique, on=\"participant_id\", how=\"left\", validate=\"m:1\")\n",
    "    globals()[f\"{name}_merged\"] = merged\n",
    "    miss_sex = merged[\"sex\"].isna().sum()\n",
    "    miss_age = merged[\"age\"].isna().sum()\n",
    "    print(f\"[merge] {name}: rows={merged.shape[0]} | unmatched sex={miss_sex} | unmatched age={miss_age}\")\n",
    "    if miss_sex:\n",
    "        ids = merged.loc[merged[\"sex\"].isna(), \"participant_id\"].unique().tolist()\n",
    "        print(f\"  → IDs missing sex in {name}: {ids}\")\n",
    "    if miss_age:\n",
    "        ids = merged.loc[merged[\"age\"].isna(), \"participant_id\"].unique().tolist()\n",
    "        print(f\"  → IDs missing age in {name}: {ids}\")\n",
    "\n",
    "remerge(\"stroop_df\")\n",
    "remerge(\"emo_df\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aecd680-56a5-4c4e-9058-9faeff77282f",
   "metadata": {},
   "source": [
    "What this does\n",
    "\n",
    "Applies sex fixes and age fix.\n",
    "\n",
    "Excludes the agreed IDs (including 1216).\n",
    "\n",
    "Re-merges into stroop_df/emo_df you already loaded.\n",
    "\n",
    "Prints any remaining IDs missing sex/age in the merged data (should drop to 0, or show the exact stragglers to fill)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a6d8bd-05e8-4618-8d1b-cfa20f54bd5e",
   "metadata": {},
   "source": [
    "'details_unique' kept 323 (removed 27)\n",
    "→ After applying your exclusions + fixes, your cleaned demographics table now has 323 participants (27 were removed).\n",
    "→ missing sex=0 | missing age=0 means the demographics are fully complete—no blanks.\n",
    "\n",
    "(merge) stroop_df: rows=87 and emo_df: rows=87\n",
    "→ Each task now has 87 participants after exclusions, and everyone has sex & age (no unmatched).\n",
    "We’re in great shape to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8b30fe8a-4815-4268-89ba-862ab19d4dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Dataset    N    F   M Age (mean ± SD) Age range\n",
      "0  Classical Stroop   88   58  29   48.40 ± 13.74   25 – 77\n",
      "1  Emotional Stroop   88   58  29   48.40 ± 13.74   25 – 77\n",
      "2  All Participants  323  226  97   47.74 ± 16.47   18 – 82\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'C:\\github\\master_calculations\\outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[130], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(demo)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# (Optional) Save clean datasets\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m stroop_merged\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/github/master_calculations/outputs/stroop_merged_clean.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     29\u001b[0m emo_merged\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/github/master_calculations/outputs/emo_merged_clean.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     30\u001b[0m details_unique\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/github/master_calculations/outputs/details_unique_clean.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3968\u001b[0m     path_or_buf,\n\u001b[0;32m   3969\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3970\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3971\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3972\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3973\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3974\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3975\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3976\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3977\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   3978\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3979\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   3980\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   3981\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   3982\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   3983\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3984\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    254\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    255\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[0;32m    256\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[0;32m    257\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[1;32m--> 749\u001b[0m     check_parent_directory(\u001b[38;5;28mstr\u001b[39m(handle))\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'C:\\github\\master_calculations\\outputs'"
     ]
    }
   ],
   "source": [
    "#final demographics table + saving the clean data:\n",
    "# Demographics summary\n",
    "def summarize_demo(df, label):\n",
    "    cols = [\"participant_id\",\"sex\",\"age\"]\n",
    "    d = df[cols].drop_duplicates(subset=\"participant_id\").copy()\n",
    "    n = len(d)\n",
    "    sex_counts = d[\"sex\"].value_counts().to_dict()\n",
    "    age_mean, age_sd = d[\"age\"].mean(), d[\"age\"].std()\n",
    "    age_min, age_max = d[\"age\"].min(), d[\"age\"].max()\n",
    "    return {\n",
    "        \"Dataset\": label,\n",
    "        \"N\": n,\n",
    "        \"F\": sex_counts.get(\"f\", 0),\n",
    "        \"M\": sex_counts.get(\"m\", 0),\n",
    "        \"Age (mean ± SD)\": f\"{age_mean:.2f} ± {age_sd:.2f}\",\n",
    "        \"Age range\": f\"{int(age_min)} – {int(age_max)}\",\n",
    "    }\n",
    "\n",
    "import pandas as pd\n",
    "demo = pd.DataFrame([\n",
    "    summarize_demo(stroop_df.merge(details_unique, on=\"participant_id\", how=\"left\"), \"Classical Stroop\"),\n",
    "    summarize_demo(emo_df.merge(details_unique, on=\"participant_id\", how=\"left\"),    \"Emotional Stroop\"),\n",
    "    summarize_demo(details_unique,                                                 \"All Participants\"),\n",
    "])\n",
    "print(demo)\n",
    "\n",
    "# (Optional) Save clean datasets\n",
    "stroop_merged.to_csv(\"C:/github/master_calculations/outputs/stroop_merged_clean.csv\", index=False)\n",
    "emo_merged.to_csv(\"C:/github/master_calculations/outputs/emo_merged_clean.csv\", index=False)\n",
    "details_unique.to_csv(\"C:/github/master_calculations/outputs/details_unique_clean.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "625e2baa-c872-41bd-8557-4aadeefc0b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "outdir = Path(\"C:/github/master_calculations/outputs\")\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "stroop_merged.to_csv(outdir / \"stroop_merged_clean.csv\", index=False)\n",
    "emo_merged.to_csv(outdir / \"emo_merged_clean.csv\", index=False)\n",
    "details_unique.to_csv(outdir / \"details_unique_clean.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15a2b58-e4f2-4120-8664-c967b057a88d",
   "metadata": {},
   "source": [
    "N=88, not 87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7b704136-fde3-4dae-8db9-b04b0174dda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Dataset    N    F   M Age (mean ± SD) Age range\n",
      "0  Classical Stroop   88   57  29   48.25 ± 13.73   25 – 77\n",
      "1  Emotional Stroop   88   57  29   48.25 ± 13.73   25 – 77\n",
      "2  All Participants  323  226  97   47.74 ± 16.47   18 – 82\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def summarize_demo(df, label):\n",
    "    d = df[[\"participant_id\",\"sex\",\"age\"]].drop_duplicates(\"participant_id\")\n",
    "    sex_counts = d[\"sex\"].value_counts().to_dict()\n",
    "    return {\n",
    "        \"Dataset\": label,\n",
    "        \"N\": len(d),\n",
    "        \"F\": sex_counts.get(\"f\", 0),\n",
    "        \"M\": sex_counts.get(\"m\", 0),\n",
    "        \"Age (mean ± SD)\": f\"{d['age'].mean():.2f} ± {d['age'].std():.2f}\",\n",
    "        \"Age range\": f\"{int(d['age'].min())} – {int(d['age'].max())}\",\n",
    "    }\n",
    "\n",
    "demo = pd.DataFrame([\n",
    "    summarize_demo(stroop_merged, \"Classical Stroop\"),   # <- use stroop_merged\n",
    "    summarize_demo(emo_merged,    \"Emotional Stroop\"),   # <- use emo_merged\n",
    "    summarize_demo(details_unique, \"All Participants\"),\n",
    "])\n",
    "print(demo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34700218-11cb-41dd-a814-7c52b3e85270",
   "metadata": {},
   "source": [
    "We’re seeing N=88 and F+M=86 (57+29), which means our summary pulled from a dataframe that still has 2 rows with missing sex and 88 unique IDs. Earlier we had 87 with no missing—so we’ve likely summarized the wrong DF (e.g., stroop_df instead of stroop_merged) or stroop_merged got rebuilt without exclusions.\n",
    "\n",
    "Running this tiny check+fix block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d1c770c4-7f75-45f5-b978-0f126f99556f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[stroop_merged] rows=88 | unique IDs=88\n",
      "sex counts (incl. missing):\n",
      "sex\n",
      "f             57\n",
      "m             29\n",
      "non-binary     1\n",
      "<NA>           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[emo_merged] rows=88 | unique IDs=88\n",
      "sex counts (incl. missing):\n",
      "sex\n",
      "f             57\n",
      "m             29\n",
      "non-binary     1\n",
      "<NA>           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "IDs with missing sex in stroop_merged: ['1284']\n",
      "\n",
      "Excluded IDs present in stroop_merged: ['1216']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def quick_check(df, name):\n",
    "    print(f\"\\n[{name}] rows={df.shape[0]} | unique IDs={df['participant_id'].nunique()}\")\n",
    "    print(\"sex counts (incl. missing):\")\n",
    "    print(df[\"sex\"].value_counts(dropna=False))\n",
    "\n",
    "# 1) Verify we're using the merged, filtered tables\n",
    "quick_check(stroop_merged, \"stroop_merged\")\n",
    "quick_check(emo_merged,    \"emo_merged\")\n",
    "\n",
    "# 2) If any missing sex remain, list the IDs\n",
    "missing_ids = stroop_merged.loc[stroop_merged[\"sex\"].isna(), \"participant_id\"].unique().tolist()\n",
    "if missing_ids:\n",
    "    print(\"\\nIDs with missing sex in stroop_merged:\", missing_ids)\n",
    "\n",
    "# 3) Ensure no excluded IDs slipped back in\n",
    "EXCLUDE_IDS = {\n",
    "    \"1004\",\"1010\",\"1014\",\"1017\",\"1041\",\"1044\",\"1054\",\"1060\",\"1065\",\"1068\",\n",
    "    \"1072\",\"1073\",\"1079\",\"1080\",\"1082\",\"1086\",\"1093\",\"1094\",\"1107\",\n",
    "    \"1143\",\"1225\",\"1238\",\"1239\",\"1288\",\"1312\",\"1338\",\"1216\"\n",
    "}\n",
    "leaked = sorted(set(stroop_merged[\"participant_id\"]) & EXCLUDE_IDS)\n",
    "print(\"\\nExcluded IDs present in stroop_merged:\", leaked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f4fcae52-2c67-4a83-b26e-67f14a522b31",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "count mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[140], line 21\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m n \u001b[38;5;241m==\u001b[39m f \u001b[38;5;241m+\u001b[39m m \u001b[38;5;241m+\u001b[39m miss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount mismatch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m: label,\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAge range\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m – \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m     }\n\u001b[0;32m     20\u001b[0m demo \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([\n\u001b[1;32m---> 21\u001b[0m     summarize_strict(stroop_merged, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassical Stroop\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     22\u001b[0m     summarize_strict(emo_merged,    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmotional Stroop\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     23\u001b[0m     summarize_strict(details_unique,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll Participants\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     24\u001b[0m ])\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(demo)\n",
      "Cell \u001b[1;32mIn[140], line 9\u001b[0m, in \u001b[0;36msummarize_strict\u001b[1;34m(df, label)\u001b[0m\n\u001b[0;32m      7\u001b[0m m \u001b[38;5;241m=\u001b[39m (d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m      8\u001b[0m miss \u001b[38;5;241m=\u001b[39m d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m n \u001b[38;5;241m==\u001b[39m f \u001b[38;5;241m+\u001b[39m m \u001b[38;5;241m+\u001b[39m miss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount mismatch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m: label,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAge range\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m – \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m }\n",
      "\u001b[1;31mAssertionError\u001b[0m: count mismatch"
     ]
    }
   ],
   "source": [
    "def summarize_strict(df, label):\n",
    "    d = df[[\"participant_id\",\"sex\",\"age\"]].drop_duplicates(\"participant_id\").copy()\n",
    "    d[\"sex\"] = (d[\"sex\"].astype(str).str.strip().str.lower()\n",
    "                .replace({\"female\":\"f\",\"male\":\"m\",\"nan\":np.nan,\"<na>\":np.nan}))\n",
    "    n = len(d)\n",
    "    f = (d[\"sex\"]==\"f\").sum()\n",
    "    m = (d[\"sex\"]==\"m\").sum()\n",
    "    miss = d[\"sex\"].isna().sum()\n",
    "    assert n == f + m + miss, \"count mismatch\"\n",
    "    return {\n",
    "        \"Dataset\": label,\n",
    "        \"N\": n,\n",
    "        \"F\": f,\n",
    "        \"M\": m,\n",
    "        \"Missing sex\": miss,\n",
    "        \"Age (mean ± SD)\": f\"{d['age'].mean():.2f} ± {d['age'].std():.2f}\",\n",
    "        \"Age range\": f\"{int(d['age'].min())} – {int(d['age'].max())}\",\n",
    "    }\n",
    "\n",
    "demo = pd.DataFrame([\n",
    "    summarize_strict(stroop_merged, \"Classical Stroop\"),\n",
    "    summarize_strict(emo_merged,    \"Emotional Stroop\"),\n",
    "    summarize_strict(details_unique,\"All Participants\"),\n",
    "])\n",
    "print(demo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "70fb4d16-1051-42f2-88ae-7f0eedc2961c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stroop_merged sex values:\n",
      " sex\n",
      "f             57\n",
      "m             29\n",
      "non-binary     1\n",
      "<NA>           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "emo_merged sex values:\n",
      " sex\n",
      "f             57\n",
      "m             29\n",
      "non-binary     1\n",
      "<NA>           1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"stroop_merged sex values:\\n\", stroop_merged[\"sex\"].astype(str).value_counts(dropna=False).head(20))\n",
    "print(\"\\nemo_merged sex values:\\n\",    emo_merged[\"sex\"].astype(str).value_counts(dropna=False).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c4ca0068-1ced-484f-8d84-d6ab7f87e7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-binary IDs in Stroop: ['1216']\n",
      "Missing-sex IDs in Stroop: ['1284']\n",
      "\n",
      "After fixes:\n",
      "stroop_merged sex counts (incl. missing):\n",
      "sex\n",
      "f    58\n",
      "m    29\n",
      "Name: count, dtype: int64\n",
      "\n",
      "emo_merged sex counts (incl. missing):\n",
      "sex\n",
      "f    58\n",
      "m    29\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final demo:\n",
      "            Dataset    N    F   M  Missing sex Age (mean ± SD) Age range\n",
      "0  Classical Stroop   87   58  29            0   48.40 ± 13.74   25 – 77\n",
      "1  Emotional Stroop   87   58  29            0   48.40 ± 13.74   25 – 77\n",
      "2  All Participants  323  226  97            0   47.74 ± 16.47   18 – 82\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Find which IDs are 'non-binary' and which are missing sex\n",
    "nb_ids_stroop = stroop_merged.loc[\n",
    "    stroop_merged[\"sex\"].astype(str).str.strip().str.lower() == \"non-binary\",\n",
    "    \"participant_id\"\n",
    "].unique().tolist()\n",
    "\n",
    "na_ids_stroop = stroop_merged.loc[\n",
    "    stroop_merged[\"sex\"].isna() |\n",
    "    stroop_merged[\"sex\"].astype(str).str.strip().str.lower().isin([\"<na>\", \"nan\", \"\"])\n",
    ", \"participant_id\"].unique().tolist()\n",
    "\n",
    "print(\"Non-binary IDs in Stroop:\", nb_ids_stroop)\n",
    "print(\"Missing-sex IDs in Stroop:\", na_ids_stroop)\n",
    "\n",
    "# 2) Exclude non-binary IDs everywhere\n",
    "EXTRA_EXCLUDE = set(nb_ids_stroop)\n",
    "if EXTRA_EXCLUDE:\n",
    "    details_unique = details_unique[~details_unique[\"participant_id\"].isin(EXTRA_EXCLUDE)].copy()\n",
    "    stroop_merged  = stroop_merged[~stroop_merged[\"participant_id\"].isin(EXTRA_EXCLUDE)].copy()\n",
    "    emo_merged     = emo_merged[~emo_merged[\"participant_id\"].isin(EXTRA_EXCLUDE)].copy()\n",
    "\n",
    "# 3) Normalize sex to only f / m / NaN\n",
    "def coerce_sex(s):\n",
    "    s = s.astype(str).str.strip().str.lower()\n",
    "    s = s.replace({\"female\": \"f\", \"male\": \"m\", \"<na>\": np.nan, \"nan\": np.nan})\n",
    "    s = s.where(s.isin([\"f\",\"m\"]), np.nan)\n",
    "    return s\n",
    "\n",
    "for df in (details_unique, stroop_merged, emo_merged):\n",
    "    df[\"sex\"] = coerce_sex(df[\"sex\"])\n",
    "\n",
    "# 4) If you KNOW the missing-sex ID (e.g., 1284) is female, set it here:\n",
    "KNOWN_SEX_FIX = {\n",
    "    \"1284\": \"f\",   # remove or change if not applicable\n",
    "}\n",
    "for pid, sx in KNOWN_SEX_FIX.items():\n",
    "    for df in (details_unique, stroop_merged, emo_merged):\n",
    "        df.loc[df[\"participant_id\"] == pid, \"sex\"] = sx\n",
    "\n",
    "# 5) Re-check counts\n",
    "print(\"\\nAfter fixes:\")\n",
    "print(\"stroop_merged sex counts (incl. missing):\")\n",
    "print(stroop_merged[\"sex\"].value_counts(dropna=False))\n",
    "print(\"\\nemo_merged sex counts (incl. missing):\")\n",
    "print(emo_merged[\"sex\"].value_counts(dropna=False))\n",
    "\n",
    "# 6) Final summary\n",
    "def summarize(df, label):\n",
    "    d = df[[\"participant_id\",\"sex\",\"age\"]].drop_duplicates(\"participant_id\").copy()\n",
    "    d[\"age\"] = pd.to_numeric(d[\"age\"], errors=\"coerce\")\n",
    "    n = len(d)\n",
    "    f = (d[\"sex\"]==\"f\").sum()\n",
    "    m = (d[\"sex\"]==\"m\").sum()\n",
    "    miss = d[\"sex\"].isna().sum()\n",
    "    return {\n",
    "        \"Dataset\": label,\n",
    "        \"N\": n, \"F\": f, \"M\": m, \"Missing sex\": miss,\n",
    "        \"Age (mean ± SD)\": f\"{d['age'].mean():.2f} ± {d['age'].std():.2f}\",\n",
    "        \"Age range\": f\"{int(d['age'].min())} – {int(d['age'].max())}\",\n",
    "    }\n",
    "\n",
    "demo = pd.DataFrame([\n",
    "    summarize(stroop_merged, \"Classical Stroop\"),\n",
    "    summarize(emo_merged,    \"Emotional Stroop\"),\n",
    "    summarize(details_unique,\"All Participants\"),\n",
    "])\n",
    "print(\"\\nFinal demo:\")\n",
    "print(demo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60767a48-a3e3-4710-b965-33466ee15966",
   "metadata": {},
   "source": [
    "✅ Participant 1216 (non-binary) was successfully excluded everywhere.\n",
    "\n",
    "✅ Participant 1284 was filled in as female, so no more <NA> missing sex.\n",
    "\n",
    "✅ Both Stroop and Emotional Stroop datasets now have 87 participants each (58F / 29M).\n",
    "\n",
    "✅ Your full participant pool is 323 total (226F / 97M), with no missing sex or age values.\n",
    "\n",
    "✅ Age stats look consistent and clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0f6b423a-8306-4e38-ba68-c80ee6ec0cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved clean datasets to: C:\\github\\master_calculations\\final_clean_data\n"
     ]
    }
   ],
   "source": [
    "#Saving the final Dataset that is cleaned\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# make folder (with spaces replaced by underscores for safe path handling)\n",
    "outdir = Path(\"C:/github/master_calculations/final_clean_data\")\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save cleaned data\n",
    "stroop_merged.to_csv(outdir / \"stroop_merged.csv\", index=False)\n",
    "emo_merged.to_csv(outdir / \"emo_merged.csv\", index=False)\n",
    "details_unique.to_csv(outdir / \"details_unique.csv\", index=False)\n",
    "\n",
    "print(\"✅ Saved clean datasets to:\", outdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d2c4c-353c-48b9-8d60-81572566b69c",
   "metadata": {},
   "source": [
    "Perfect — let’s save both CSV and Excel (.xlsx) versions in our final_clean_data folder 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "dd3217d9-4982-47be-83ba-2692eada0b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved CSVs + Excel workbook to: C:\\github\\master_calculations\\final_clean_data\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# create folder if not exists\n",
    "outdir = Path(\"C:/github/master_calculations/final_clean_data\")\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# CSV\n",
    "stroop_merged.to_csv(outdir / \"stroop_merged.csv\", index=False)\n",
    "emo_merged.to_csv(outdir / \"emo_merged.csv\", index=False)\n",
    "details_unique.to_csv(outdir / \"details_unique.csv\", index=False)\n",
    "\n",
    "# Excel\n",
    "with pd.ExcelWriter(outdir / \"clean_data.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    stroop_merged.to_excel(writer, sheet_name=\"Stroop\", index=False)\n",
    "    emo_merged.to_excel(writer, sheet_name=\"Emo\", index=False)\n",
    "    details_unique.to_excel(writer, sheet_name=\"Details\", index=False)\n",
    "\n",
    "print(\"✅ Saved CSVs + Excel workbook to:\", outdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc95d91-03e3-4c19-83af-6189af3d31e5",
   "metadata": {},
   "source": [
    "Let’s add our demographics summary table as a fourth sheet in the Excel workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "316330d7-af17-4d39-97bd-ac74968d2331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Dataset    N    F   M  Missing sex Age (mean ± SD) Age range\n",
      "0  Classical Stroop   87   58  29            0   48.40 ± 13.74   25 – 77\n",
      "1  Emotional Stroop   87   58  29            0   48.40 ± 13.74   25 – 77\n",
      "2  All Participants  323  226  97            0   47.74 ± 16.47   18 – 82\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def summarize(df, label):\n",
    "    d = df[[\"participant_id\",\"sex\",\"age\"]].drop_duplicates(\"participant_id\").copy()\n",
    "    d[\"age\"] = pd.to_numeric(d[\"age\"], errors=\"coerce\")\n",
    "    return {\n",
    "        \"Dataset\": label,\n",
    "        \"N\": len(d),\n",
    "        \"F\": (d[\"sex\"]==\"f\").sum(),\n",
    "        \"M\": (d[\"sex\"]==\"m\").sum(),\n",
    "        \"Missing sex\": d[\"sex\"].isna().sum(),\n",
    "        \"Age (mean ± SD)\": f\"{d['age'].mean():.2f} ± {d['age'].std():.2f}\",\n",
    "        \"Age range\": f\"{int(d['age'].min())} – {int(d['age'].max())}\",bb\n",
    "    }\n",
    "\n",
    "demo = pd.DataFrame([\n",
    "    summarize(stroop_merged, \"Classical Stroop\"),\n",
    "    summarize(emo_merged,    \"Emotional Stroop\"),\n",
    "    summarize(details_unique,\"All Participants\"),\n",
    "])\n",
    "\n",
    "print(demo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "12be7953-eba5-4872-b022-505b139f836f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Dataset          |   N |   F |   M |   Missing sex | Age (mean ± SD)   | Age range   |\n",
      "|:-----------------|----:|----:|----:|--------------:|:------------------|:------------|\n",
      "| Classical Stroop |  87 |  58 |  29 |             0 | 48.40 ± 13.74     | 25 – 77     |\n",
      "| Emotional Stroop |  87 |  58 |  29 |             0 | 48.40 ± 13.74     | 25 – 77     |\n",
      "| All Participants | 323 | 226 |  97 |             0 | 47.74 ± 16.47     | 18 – 82     |\n",
      "\n",
      "✅ Saved demographics_summary.xlsx in C:\\github\\master_calculations\\final_clean_data\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# 📊 Final demographics summary function\n",
    "def summarize(df, label):\n",
    "    d = df[[\"participant_id\",\"sex\",\"age\"]].drop_duplicates(\"participant_id\").copy()\n",
    "    d[\"age\"] = pd.to_numeric(d[\"age\"], errors=\"coerce\")\n",
    "    return {\n",
    "        \"Dataset\": label,\n",
    "        \"N\": len(d),\n",
    "        \"F\": (d[\"sex\"]==\"f\").sum(),\n",
    "        \"M\": (d[\"sex\"]==\"m\").sum(),\n",
    "        \"Missing sex\": d[\"sex\"].isna().sum(),\n",
    "        \"Age (mean ± SD)\": f\"{d['age'].mean():.2f} ± {d['age'].std():.2f}\",\n",
    "        \"Age range\": f\"{int(d['age'].min())} – {int(d['age'].max())}\",\n",
    "    }\n",
    "\n",
    "# ✅ Build summary table\n",
    "demo = pd.DataFrame([\n",
    "    summarize(stroop_merged, \"Classical Stroop\"),\n",
    "    summarize(emo_merged,    \"Emotional Stroop\"),\n",
    "    summarize(details_unique,\"All Participants\"),\n",
    "])\n",
    "\n",
    "# 🖥️ Pretty-print for thesis (Markdown-style)\n",
    "print(demo.to_markdown(index=False))\n",
    "\n",
    "# 💾 Save to Excel in final_clean_data\n",
    "outdir = Path(\"C:/github/master_calculations/final_clean_data\")\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "demo.to_excel(outdir / \"demographics_summary.xlsx\", index=False)\n",
    "\n",
    "print(f\"\\n✅ Saved demographics_summary.xlsx in {outdir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7afd19-6b3d-427a-b839-4c8aea570771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
