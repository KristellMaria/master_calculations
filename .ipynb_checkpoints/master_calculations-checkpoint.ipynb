{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fa28fec-d5d7-425f-80f7-b07158469676",
   "metadata": {},
   "source": [
    "### Does Bilingualism affect the Inhibition Deficit and the Aging effect on the elderly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a74d5-1e0e-417c-8446-ab5cbf4cef26",
   "metadata": {},
   "source": [
    "#### Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e04fc9-c0dc-438f-8d5e-9edc2d21775e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcc55234-81bf-4231-a8a4-c5ec996673e6",
   "metadata": {},
   "source": [
    "#### Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6669df-fbf8-44cb-9679-fc4aa71342a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9b09496-be7b-4efc-ba7b-a2dfc70066af",
   "metadata": {},
   "source": [
    "##### Pacages Imported to perform the different Statistical tests, analyses, and graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99e7ea39-73a2-4850-a3f4-a8ecfd36318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install package to open and read excel sheets\n",
    "from pathlib import Path\n",
    "#!pip install openpyxl\n",
    "\n",
    "# Installing package to perform Normality check (Shapiro-Wilk test)\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Installing package to perform other Statistical analysis like Kruskal-Wallis test, and hypothesis testing \n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Installing Scikit-posthocs package to perform post-hoc test\n",
    "#!pip install scikit-posthocs\n",
    "\n",
    "# Installing to perform specific post-hoc test, Dunn's post-hoc test.\n",
    "import scikit_posthocs as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Installing pingouin Library package to perform mixed-design ANOVA, and other Statistical analysis:\n",
    "#!pip install pingouin\n",
    "import pingouin as pg\n",
    "import itertools\n",
    "\n",
    "# Installing package to create some visuals like error-bars.\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce4ef9-449c-4554-8317-497258ab1978",
   "metadata": {},
   "source": [
    "##### Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dad669-4ce2-4c49-982c-7e524f8cffd3",
   "metadata": {},
   "source": [
    "In this pipeline we use mostly Pandas packaging. Pandas DataFrames offer a powerful and flexible structure for data manipulation and analysis. They enable efficient data cleaning, merging, reshaping, and summarization with concise, readable code—making complex data workflows accessible and reproducible, which is essential for robust scientific research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc1ef5-b9f4-4bdb-ba89-ed3531cd49d3",
   "metadata": {},
   "source": [
    "##### Calculation Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c6ade8-3b5c-4586-a90f-bb274a319b63",
   "metadata": {},
   "source": [
    "- Upload the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe9cf25-6d7b-4e49-8f9a-93128675c08d",
   "metadata": {},
   "source": [
    "1. Upload data from csv.\n",
    "   It was detected when we uploaded the data that the tables was showing separation by ; not ,\n",
    "   * had to define new sep as it was separated by ; not ,\n",
    "   * Data values were written with comma rather than dot. Therefore we needed to replace the commas with dots. Then python could automatically convert to float values we also used slicing to get the columns needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48955073-141a-4225-87ea-815816f0e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "473cda06-e8d3-499b-99be-8c832b2017c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your files (note the extension is now .csv)\n",
    "emo_df = pd.read_csv(Path(\"C:/github/master_calculations/data/emotional_wide.csv\"), sep=\";\", decimal=\",\")\n",
    "stroop_df = pd.read_csv(Path(\"C:/github/master_calculations/data/stroop_wide.csv\"), sep=\";\", decimal=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efed7090-655d-4231-8636-00616d62f95b",
   "metadata": {},
   "source": [
    "- Open files:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e21c00d-49f0-4c12-a32c-1efac2c7b109",
   "metadata": {},
   "source": [
    "Classical Stroop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48320315-0950-4ba8-915a-b232b5b4af01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>respc.corr_mean.incongruent</th>\n",
       "      <th>respc.corr_std.incongruent</th>\n",
       "      <th>respc.rt_mean.incongruent</th>\n",
       "      <th>respc.rt_std.incongruent</th>\n",
       "      <th>respc.corr_mean.congruent</th>\n",
       "      <th>respc.corr_std.congruent</th>\n",
       "      <th>respc.rt_mean.congruent</th>\n",
       "      <th>respc.rt_std.congruent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.715111</td>\n",
       "      <td>0.254540</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.074536</td>\n",
       "      <td>0.773110</td>\n",
       "      <td>0.257307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1204</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.606518</td>\n",
       "      <td>0.126010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.677501</td>\n",
       "      <td>0.173684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1205</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.995745</td>\n",
       "      <td>0.265899</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.074536</td>\n",
       "      <td>1.118971</td>\n",
       "      <td>0.252094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1206</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.091287</td>\n",
       "      <td>0.704573</td>\n",
       "      <td>0.155570</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.074536</td>\n",
       "      <td>0.816686</td>\n",
       "      <td>0.219771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1207</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.683636</td>\n",
       "      <td>0.224203</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.223607</td>\n",
       "      <td>0.797774</td>\n",
       "      <td>0.224723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1319</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.725578</td>\n",
       "      <td>0.237461</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.751577</td>\n",
       "      <td>0.212407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1326</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.149071</td>\n",
       "      <td>0.937385</td>\n",
       "      <td>0.420897</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.886707</td>\n",
       "      <td>0.254680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1337</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.074536</td>\n",
       "      <td>0.672102</td>\n",
       "      <td>0.261308</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.074536</td>\n",
       "      <td>0.654819</td>\n",
       "      <td>0.181993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1344</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.074536</td>\n",
       "      <td>1.105055</td>\n",
       "      <td>0.271146</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.240358</td>\n",
       "      <td>1.289605</td>\n",
       "      <td>0.350826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1345</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.074536</td>\n",
       "      <td>0.810246</td>\n",
       "      <td>0.194319</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.074536</td>\n",
       "      <td>1.032755</td>\n",
       "      <td>0.272284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    participant_id  respc.corr_mean.incongruent  respc.corr_std.incongruent  \\\n",
       "0             1203                     1.000000                    0.000000   \n",
       "1             1204                     1.000000                    0.000000   \n",
       "2             1205                     1.000000                    0.000000   \n",
       "3             1206                     0.933333                    0.091287   \n",
       "4             1207                     1.000000                    0.000000   \n",
       "..             ...                          ...                         ...   \n",
       "83            1319                     1.000000                    0.000000   \n",
       "84            1326                     0.933333                    0.149071   \n",
       "85            1337                     0.966667                    0.074536   \n",
       "86            1344                     0.966667                    0.074536   \n",
       "87            1345                     0.966667                    0.074536   \n",
       "\n",
       "    respc.rt_mean.incongruent  respc.rt_std.incongruent  \\\n",
       "0                    0.715111                  0.254540   \n",
       "1                    0.606518                  0.126010   \n",
       "2                    0.995745                  0.265899   \n",
       "3                    0.704573                  0.155570   \n",
       "4                    0.683636                  0.224203   \n",
       "..                        ...                       ...   \n",
       "83                   0.725578                  0.237461   \n",
       "84                   0.937385                  0.420897   \n",
       "85                   0.672102                  0.261308   \n",
       "86                   1.105055                  0.271146   \n",
       "87                   0.810246                  0.194319   \n",
       "\n",
       "    respc.corr_mean.congruent  respc.corr_std.congruent  \\\n",
       "0                    0.966667                  0.074536   \n",
       "1                    1.000000                  0.000000   \n",
       "2                    0.966667                  0.074536   \n",
       "3                    0.966667                  0.074536   \n",
       "4                    0.900000                  0.223607   \n",
       "..                        ...                       ...   \n",
       "83                   1.000000                  0.000000   \n",
       "84                   1.000000                  0.000000   \n",
       "85                   0.966667                  0.074536   \n",
       "86                   0.866667                  0.240358   \n",
       "87                   0.966667                  0.074536   \n",
       "\n",
       "    respc.rt_mean.congruent  respc.rt_std.congruent  \n",
       "0                  0.773110                0.257307  \n",
       "1                  0.677501                0.173684  \n",
       "2                  1.118971                0.252094  \n",
       "3                  0.816686                0.219771  \n",
       "4                  0.797774                0.224723  \n",
       "..                      ...                     ...  \n",
       "83                 0.751577                0.212407  \n",
       "84                 0.886707                0.254680  \n",
       "85                 0.654819                0.181993  \n",
       "86                 1.289605                0.350826  \n",
       "87                 1.032755                0.272284  \n",
       "\n",
       "[88 rows x 9 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stroop_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74b0be6-ebca-4e4b-a095-e8be26c12f7a",
   "metadata": {},
   "source": [
    "Emotional Stroop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38e42ce9-3e9f-44d9-845d-b27e2e216140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>resp.corr_mean.incongruent.glad.adult</th>\n",
       "      <th>resp.corr_std.incongruent.glad.adult</th>\n",
       "      <th>resp.rt_mean.incongruent.glad.adult</th>\n",
       "      <th>resp.rt_std.incongruent.glad.adult</th>\n",
       "      <th>resp.corr_mean.incongruent.trist.adult</th>\n",
       "      <th>resp.corr_std.incongruent.trist.adult</th>\n",
       "      <th>resp.rt_mean.incongruent.trist.adult</th>\n",
       "      <th>resp.rt_std.incongruent.trist.adult</th>\n",
       "      <th>resp.corr_mean.incongruent.neu.adult</th>\n",
       "      <th>...</th>\n",
       "      <th>resp.rt_mean.congruent.glad.baby</th>\n",
       "      <th>resp.rt_std.congruent.glad.baby</th>\n",
       "      <th>resp.corr_mean.congruent.trist.baby</th>\n",
       "      <th>resp.corr_std.congruent.trist.baby</th>\n",
       "      <th>resp.rt_mean.congruent.trist.baby</th>\n",
       "      <th>resp.rt_std.congruent.trist.baby</th>\n",
       "      <th>resp.corr_mean.congruent.neu.baby</th>\n",
       "      <th>resp.corr_std.congruent.neu.baby</th>\n",
       "      <th>resp.rt_mean.congruent.neu.baby</th>\n",
       "      <th>resp.rt_std.congruent.neu.baby</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.834702</td>\n",
       "      <td>0.143169</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.141421</td>\n",
       "      <td>0.925454</td>\n",
       "      <td>0.278187</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.854535</td>\n",
       "      <td>0.232593</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>0.924575</td>\n",
       "      <td>0.271926</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>1.000787</td>\n",
       "      <td>0.318872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1204</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.141421</td>\n",
       "      <td>0.631355</td>\n",
       "      <td>0.123766</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.651777</td>\n",
       "      <td>0.033001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704160</td>\n",
       "      <td>0.173085</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>0.581735</td>\n",
       "      <td>0.123698</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.580491</td>\n",
       "      <td>0.083795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1205</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.855834</td>\n",
       "      <td>0.194823</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.771844</td>\n",
       "      <td>0.109109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.850434</td>\n",
       "      <td>0.135730</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.117851</td>\n",
       "      <td>0.863746</td>\n",
       "      <td>0.138510</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>0.794245</td>\n",
       "      <td>0.176831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1206</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.734624</td>\n",
       "      <td>0.079265</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.852039</td>\n",
       "      <td>0.151549</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.820891</td>\n",
       "      <td>0.107333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.830719</td>\n",
       "      <td>0.146121</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.744530</td>\n",
       "      <td>0.085321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1207</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.936832</td>\n",
       "      <td>0.195431</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.804329</td>\n",
       "      <td>0.208366</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.935145</td>\n",
       "      <td>0.187571</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>0.885868</td>\n",
       "      <td>0.133503</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>0.743572</td>\n",
       "      <td>0.095836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1319</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.968130</td>\n",
       "      <td>0.235274</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.879301</td>\n",
       "      <td>0.067024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.073240</td>\n",
       "      <td>0.315628</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>1.108588</td>\n",
       "      <td>0.372499</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.797543</td>\n",
       "      <td>0.135086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1326</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.844349</td>\n",
       "      <td>0.111004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.978748</td>\n",
       "      <td>0.102694</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.852385</td>\n",
       "      <td>0.072896</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>0.949973</td>\n",
       "      <td>0.147841</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>0.858571</td>\n",
       "      <td>0.120270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1337</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.744503</td>\n",
       "      <td>0.144672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.839017</td>\n",
       "      <td>0.099145</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.760030</td>\n",
       "      <td>0.141914</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>0.851125</td>\n",
       "      <td>0.221673</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.845381</td>\n",
       "      <td>0.143963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1344</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.886098</td>\n",
       "      <td>0.155842</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.760499</td>\n",
       "      <td>0.151753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.824691</td>\n",
       "      <td>0.182646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.868993</td>\n",
       "      <td>0.095461</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.872308</td>\n",
       "      <td>0.172360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1345</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.961006</td>\n",
       "      <td>0.336347</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098052</td>\n",
       "      <td>0.225527</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.983500</td>\n",
       "      <td>0.274123</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>1.088604</td>\n",
       "      <td>0.220189</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.729422</td>\n",
       "      <td>0.147840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    participant_id  resp.corr_mean.incongruent.glad.adult  \\\n",
       "0             1203                                    1.0   \n",
       "1             1204                                    0.9   \n",
       "2             1205                                    1.0   \n",
       "3             1206                                    1.0   \n",
       "4             1207                                    1.0   \n",
       "..             ...                                    ...   \n",
       "83            1319                                    1.0   \n",
       "84            1326                                    1.0   \n",
       "85            1337                                    1.0   \n",
       "86            1344                                    1.0   \n",
       "87            1345                                    1.0   \n",
       "\n",
       "    resp.corr_std.incongruent.glad.adult  resp.rt_mean.incongruent.glad.adult  \\\n",
       "0                               0.000000                             0.834702   \n",
       "1                               0.141421                             0.631355   \n",
       "2                               0.000000                             0.855834   \n",
       "3                               0.000000                             0.734624   \n",
       "4                               0.000000                             0.936832   \n",
       "..                                   ...                                  ...   \n",
       "83                              0.000000                             0.968130   \n",
       "84                              0.000000                             0.844349   \n",
       "85                              0.000000                             0.744503   \n",
       "86                              0.000000                             0.886098   \n",
       "87                              0.000000                             0.961006   \n",
       "\n",
       "    resp.rt_std.incongruent.glad.adult  \\\n",
       "0                             0.143169   \n",
       "1                             0.123766   \n",
       "2                             0.194823   \n",
       "3                             0.079265   \n",
       "4                             0.195431   \n",
       "..                                 ...   \n",
       "83                            0.235274   \n",
       "84                            0.111004   \n",
       "85                            0.144672   \n",
       "86                            0.155842   \n",
       "87                            0.336347   \n",
       "\n",
       "    resp.corr_mean.incongruent.trist.adult  \\\n",
       "0                                      0.9   \n",
       "1                                      1.0   \n",
       "2                                      1.0   \n",
       "3                                      1.0   \n",
       "4                                      1.0   \n",
       "..                                     ...   \n",
       "83                                     1.0   \n",
       "84                                     1.0   \n",
       "85                                     1.0   \n",
       "86                                     1.0   \n",
       "87                                     1.0   \n",
       "\n",
       "    resp.corr_std.incongruent.trist.adult  \\\n",
       "0                                0.141421   \n",
       "1                                0.000000   \n",
       "2                                0.000000   \n",
       "3                                0.000000   \n",
       "4                                0.000000   \n",
       "..                                    ...   \n",
       "83                               0.000000   \n",
       "84                               0.000000   \n",
       "85                               0.000000   \n",
       "86                               0.000000   \n",
       "87                               0.000000   \n",
       "\n",
       "    resp.rt_mean.incongruent.trist.adult  resp.rt_std.incongruent.trist.adult  \\\n",
       "0                               0.925454                             0.278187   \n",
       "1                               0.651777                             0.033001   \n",
       "2                               0.771844                             0.109109   \n",
       "3                               0.852039                             0.151549   \n",
       "4                               0.804329                             0.208366   \n",
       "..                                   ...                                  ...   \n",
       "83                              0.879301                             0.067024   \n",
       "84                              0.978748                             0.102694   \n",
       "85                              0.839017                             0.099145   \n",
       "86                              0.760499                             0.151753   \n",
       "87                              1.098052                             0.225527   \n",
       "\n",
       "    resp.corr_mean.incongruent.neu.adult  ...  \\\n",
       "0                                    1.0  ...   \n",
       "1                                    1.0  ...   \n",
       "2                                    1.0  ...   \n",
       "3                                    1.0  ...   \n",
       "4                                    1.0  ...   \n",
       "..                                   ...  ...   \n",
       "83                                   1.0  ...   \n",
       "84                                   1.0  ...   \n",
       "85                                   1.0  ...   \n",
       "86                                   1.0  ...   \n",
       "87                                   1.0  ...   \n",
       "\n",
       "    resp.rt_mean.congruent.glad.baby  resp.rt_std.congruent.glad.baby  \\\n",
       "0                           0.854535                         0.232593   \n",
       "1                           0.704160                         0.173085   \n",
       "2                           0.850434                         0.135730   \n",
       "3                           0.820891                         0.107333   \n",
       "4                           0.935145                         0.187571   \n",
       "..                               ...                              ...   \n",
       "83                          1.073240                         0.315628   \n",
       "84                          0.852385                         0.072896   \n",
       "85                          0.760030                         0.141914   \n",
       "86                          0.824691                         0.182646   \n",
       "87                          0.983500                         0.274123   \n",
       "\n",
       "    resp.corr_mean.congruent.trist.baby  resp.corr_std.congruent.trist.baby  \\\n",
       "0                              0.958333                            0.058926   \n",
       "1                              0.958333                            0.058926   \n",
       "2                              0.916667                            0.117851   \n",
       "3                              1.000000                            0.000000   \n",
       "4                              0.958333                            0.058926   \n",
       "..                                  ...                                 ...   \n",
       "83                             0.875000                            0.176777   \n",
       "84                             0.958333                            0.058926   \n",
       "85                             0.958333                            0.058926   \n",
       "86                             1.000000                            0.000000   \n",
       "87                             0.958333                            0.058926   \n",
       "\n",
       "    resp.rt_mean.congruent.trist.baby  resp.rt_std.congruent.trist.baby  \\\n",
       "0                            0.924575                          0.271926   \n",
       "1                            0.581735                          0.123698   \n",
       "2                            0.863746                          0.138510   \n",
       "3                            0.830719                          0.146121   \n",
       "4                            0.885868                          0.133503   \n",
       "..                                ...                               ...   \n",
       "83                           1.108588                          0.372499   \n",
       "84                           0.949973                          0.147841   \n",
       "85                           0.851125                          0.221673   \n",
       "86                           0.868993                          0.095461   \n",
       "87                           1.088604                          0.220189   \n",
       "\n",
       "    resp.corr_mean.congruent.neu.baby  resp.corr_std.congruent.neu.baby  \\\n",
       "0                            0.958333                          0.058926   \n",
       "1                            1.000000                          0.000000   \n",
       "2                            0.958333                          0.058926   \n",
       "3                            1.000000                          0.000000   \n",
       "4                            0.875000                          0.058926   \n",
       "..                                ...                               ...   \n",
       "83                           1.000000                          0.000000   \n",
       "84                           0.958333                          0.058926   \n",
       "85                           1.000000                          0.000000   \n",
       "86                           1.000000                          0.000000   \n",
       "87                           1.000000                          0.000000   \n",
       "\n",
       "    resp.rt_mean.congruent.neu.baby  resp.rt_std.congruent.neu.baby  \n",
       "0                          1.000787                        0.318872  \n",
       "1                          0.580491                        0.083795  \n",
       "2                          0.794245                        0.176831  \n",
       "3                          0.744530                        0.085321  \n",
       "4                          0.743572                        0.095836  \n",
       "..                              ...                             ...  \n",
       "83                         0.797543                        0.135086  \n",
       "84                         0.858571                        0.120270  \n",
       "85                         0.845381                        0.143963  \n",
       "86                         0.872308                        0.172360  \n",
       "87                         0.729422                        0.147840  \n",
       "\n",
       "[88 rows x 49 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2cc0893-d7d9-46c4-9e0c-c6a5aa953b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "participant_id                              int64\n",
       "resp.corr_mean.incongruent.glad.adult     float64\n",
       "resp.corr_std.incongruent.glad.adult      float64\n",
       "resp.rt_mean.incongruent.glad.adult       float64\n",
       "resp.rt_std.incongruent.glad.adult        float64\n",
       "resp.corr_mean.incongruent.trist.adult    float64\n",
       "resp.corr_std.incongruent.trist.adult     float64\n",
       "resp.rt_mean.incongruent.trist.adult      float64\n",
       "resp.rt_std.incongruent.trist.adult       float64\n",
       "resp.corr_mean.incongruent.neu.adult      float64\n",
       "resp.corr_std.incongruent.neu.adult       float64\n",
       "resp.rt_mean.incongruent.neu.adult        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm types look right\n",
    "stroop_df.dtypes.head(12)\n",
    "emo_df.dtypes.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "128bab90-c1ab-479a-8ce6-fcef9923013c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "participant_id                           0\n",
       "resp.corr_mean.incongruent.glad.baby     0\n",
       "resp.rt_mean.incongruent.glad.baby       0\n",
       "resp.rt_std.incongruent.glad.baby        0\n",
       "resp.corr_mean.incongruent.trist.baby    0\n",
       "resp.corr_std.incongruent.trist.baby     0\n",
       "resp.rt_mean.incongruent.trist.baby      0\n",
       "resp.rt_std.incongruent.trist.baby       0\n",
       "resp.corr_mean.incongruent.neu.baby      0\n",
       "resp.corr_std.incongruent.neu.baby       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See if any NaNs appeared after conversion (e.g., non-numeric text)\n",
    "stroop_df.isna().sum().sort_values(ascending=False).head(10)\n",
    "emo_df.isna().sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "979e1a5c-0fcf-48a9-9aea-a0de54b7478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroop_df[\"participant_id\"] = stroop_df[\"participant_id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e63f222-e87b-444f-bdc0-2fd2d2cf2470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>resp.corr_mean.incongruent.glad.adult</th>\n",
       "      <th>resp.corr_std.incongruent.glad.adult</th>\n",
       "      <th>resp.rt_mean.incongruent.glad.adult</th>\n",
       "      <th>resp.rt_std.incongruent.glad.adult</th>\n",
       "      <th>resp.corr_mean.incongruent.trist.adult</th>\n",
       "      <th>resp.corr_std.incongruent.trist.adult</th>\n",
       "      <th>resp.rt_mean.incongruent.trist.adult</th>\n",
       "      <th>resp.rt_std.incongruent.trist.adult</th>\n",
       "      <th>resp.corr_mean.incongruent.neu.adult</th>\n",
       "      <th>...</th>\n",
       "      <th>resp.rt_mean.congruent.glad.baby</th>\n",
       "      <th>resp.rt_std.congruent.glad.baby</th>\n",
       "      <th>resp.corr_mean.congruent.trist.baby</th>\n",
       "      <th>resp.corr_std.congruent.trist.baby</th>\n",
       "      <th>resp.rt_mean.congruent.trist.baby</th>\n",
       "      <th>resp.rt_std.congruent.trist.baby</th>\n",
       "      <th>resp.corr_mean.congruent.neu.baby</th>\n",
       "      <th>resp.corr_std.congruent.neu.baby</th>\n",
       "      <th>resp.rt_mean.congruent.neu.baby</th>\n",
       "      <th>resp.rt_std.congruent.neu.baby</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1266.170455</td>\n",
       "      <td>0.965909</td>\n",
       "      <td>0.032141</td>\n",
       "      <td>0.839050</td>\n",
       "      <td>0.162535</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>0.038569</td>\n",
       "      <td>0.876550</td>\n",
       "      <td>0.181659</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.839282</td>\n",
       "      <td>0.170302</td>\n",
       "      <td>0.936080</td>\n",
       "      <td>0.040846</td>\n",
       "      <td>0.905969</td>\n",
       "      <td>0.200276</td>\n",
       "      <td>0.974432</td>\n",
       "      <td>0.020088</td>\n",
       "      <td>0.799207</td>\n",
       "      <td>0.145589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>41.831688</td>\n",
       "      <td>0.116349</td>\n",
       "      <td>0.073429</td>\n",
       "      <td>0.118615</td>\n",
       "      <td>0.087137</td>\n",
       "      <td>0.110570</td>\n",
       "      <td>0.085035</td>\n",
       "      <td>0.148745</td>\n",
       "      <td>0.116965</td>\n",
       "      <td>0.038428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124196</td>\n",
       "      <td>0.087086</td>\n",
       "      <td>0.176545</td>\n",
       "      <td>0.054607</td>\n",
       "      <td>0.138716</td>\n",
       "      <td>0.084498</td>\n",
       "      <td>0.096522</td>\n",
       "      <td>0.038827</td>\n",
       "      <td>0.124248</td>\n",
       "      <td>0.072272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1203.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.628502</td>\n",
       "      <td>0.036224</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575645</td>\n",
       "      <td>0.033001</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629927</td>\n",
       "      <td>0.062028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.581735</td>\n",
       "      <td>0.076558</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.580491</td>\n",
       "      <td>0.051065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1228.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755970</td>\n",
       "      <td>0.099702</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.781008</td>\n",
       "      <td>0.099415</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.749487</td>\n",
       "      <td>0.112040</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.814037</td>\n",
       "      <td>0.133147</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.715115</td>\n",
       "      <td>0.102361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1262.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.832294</td>\n",
       "      <td>0.143920</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.845678</td>\n",
       "      <td>0.150347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823834</td>\n",
       "      <td>0.148653</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.898520</td>\n",
       "      <td>0.188464</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.782664</td>\n",
       "      <td>0.130587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1301.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.898917</td>\n",
       "      <td>0.200575</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.952084</td>\n",
       "      <td>0.240777</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899481</td>\n",
       "      <td>0.201692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>0.991394</td>\n",
       "      <td>0.248895</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>0.863237</td>\n",
       "      <td>0.168149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1345.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.424264</td>\n",
       "      <td>1.159541</td>\n",
       "      <td>0.501599</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.424264</td>\n",
       "      <td>1.334006</td>\n",
       "      <td>0.780579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.258131</td>\n",
       "      <td>0.657062</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.235702</td>\n",
       "      <td>1.396446</td>\n",
       "      <td>0.487515</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.235702</td>\n",
       "      <td>1.241493</td>\n",
       "      <td>0.490676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       participant_id  resp.corr_mean.incongruent.glad.adult  \\\n",
       "count       88.000000                              88.000000   \n",
       "mean      1266.170455                               0.965909   \n",
       "std         41.831688                               0.116349   \n",
       "min       1203.000000                               0.000000   \n",
       "25%       1228.750000                               1.000000   \n",
       "50%       1262.500000                               1.000000   \n",
       "75%       1301.250000                               1.000000   \n",
       "max       1345.000000                               1.000000   \n",
       "\n",
       "       resp.corr_std.incongruent.glad.adult  \\\n",
       "count                             88.000000   \n",
       "mean                               0.032141   \n",
       "std                                0.073429   \n",
       "min                                0.000000   \n",
       "25%                                0.000000   \n",
       "50%                                0.000000   \n",
       "75%                                0.000000   \n",
       "max                                0.424264   \n",
       "\n",
       "       resp.rt_mean.incongruent.glad.adult  \\\n",
       "count                            88.000000   \n",
       "mean                              0.839050   \n",
       "std                               0.118615   \n",
       "min                               0.628502   \n",
       "25%                               0.755970   \n",
       "50%                               0.832294   \n",
       "75%                               0.898917   \n",
       "max                               1.159541   \n",
       "\n",
       "       resp.rt_std.incongruent.glad.adult  \\\n",
       "count                           88.000000   \n",
       "mean                             0.162535   \n",
       "std                              0.087137   \n",
       "min                              0.036224   \n",
       "25%                              0.099702   \n",
       "50%                              0.143920   \n",
       "75%                              0.200575   \n",
       "max                              0.501599   \n",
       "\n",
       "       resp.corr_mean.incongruent.trist.adult  \\\n",
       "count                               88.000000   \n",
       "mean                                 0.963636   \n",
       "std                                  0.110570   \n",
       "min                                  0.100000   \n",
       "25%                                  1.000000   \n",
       "50%                                  1.000000   \n",
       "75%                                  1.000000   \n",
       "max                                  1.000000   \n",
       "\n",
       "       resp.corr_std.incongruent.trist.adult  \\\n",
       "count                              88.000000   \n",
       "mean                                0.038569   \n",
       "std                                 0.085035   \n",
       "min                                 0.000000   \n",
       "25%                                 0.000000   \n",
       "50%                                 0.000000   \n",
       "75%                                 0.000000   \n",
       "max                                 0.424264   \n",
       "\n",
       "       resp.rt_mean.incongruent.trist.adult  \\\n",
       "count                             88.000000   \n",
       "mean                               0.876550   \n",
       "std                                0.148745   \n",
       "min                                0.575645   \n",
       "25%                                0.781008   \n",
       "50%                                0.845678   \n",
       "75%                                0.952084   \n",
       "max                                1.334006   \n",
       "\n",
       "       resp.rt_std.incongruent.trist.adult  \\\n",
       "count                            88.000000   \n",
       "mean                              0.181659   \n",
       "std                               0.116965   \n",
       "min                               0.033001   \n",
       "25%                               0.099415   \n",
       "50%                               0.150347   \n",
       "75%                               0.240777   \n",
       "max                               0.780579   \n",
       "\n",
       "       resp.corr_mean.incongruent.neu.adult  ...  \\\n",
       "count                             88.000000  ...   \n",
       "mean                               0.979167  ...   \n",
       "std                                0.038428  ...   \n",
       "min                                0.833333  ...   \n",
       "25%                                1.000000  ...   \n",
       "50%                                1.000000  ...   \n",
       "75%                                1.000000  ...   \n",
       "max                                1.000000  ...   \n",
       "\n",
       "       resp.rt_mean.congruent.glad.baby  resp.rt_std.congruent.glad.baby  \\\n",
       "count                         88.000000                        88.000000   \n",
       "mean                           0.839282                         0.170302   \n",
       "std                            0.124196                         0.087086   \n",
       "min                            0.629927                         0.062028   \n",
       "25%                            0.749487                         0.112040   \n",
       "50%                            0.823834                         0.148653   \n",
       "75%                            0.899481                         0.201692   \n",
       "max                            1.258131                         0.657062   \n",
       "\n",
       "       resp.corr_mean.congruent.trist.baby  \\\n",
       "count                            88.000000   \n",
       "mean                              0.936080   \n",
       "std                               0.176545   \n",
       "min                               0.000000   \n",
       "25%                               0.916667   \n",
       "50%                               1.000000   \n",
       "75%                               1.000000   \n",
       "max                               1.000000   \n",
       "\n",
       "       resp.corr_std.congruent.trist.baby  resp.rt_mean.congruent.trist.baby  \\\n",
       "count                           88.000000                          88.000000   \n",
       "mean                             0.040846                           0.905969   \n",
       "std                              0.054607                           0.138716   \n",
       "min                              0.000000                           0.581735   \n",
       "25%                              0.000000                           0.814037   \n",
       "50%                              0.000000                           0.898520   \n",
       "75%                              0.058926                           0.991394   \n",
       "max                              0.235702                           1.396446   \n",
       "\n",
       "       resp.rt_std.congruent.trist.baby  resp.corr_mean.congruent.neu.baby  \\\n",
       "count                         88.000000                          88.000000   \n",
       "mean                           0.200276                           0.974432   \n",
       "std                            0.084498                           0.096522   \n",
       "min                            0.076558                           0.125000   \n",
       "25%                            0.133147                           0.958333   \n",
       "50%                            0.188464                           1.000000   \n",
       "75%                            0.248895                           1.000000   \n",
       "max                            0.487515                           1.000000   \n",
       "\n",
       "       resp.corr_std.congruent.neu.baby  resp.rt_mean.congruent.neu.baby  \\\n",
       "count                         88.000000                        88.000000   \n",
       "mean                           0.020088                         0.799207   \n",
       "std                            0.038827                         0.124248   \n",
       "min                            0.000000                         0.580491   \n",
       "25%                            0.000000                         0.715115   \n",
       "50%                            0.000000                         0.782664   \n",
       "75%                            0.058926                         0.863237   \n",
       "max                            0.235702                         1.241493   \n",
       "\n",
       "       resp.rt_std.congruent.neu.baby  \n",
       "count                       88.000000  \n",
       "mean                         0.145589  \n",
       "std                          0.072272  \n",
       "min                          0.051065  \n",
       "25%                          0.102361  \n",
       "50%                          0.130587  \n",
       "75%                          0.168149  \n",
       "max                          0.490676  \n",
       "\n",
       "[8 rows x 49 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07980200-d825-4260-afec-c416306e6622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respc.corr_mean.incongruent</th>\n",
       "      <th>respc.corr_std.incongruent</th>\n",
       "      <th>respc.rt_mean.incongruent</th>\n",
       "      <th>respc.rt_std.incongruent</th>\n",
       "      <th>respc.corr_mean.congruent</th>\n",
       "      <th>respc.corr_std.congruent</th>\n",
       "      <th>respc.rt_mean.congruent</th>\n",
       "      <th>respc.rt_std.congruent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.976894</td>\n",
       "      <td>0.048384</td>\n",
       "      <td>0.853074</td>\n",
       "      <td>0.251353</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.090096</td>\n",
       "      <td>0.992118</td>\n",
       "      <td>0.310842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.035145</td>\n",
       "      <td>0.072897</td>\n",
       "      <td>0.212774</td>\n",
       "      <td>0.145106</td>\n",
       "      <td>0.152543</td>\n",
       "      <td>0.109454</td>\n",
       "      <td>0.291179</td>\n",
       "      <td>0.164527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.481819</td>\n",
       "      <td>0.070943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.596278</td>\n",
       "      <td>0.095487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.703588</td>\n",
       "      <td>0.154486</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.782203</td>\n",
       "      <td>0.207822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.786450</td>\n",
       "      <td>0.227217</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.074536</td>\n",
       "      <td>0.896302</td>\n",
       "      <td>0.255542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.074536</td>\n",
       "      <td>0.965770</td>\n",
       "      <td>0.297830</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.149071</td>\n",
       "      <td>1.177996</td>\n",
       "      <td>0.363376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.314894</td>\n",
       "      <td>1.598872</td>\n",
       "      <td>1.007927</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.514220</td>\n",
       "      <td>2.110163</td>\n",
       "      <td>1.002553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       respc.corr_mean.incongruent  respc.corr_std.incongruent  \\\n",
       "count                    88.000000                   88.000000   \n",
       "mean                      0.976894                    0.048384   \n",
       "std                       0.035145                    0.072897   \n",
       "min                       0.833333                    0.000000   \n",
       "25%                       0.966667                    0.000000   \n",
       "50%                       1.000000                    0.000000   \n",
       "75%                       1.000000                    0.074536   \n",
       "max                       1.000000                    0.314894   \n",
       "\n",
       "       respc.rt_mean.incongruent  respc.rt_std.incongruent  \\\n",
       "count                  88.000000                 88.000000   \n",
       "mean                    0.853074                  0.251353   \n",
       "std                     0.212774                  0.145106   \n",
       "min                     0.481819                  0.070943   \n",
       "25%                     0.703588                  0.154486   \n",
       "50%                     0.786450                  0.227217   \n",
       "75%                     0.965770                  0.297830   \n",
       "max                     1.598872                  1.007927   \n",
       "\n",
       "       respc.corr_mean.congruent  respc.corr_std.congruent  \\\n",
       "count                  88.000000                 88.000000   \n",
       "mean                    0.933333                  0.090096   \n",
       "std                     0.152543                  0.109454   \n",
       "min                     0.000000                  0.000000   \n",
       "25%                     0.933333                  0.000000   \n",
       "50%                     0.966667                  0.074536   \n",
       "75%                     1.000000                  0.149071   \n",
       "max                     1.000000                  0.514220   \n",
       "\n",
       "       respc.rt_mean.congruent  respc.rt_std.congruent  \n",
       "count                88.000000               88.000000  \n",
       "mean                  0.992118                0.310842  \n",
       "std                   0.291179                0.164527  \n",
       "min                   0.596278                0.095487  \n",
       "25%                   0.782203                0.207822  \n",
       "50%                   0.896302                0.255542  \n",
       "75%                   1.177996                0.363376  \n",
       "max                   2.110163                1.002553  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stroop_df.describe ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bffc9f63-2964-4728-a24f-a0b3776f9443",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"C:/github/master_calculations/data/p_details.xlsx\")  # sjekk at navnet og endelsen stemmer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cb7b58d-9c8b-4fa5-9903-a965ca6c052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "details_df = pd.read_excel(path, engine=\"openpyxl\", header=0, decimal=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d37bda17-2b57-43df-a4ff-15fa3d11078f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Session 1-Interview</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Session 2- EEG</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>Session 3- MRI (as applicable)</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>participant_ID</td>\n",
       "      <td>email</td>\n",
       "      <td>age</td>\n",
       "      <td>sex</td>\n",
       "      <td>Interview_date</td>\n",
       "      <td>Attended (y/n)</td>\n",
       "      <td>tester_initials</td>\n",
       "      <td>session_notes</td>\n",
       "      <td>EEG_date</td>\n",
       "      <td>Attended (y/n)</td>\n",
       "      <td>Tester Initials</td>\n",
       "      <td>cap_size</td>\n",
       "      <td>distance_from_screen_cm</td>\n",
       "      <td>MMSE_score</td>\n",
       "      <td>session_notes</td>\n",
       "      <td>MRI_date</td>\n",
       "      <td>Notes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>eirik.m.rosoy@gmail.com</td>\n",
       "      <td>49</td>\n",
       "      <td>m</td>\n",
       "      <td>2022-02-01 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>JR/VD</td>\n",
       "      <td>Q20: accent in French must be only moderate, n...</td>\n",
       "      <td>2022-02-01 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>GB7JR/VD</td>\n",
       "      <td>59</td>\n",
       "      <td>-</td>\n",
       "      <td>30</td>\n",
       "      <td>70 minutes ENC to RET</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>gro.kiil.larsen@gmail.com</td>\n",
       "      <td>45</td>\n",
       "      <td>f</td>\n",
       "      <td>2022-02-04 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>TV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-07 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>HT/SZ/TV</td>\n",
       "      <td>59</td>\n",
       "      <td>-</td>\n",
       "      <td>29</td>\n",
       "      <td>Isssue with triggers during ENC portion of epi...</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003</td>\n",
       "      <td>beathe.thomsen@tffk.no</td>\n",
       "      <td>48</td>\n",
       "      <td>f</td>\n",
       "      <td>2022-02-10 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>HT</td>\n",
       "      <td>Q20: accents should be: English - very strong,...</td>\n",
       "      <td>2022-02-17 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>HT/SZ</td>\n",
       "      <td>55</td>\n",
       "      <td>-</td>\n",
       "      <td>30</td>\n",
       "      <td>87 minutes ENC to RET Had issues with datamous...</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>sapmi77@yahoo.no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-17 00:00:00</td>\n",
       "      <td>n</td>\n",
       "      <td>JR</td>\n",
       "      <td>NO SHOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>1346</td>\n",
       "      <td>k-lien@live.no</td>\n",
       "      <td>44</td>\n",
       "      <td>m</td>\n",
       "      <td>2024-11-06 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>KE</td>\n",
       "      <td>noisy background (UiT common area), still student</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>1347</td>\n",
       "      <td>siwmb@online.no</td>\n",
       "      <td>56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-11-13 00:00:00</td>\n",
       "      <td>n</td>\n",
       "      <td>KE</td>\n",
       "      <td>rescheduled</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>1348</td>\n",
       "      <td>lenelias82@gmail.com</td>\n",
       "      <td>42</td>\n",
       "      <td>f</td>\n",
       "      <td>2024-11-14 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>KE</td>\n",
       "      <td>metal object in the hip (?)</td>\n",
       "      <td>26.11.24</td>\n",
       "      <td>Y</td>\n",
       "      <td>AG/FG</td>\n",
       "      <td>53</td>\n",
       "      <td>59</td>\n",
       "      <td>28</td>\n",
       "      <td>65min enc to ret</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>1349</td>\n",
       "      <td>torbjorn.nordmo@gmail.com</td>\n",
       "      <td>59</td>\n",
       "      <td>m</td>\n",
       "      <td>2024-12-03 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>KE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>1350</td>\n",
       "      <td>tove_kristine@hotmail.com</td>\n",
       "      <td>52</td>\n",
       "      <td>f</td>\n",
       "      <td>2024-12-05 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>KE</td>\n",
       "      <td>metallic object in the shoulder - also, someho...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>352 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0                 Unnamed: 1 Unnamed: 2 Unnamed: 3  \\\n",
       "0    participant_ID                      email        age        sex   \n",
       "1              1001    eirik.m.rosoy@gmail.com         49          m   \n",
       "2              1002  gro.kiil.larsen@gmail.com         45          f   \n",
       "3              1003     beathe.thomsen@tffk.no         48          f   \n",
       "4              1004           sapmi77@yahoo.no        NaN        NaN   \n",
       "..              ...                        ...        ...        ...   \n",
       "347            1346             k-lien@live.no         44          m   \n",
       "348            1347            siwmb@online.no         56        NaN   \n",
       "349            1348       lenelias82@gmail.com         42          f   \n",
       "350            1349  torbjorn.nordmo@gmail.com         59          m   \n",
       "351            1350  tove_kristine@hotmail.com         52          f   \n",
       "\n",
       "     Session 1-Interview      Unnamed: 5       Unnamed: 6  \\\n",
       "0         Interview_date  Attended (y/n)  tester_initials   \n",
       "1    2022-02-01 00:00:00               y            JR/VD   \n",
       "2    2022-02-04 00:00:00               y               TV   \n",
       "3    2022-02-10 00:00:00               y               HT   \n",
       "4    2022-02-17 00:00:00               n              JR    \n",
       "..                   ...             ...              ...   \n",
       "347  2024-11-06 00:00:00               y               KE   \n",
       "348  2024-11-13 00:00:00               n               KE   \n",
       "349  2024-11-14 00:00:00               y               KE   \n",
       "350  2024-12-03 00:00:00               y               KE   \n",
       "351  2024-12-05 00:00:00               y               KE   \n",
       "\n",
       "                                            Unnamed: 7      Session 2- EEG   \\\n",
       "0                                        session_notes             EEG_date   \n",
       "1    Q20: accent in French must be only moderate, n...  2022-02-01 00:00:00   \n",
       "2                                                  NaN  2022-02-07 00:00:00   \n",
       "3    Q20: accents should be: English - very strong,...  2022-02-17 00:00:00   \n",
       "4                                              NO SHOW                  NaN   \n",
       "..                                                 ...                  ...   \n",
       "347  noisy background (UiT common area), still student                  NaN   \n",
       "348                                        rescheduled                  NaN   \n",
       "349                        metal object in the hip (?)             26.11.24   \n",
       "350                                                NaN                  NaN   \n",
       "351  metallic object in the shoulder - also, someho...                  NaN   \n",
       "\n",
       "         Unnamed: 9      Unnamed: 10 Unnamed: 11              Unnamed: 12  \\\n",
       "0    Attended (y/n)  Tester Initials    cap_size  distance_from_screen_cm   \n",
       "1                 y         GB7JR/VD          59                        -   \n",
       "2                 y         HT/SZ/TV          59                        -   \n",
       "3                 y            HT/SZ          55                        -   \n",
       "4               NaN              NaN         NaN                      NaN   \n",
       "..              ...              ...         ...                      ...   \n",
       "347             NaN              NaN         NaN                      NaN   \n",
       "348             NaN              NaN         NaN                      NaN   \n",
       "349               Y            AG/FG          53                       59   \n",
       "350             NaN              NaN         NaN                      NaN   \n",
       "351             NaN              NaN         NaN                      NaN   \n",
       "\n",
       "    Unnamed: 13                                        Unnamed: 14  \\\n",
       "0    MMSE_score                                      session_notes   \n",
       "1            30                              70 minutes ENC to RET   \n",
       "2            29  Isssue with triggers during ENC portion of epi...   \n",
       "3            30  87 minutes ENC to RET Had issues with datamous...   \n",
       "4           NaN                                                NaN   \n",
       "..          ...                                                ...   \n",
       "347         NaN                                                NaN   \n",
       "348         NaN                                                NaN   \n",
       "349          28                                  65min enc to ret    \n",
       "350         NaN                                                NaN   \n",
       "351         NaN                                                NaN   \n",
       "\n",
       "    Session 3- MRI (as applicable) Unnamed: 16  \n",
       "0                         MRI_date       Notes  \n",
       "1                           TBC(?)         NaN  \n",
       "2                           TBC(?)         NaN  \n",
       "3                           TBC(?)         NaN  \n",
       "4                           TBC(?)         NaN  \n",
       "..                             ...         ...  \n",
       "347                            NaN         NaN  \n",
       "348                            NaN         NaN  \n",
       "349                            NaN         NaN  \n",
       "350                            NaN         NaN  \n",
       "351                            NaN         NaN  \n",
       "\n",
       "[352 rows x 17 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "181a6842-993e-40da-80a4-f6a64239dbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Session 1-Interview', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Session 2- EEG ', 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14', 'Session 3- MRI (as applicable)', 'Unnamed: 16']\n"
     ]
    }
   ],
   "source": [
    "print(details_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "deb758fc-a1fb-486a-b6c7-d098c5ff39b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Session 1-Interview</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Session 2- EEG</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>Session 3- MRI (as applicable)</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>participant_ID</td>\n",
       "      <td>email</td>\n",
       "      <td>age</td>\n",
       "      <td>sex</td>\n",
       "      <td>Interview_date</td>\n",
       "      <td>Attended (y/n)</td>\n",
       "      <td>tester_initials</td>\n",
       "      <td>session_notes</td>\n",
       "      <td>EEG_date</td>\n",
       "      <td>Attended (y/n)</td>\n",
       "      <td>Tester Initials</td>\n",
       "      <td>cap_size</td>\n",
       "      <td>distance_from_screen_cm</td>\n",
       "      <td>MMSE_score</td>\n",
       "      <td>session_notes</td>\n",
       "      <td>MRI_date</td>\n",
       "      <td>Notes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>eirik.m.rosoy@gmail.com</td>\n",
       "      <td>49</td>\n",
       "      <td>m</td>\n",
       "      <td>2022-02-01 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>JR/VD</td>\n",
       "      <td>Q20: accent in French must be only moderate, n...</td>\n",
       "      <td>2022-02-01 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>GB7JR/VD</td>\n",
       "      <td>59</td>\n",
       "      <td>-</td>\n",
       "      <td>30</td>\n",
       "      <td>70 minutes ENC to RET</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>gro.kiil.larsen@gmail.com</td>\n",
       "      <td>45</td>\n",
       "      <td>f</td>\n",
       "      <td>2022-02-04 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>TV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-07 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>HT/SZ/TV</td>\n",
       "      <td>59</td>\n",
       "      <td>-</td>\n",
       "      <td>29</td>\n",
       "      <td>Isssue with triggers during ENC portion of epi...</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003</td>\n",
       "      <td>beathe.thomsen@tffk.no</td>\n",
       "      <td>48</td>\n",
       "      <td>f</td>\n",
       "      <td>2022-02-10 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>HT</td>\n",
       "      <td>Q20: accents should be: English - very strong,...</td>\n",
       "      <td>2022-02-17 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>HT/SZ</td>\n",
       "      <td>55</td>\n",
       "      <td>-</td>\n",
       "      <td>30</td>\n",
       "      <td>87 minutes ENC to RET Had issues with datamous...</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>sapmi77@yahoo.no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-17 00:00:00</td>\n",
       "      <td>n</td>\n",
       "      <td>JR</td>\n",
       "      <td>NO SHOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1005</td>\n",
       "      <td>eline.holdo@gmail.com</td>\n",
       "      <td>46</td>\n",
       "      <td>f</td>\n",
       "      <td>2022-02-18 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>JR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-21 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>HT/JR</td>\n",
       "      <td>57</td>\n",
       "      <td>-</td>\n",
       "      <td>30</td>\n",
       "      <td>55 minutes ENC to RET</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1006</td>\n",
       "      <td>bjorg.hestvik@gmail.com</td>\n",
       "      <td>65</td>\n",
       "      <td>f</td>\n",
       "      <td>2022-02-21 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>TV/SZ</td>\n",
       "      <td>SZ Training session</td>\n",
       "      <td>2022-02-28 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>HT/JR</td>\n",
       "      <td>55</td>\n",
       "      <td>-</td>\n",
       "      <td>29</td>\n",
       "      <td>Stopped during practice ENC, because EEG was v...</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1007</td>\n",
       "      <td>stine@fam-barlindhaug.no</td>\n",
       "      <td>54</td>\n",
       "      <td>f</td>\n",
       "      <td>2022-02-22 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>TV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-05-18 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>HT/AK/MF</td>\n",
       "      <td>57</td>\n",
       "      <td>56</td>\n",
       "      <td>30</td>\n",
       "      <td>62 minutes ENC to RET</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1008</td>\n",
       "      <td>helge.matland@gmail.com</td>\n",
       "      <td>67</td>\n",
       "      <td>m</td>\n",
       "      <td>2022-02-22 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>JR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-25 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>HT/VD</td>\n",
       "      <td>59</td>\n",
       "      <td>-</td>\n",
       "      <td>29</td>\n",
       "      <td>59 minutes ENC to RET. Ppt said he forgot whic...</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1009</td>\n",
       "      <td>i-hemmin@online.no</td>\n",
       "      <td>56</td>\n",
       "      <td>f</td>\n",
       "      <td>2022-02-22 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>JR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-03-08 00:00:00</td>\n",
       "      <td>y</td>\n",
       "      <td>AG/VD</td>\n",
       "      <td>57</td>\n",
       "      <td>61</td>\n",
       "      <td>29</td>\n",
       "      <td>66 minutes from ENC to RET. P8 &amp; P10 might hav...</td>\n",
       "      <td>TBC(?)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                 Unnamed: 1 Unnamed: 2 Unnamed: 3  \\\n",
       "0  participant_ID                      email        age        sex   \n",
       "1            1001    eirik.m.rosoy@gmail.com         49          m   \n",
       "2            1002  gro.kiil.larsen@gmail.com         45          f   \n",
       "3            1003     beathe.thomsen@tffk.no         48          f   \n",
       "4            1004           sapmi77@yahoo.no        NaN        NaN   \n",
       "5            1005      eline.holdo@gmail.com         46          f   \n",
       "6            1006    bjorg.hestvik@gmail.com         65          f   \n",
       "7            1007   stine@fam-barlindhaug.no         54          f   \n",
       "8            1008    helge.matland@gmail.com         67          m   \n",
       "9            1009         i-hemmin@online.no         56          f   \n",
       "\n",
       "   Session 1-Interview      Unnamed: 5       Unnamed: 6  \\\n",
       "0       Interview_date  Attended (y/n)  tester_initials   \n",
       "1  2022-02-01 00:00:00               y            JR/VD   \n",
       "2  2022-02-04 00:00:00               y               TV   \n",
       "3  2022-02-10 00:00:00               y               HT   \n",
       "4  2022-02-17 00:00:00               n              JR    \n",
       "5  2022-02-18 00:00:00               y               JR   \n",
       "6  2022-02-21 00:00:00               y            TV/SZ   \n",
       "7  2022-02-22 00:00:00               y               TV   \n",
       "8  2022-02-22 00:00:00               y               JR   \n",
       "9  2022-02-22 00:00:00               y               JR   \n",
       "\n",
       "                                          Unnamed: 7      Session 2- EEG   \\\n",
       "0                                      session_notes             EEG_date   \n",
       "1  Q20: accent in French must be only moderate, n...  2022-02-01 00:00:00   \n",
       "2                                                NaN  2022-02-07 00:00:00   \n",
       "3  Q20: accents should be: English - very strong,...  2022-02-17 00:00:00   \n",
       "4                                            NO SHOW                  NaN   \n",
       "5                                                NaN  2022-02-21 00:00:00   \n",
       "6                                SZ Training session  2022-02-28 00:00:00   \n",
       "7                                                NaN  2022-05-18 00:00:00   \n",
       "8                                                NaN  2022-02-25 00:00:00   \n",
       "9                                                NaN  2022-03-08 00:00:00   \n",
       "\n",
       "       Unnamed: 9      Unnamed: 10 Unnamed: 11              Unnamed: 12  \\\n",
       "0  Attended (y/n)  Tester Initials    cap_size  distance_from_screen_cm   \n",
       "1               y         GB7JR/VD          59                        -   \n",
       "2               y         HT/SZ/TV          59                        -   \n",
       "3               y            HT/SZ          55                        -   \n",
       "4             NaN              NaN         NaN                      NaN   \n",
       "5               y            HT/JR          57                        -   \n",
       "6               y            HT/JR          55                        -   \n",
       "7               y         HT/AK/MF          57                       56   \n",
       "8               y            HT/VD          59                        -   \n",
       "9               y            AG/VD          57                       61   \n",
       "\n",
       "  Unnamed: 13                                        Unnamed: 14  \\\n",
       "0  MMSE_score                                      session_notes   \n",
       "1          30                              70 minutes ENC to RET   \n",
       "2          29  Isssue with triggers during ENC portion of epi...   \n",
       "3          30  87 minutes ENC to RET Had issues with datamous...   \n",
       "4         NaN                                                NaN   \n",
       "5          30                              55 minutes ENC to RET   \n",
       "6          29  Stopped during practice ENC, because EEG was v...   \n",
       "7          30                              62 minutes ENC to RET   \n",
       "8          29  59 minutes ENC to RET. Ppt said he forgot whic...   \n",
       "9          29  66 minutes from ENC to RET. P8 & P10 might hav...   \n",
       "\n",
       "  Session 3- MRI (as applicable) Unnamed: 16  \n",
       "0                       MRI_date       Notes  \n",
       "1                         TBC(?)         NaN  \n",
       "2                         TBC(?)         NaN  \n",
       "3                         TBC(?)         NaN  \n",
       "4                         TBC(?)         NaN  \n",
       "5                         TBC(?)         NaN  \n",
       "6                         TBC(?)         NaN  \n",
       "7                         TBC(?)         NaN  \n",
       "8                         TBC(?)         NaN  \n",
       "9                         TBC(?)         NaN  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3918470b-228e-4818-a765-f60d8a555e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "details_df = pd.read_excel(\n",
    "    path,\n",
    "    engine=\"openpyxl\",\n",
    "    header=1,   # Bruk rad 2 som kolonnenavn (0 = første rad)\n",
    "    decimal=\",\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d680e07-ef7d-4f98-b6c9-97b1e5d55a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   participant_ID  sex  age\n",
      "0            1001    m   49\n",
      "1            1002    f   45\n",
      "2            1003    f   48\n",
      "3            1004  NaN  NaN\n",
      "4            1005    f   46\n"
     ]
    }
   ],
   "source": [
    "demo_df = details_df[[\"participant_ID\", \"sex\", \"age\"]]\n",
    "print(demo_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a367a9f3-8fad-4cc8-a34c-e6cc7c2d994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging participant details and emotional dataframe, and participant details and stroop dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a26c8a56-a036-4681-bc70-093936ae5810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging participant details and stroop dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be8bc628-70b4-4837-82a3-cdce0886860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first standardize the ID in both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ef85c9d-75eb-44a3-9733-024d769be7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_id(s: pd.Series) -> pd.Series:\n",
    "    return (s.astype(str).str.replace(r\"\\.0$\", \"\", regex=True).str.strip())\n",
    "\n",
    "details_df[\"participant_id\"] = normalize_id(details_df[\"participant_ID\"])\n",
    "stroop_df[\"participant_id\"]  = normalize_id(stroop_df[\"participant_id\"])\n",
    "emo_df[\"participant_id\"]     = normalize_id(emo_df[\"participant_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "56487728-ec6e-411f-8497-b36f307faaf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>1224</td>\n",
       "      <td>f</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>1224</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    participant_id  sex age\n",
       "223           1224    f  32\n",
       "224           1224  NaN  41"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many duplicates?\n",
    "details_keep[\"participant_id\"].duplicated().sum()\n",
    "\n",
    "# inspect which IDs are duplicated\n",
    "dups = details_keep[details_keep[\"participant_id\"].duplicated(keep=False)]\\\n",
    "        .sort_values(\"participant_id\")\n",
    "dups.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "495091b1-2e76-4264-969a-62c19e487af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_age</th>\n",
       "      <th>ages</th>\n",
       "      <th>n_sex</th>\n",
       "      <th>sexes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>participant_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>2</td>\n",
       "      <td>(32, 41)</td>\n",
       "      <td>1</td>\n",
       "      <td>(f,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                n_age      ages  n_sex sexes\n",
       "participant_id                              \n",
       "1224                2  (32, 41)      1  (f,)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conflicts = (details_keep\n",
    "    .groupby(\"participant_id\")\n",
    "    .agg(\n",
    "        n_age=(\"age\",    lambda s: s.dropna().nunique()),\n",
    "        ages=(\"age\",     lambda s: tuple(sorted(s.dropna().unique()))),\n",
    "        n_sex=(\"sex\",    lambda s: s.dropna().nunique()),\n",
    "        sexes=(\"sex\",    lambda s: tuple(sorted(s.dropna().unique())))\n",
    "    )\n",
    "    .query(\"n_age > 1 or n_sex > 1\")\n",
    ")\n",
    "conflicts  # <- manually check these later (e.g., 1224: ages (32, 41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "85bcedaf-a28c-4c41-8c74-16486ace7a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, de-duplicate using \"first non-null\" rule\n",
    "def first_nonnull(s):\n",
    "    s = s.dropna()\n",
    "    return s.iloc[0] if len(s) else pd.NA\n",
    "\n",
    "details_unique = (details_keep\n",
    "    .sort_values([\"participant_id\"])\n",
    "    .groupby(\"participant_id\", as_index=False)\n",
    "    .agg(sex=(\"sex\", first_nonnull),\n",
    "         age=(\"age\", first_nonnull))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2a995a54-c135-4633-af5b-749214e228dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then manually fix participant 1224\n",
    "details_unique.loc[details_unique[\"participant_id\"] == \"1224\", \"age\"] = 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e12b6c72-4203-4fcd-9253-7c0d11a99f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroop_merged = stroop_df.merge(details_unique, on=\"participant_id\", how=\"left\", validate=\"m:1\")\n",
    "emo_merged    = emo_df.merge(details_unique,    on=\"participant_id\", how=\"left\", validate=\"m:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "704ab209-7b4f-4408-bd80-c996c2fdb6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stroop unmatched: 1\n",
      "Emo unmatched:    1\n"
     ]
    }
   ],
   "source": [
    "# check that everyone got matched\n",
    "print(\"Stroop unmatched:\", stroop_merged[\"sex\"].isna().sum())\n",
    "print(\"Emo unmatched:   \", emo_merged[\"sex\"].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "efb8bcf0-09a7-4a0c-8bf5-c00bdf26bffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched in Stroop: ['1284']\n",
      "Unmatched in Emo:    ['1284']\n"
     ]
    }
   ],
   "source": [
    "stroop_unmatched = stroop_merged[stroop_merged[\"sex\"].isna()][\"participant_id\"].unique()\n",
    "emo_unmatched    = emo_merged[emo_merged[\"sex\"].isna()][\"participant_id\"].unique()\n",
    "\n",
    "print(\"Unmatched in Stroop:\", stroop_unmatched)\n",
    "print(\"Unmatched in Emo:   \", emo_unmatched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "52de2868-14a1-42d6-81f9-8f3dd247fe93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>1284</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    participant_id   sex age\n",
       "283           1284  <NA>  53"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details_unique.query(\"participant_id == '1284'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f9439b2a-2c90-45e6-95d7-acee6c51b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing sex for participant 1284\n",
    "details_unique.loc[details_unique[\"participant_id\"] == \"1284\", \"sex\"] = \"f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "75bcce09-4976-47e9-9497-71ffcadb36e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-merge so the correction is included\n",
    "stroop_merged = stroop_df.merge(details_unique, on=\"participant_id\", how=\"left\", validate=\"m:1\")\n",
    "emo_merged    = emo_df.merge(details_unique,    on=\"participant_id\", how=\"left\", validate=\"m:1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "67461bbd-3036-4d02-9733-cd832ff00f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stroop unmatched: 0\n",
      "Emo unmatched:    0\n"
     ]
    }
   ],
   "source": [
    "# Double-check: no unmatched left\n",
    "print(\"Stroop unmatched:\", stroop_merged[\"sex\"].isna().sum())\n",
    "print(\"Emo unmatched:   \", emo_merged[\"sex\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "940f0e18-6cf8-4fa4-b3e3-8a1583fdde46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a7a95630-ba73-4c82-a30a-4fc51767293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_demo(df, label):\n",
    "    \"\"\"Summarize demographics for a dataset\"\"\"\n",
    "    # drop duplicates in case multiple rows per participant\n",
    "    df_unique = df.drop_duplicates(subset=\"participant_id\")\n",
    "\n",
    "    n = len(df_unique)\n",
    "    sex_counts = df_unique[\"sex\"].value_counts(dropna=False).to_dict()\n",
    "    age_mean = df_unique[\"age\"].mean()\n",
    "    age_sd   = df_unique[\"age\"].std()\n",
    "    age_min  = df_unique[\"age\"].min()\n",
    "    age_max  = df_unique[\"age\"].max()\n",
    "\n",
    "    summary = {\n",
    "        \"Dataset\": label,\n",
    "        \"N\": n,\n",
    "        \"Sex counts\": sex_counts,\n",
    "        \"Age (mean ± SD)\": f\"{age_mean:.2f} ± {age_sd:.2f}\",\n",
    "        \"Age range\": f\"{age_min} – {age_max}\"\n",
    "    }\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dd7afc0e-3c48-4fe9-87ac-eeebcc7a2de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will make sure age is numeric to continue on to make some information tables\n",
    "# ensure age is numeric in the three dataframes we’ll summarize from\n",
    "for _df in (details_unique, stroop_merged, emo_merged):\n",
    "    _df[\"age\"] = pd.to_numeric(_df[\"age\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9eac913b-57f1-4752-ac68-9850e587c914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_demo(df, label):\n",
    "    # one row per participant\n",
    "    cols = [c for c in [\"participant_id\", \"sex\", \"age\"] if c in df.columns]\n",
    "    dfu = df[cols].drop_duplicates(subset=\"participant_id\").copy()\n",
    "\n",
    "    # standardize sex strings a bit\n",
    "    dfu[\"sex\"] = dfu[\"sex\"].astype(str).str.strip().str.lower().replace({\"nan\": np.nan})\n",
    "\n",
    "    # numeric age (already coerced, but safe)\n",
    "    dfu[\"age\"] = pd.to_numeric(dfu[\"age\"], errors=\"coerce\")\n",
    "\n",
    "    # counts\n",
    "    n_total = len(dfu)\n",
    "    sex_counts = dfu[\"sex\"].value_counts(dropna=False).to_dict()\n",
    "\n",
    "    # age stats\n",
    "    age_mean = dfu[\"age\"].mean()\n",
    "    age_sd   = dfu[\"age\"].std()\n",
    "    age_min  = dfu[\"age\"].min()\n",
    "    age_max  = dfu[\"age\"].max()\n",
    "\n",
    "    return {\n",
    "        \"Dataset\": label,\n",
    "        \"N\": n_total,\n",
    "        \"Sex counts\": sex_counts,                 # e.g. {'f': 45, 'm': 43, nan: 0}\n",
    "        \"Age (mean ± SD)\": f\"{age_mean:.2f} ± {age_sd:.2f}\",\n",
    "        \"Age range\": f\"{int(age_min)} – {int(age_max)}\" if pd.notna(age_min) else \"—\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2791566a-10da-4baa-afd6-3814e1931ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Dataset    N                                         Sex counts  \\\n",
      "0  Classical Stroop   88                {'f': 58, 'm': 29, 'non-binary': 1}   \n",
      "1  Emotional Stroop   88                {'f': 58, 'm': 29, 'non-binary': 1}   \n",
      "2  All Participants  350  {'f': 228, 'm': 101, '<na>': 20, 'non-binary': 1}   \n",
      "\n",
      "  Age (mean ± SD) Age range  \n",
      "0   48.25 ± 13.73   25 – 77  \n",
      "1   48.25 ± 13.73   25 – 77  \n",
      "2   47.63 ± 16.43   18 – 82  \n"
     ]
    }
   ],
   "source": [
    "#building the three tables: one for stroop, one for emotional, and one where both are combined\n",
    "stroop_demo   = summarize_demo(stroop_merged,   \"Classical Stroop\")\n",
    "emo_demo      = summarize_demo(emo_merged,      \"Emotional Stroop\")\n",
    "combined_demo = summarize_demo(details_unique,  \"All Participants\")\n",
    "\n",
    "demo_table = pd.DataFrame([stroop_demo, emo_demo, combined_demo])\n",
    "print(demo_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e7a06c8e-cfd0-4eed-abb3-78096ee481bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex\n",
       "f             228\n",
       "m             101\n",
       "<NA>           20\n",
       "non-binary      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets check for the 'non-binary group'. We don't have any non-binary group. Let's fix this wil willing in manually sex for the ones that are in this category.\n",
    "details_unique[\"sex\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cc28963f-e71d-47a4-a70c-5cb9762f17e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets see which participants are tagged this way\n",
    "# 1) Make a normalized helper series (lowercase, trimmed; keep NaN as NaN)\n",
    "sex_norm = (details_unique[\"sex\"]\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .str.lower()\n",
    "            .replace({\"nan\": np.nan}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c036387b-b978-4f6c-962f-73a2d6725158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sex values: ['<na>', 'f', 'm', 'non-binary']\n"
     ]
    }
   ],
   "source": [
    "# 2) See ALL unique values currently present (helps spot typos)\n",
    "print(\"Unique sex values:\", sorted(sex_norm.dropna().unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "46b02e78-5082-4cb0-8701-22316521358b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-standard sex entries:\n",
      "     participant_id         sex   age\n",
      "3             1004        <NA>   NaN\n",
      "9             1010        <NA>   NaN\n",
      "13            1014        <NA>   NaN\n",
      "16            1017        <NA>   NaN\n",
      "43            1044        <NA>   NaN\n",
      "72            1073        <NA>   NaN\n",
      "79            1080        <NA>   NaN\n",
      "102           1103        <NA>  66.0\n",
      "106           1107        <NA>   NaN\n",
      "142           1143        <NA>  62.0\n",
      "215           1216  non-binary  35.0\n",
      "224           1225        <NA>  23.0\n",
      "232           1233        <NA>  34.0\n",
      "237           1238        <NA>  58.0\n",
      "238           1239        <NA>  51.0\n",
      "274           1275        <NA>  33.0\n",
      "287           1288        <NA>  30.0\n",
      "311           1312        <NA>   NaN\n",
      "328           1329        <NA>  70.0\n",
      "337           1338        <NA>   NaN\n",
      "346           1347        <NA>  56.0\n"
     ]
    }
   ],
   "source": [
    "# 3) Find anything NOT in {'f','m'}  (potential typos / unwanted categories)\n",
    "weird_mask = sex_norm.notna() & ~sex_norm.isin({\"f\", \"m\"})\n",
    "weird_rows = details_unique.loc[weird_mask, [\"participant_id\", \"sex\", \"age\"]]\n",
    "print(\"Non-standard sex entries:\\n\", weird_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96820a3d-a5c9-4812-9f85-2be07c79bfd7",
   "metadata": {},
   "source": [
    "So there is age missing, and sex missing. Let's check all the participants in two groups: the ones that are missing sex, and the ones that are missing age. Let's start with the ones missing age. Scan all these and then manually fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d693b92a-b657-43c5-aa93-3e84ac192043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find all participants with missing age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1198ad11-0b07-4caa-9694-22c18e568352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure age is numeric so we can reliably find missing\n",
    "details_unique[\"age\"] = pd.to_numeric(details_unique[\"age\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "40e365ab-ddb1-421f-9c5c-9d6a889b3a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participants with missing age: 22\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1010</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1014</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1017</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1041</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1044</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1048</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1054</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1060</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1065</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1068</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1072</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1073</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1079</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1080</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1082</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1086</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1093</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1094</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1107</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>1312</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>1338</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    participant_id   sex  age\n",
       "3             1004  <NA>  NaN\n",
       "9             1010  <NA>  NaN\n",
       "13            1014  <NA>  NaN\n",
       "16            1017  <NA>  NaN\n",
       "40            1041     f  NaN\n",
       "43            1044  <NA>  NaN\n",
       "47            1048     m  NaN\n",
       "53            1054     f  NaN\n",
       "59            1060     f  NaN\n",
       "64            1065     m  NaN\n",
       "67            1068     m  NaN\n",
       "71            1072     m  NaN\n",
       "72            1073  <NA>  NaN\n",
       "78            1079     m  NaN\n",
       "79            1080  <NA>  NaN\n",
       "81            1082     m  NaN\n",
       "85            1086     f  NaN\n",
       "92            1093     f  NaN\n",
       "93            1094     m  NaN\n",
       "106           1107  <NA>  NaN\n",
       "311           1312  <NA>  NaN\n",
       "337           1338  <NA>  NaN"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list all with missing age\n",
    "missing_age = (details_unique\n",
    "               .loc[details_unique[\"age\"].isna(), [\"participant_id\", \"sex\", \"age\"]]\n",
    "               .sort_values(\"participant_id\"))\n",
    "print(f\"Participants with missing age: {len(missing_age)}\")\n",
    "missing_age.head(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fb9d3f-99ae-4280-971a-8540d09823a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checked the following participants age, and the following did not show up:\n",
    "    # \"1004\": no show,\n",
    "    # \"1010\": no show,\n",
    "    # \"1014\": no show,\n",
    "    # \"1017\": no show\n",
    "    # \"1041\": no show\n",
    "    # \"1044: no show, \n",
    "    # \"1048\": spurte alder\n",
    "    # \"1054\": no show\n",
    "    # \"1060\": excluded due to head trauma\n",
    "    # \"1065\": no show\n",
    "    # \"1068\": no show\n",
    "    # \"1072\": no show\n",
    "    # \"1073\": no show\n",
    "    # \"1079\": no show\n",
    "    # \"1080\": no show\n",
    "    # \"1082\": no show\n",
    "    # \"1086\": no show\n",
    "    # \"1093\": no show\n",
    "    # \"1094\": no show\n",
    "    # \"1107\": excluded due to hist. with antidepressants and other psychotripic medicines. Suffered a serious fall as well. \n",
    "    # \"1312\": no show\n",
    "    # \"1338\": no show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c56c811c-2e65-4593-a2b8-b6824111f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclusions: participant_id (as strings) -> reason\n",
    "exclusions = {\n",
    "    \"1004\": \"no show\",\n",
    "    \"1010\": \"no show\",\n",
    "    \"1014\": \"no show\",\n",
    "    \"1017\": \"no show\",\n",
    "    \"1041\": \"no show\",\n",
    "    \"1044\": \"no show\",  # <- you missed a quote earlier; fixed here\n",
    "    \"1048\": \"asked age / missing age\",\n",
    "    \"1054\": \"no show\",\n",
    "    \"1060\": \"head trauma\",\n",
    "    \"1065\": \"no show\",\n",
    "    \"1068\": \"no show\",\n",
    "    \"1072\": \"no show\",\n",
    "    \"1073\": \"no show\",\n",
    "    \"1079\": \"no show\",\n",
    "    \"1080\": \"no show\",\n",
    "    \"1082\": \"no show\",\n",
    "    \"1086\": \"no show\",\n",
    "    \"1093\": \"no show\",\n",
    "    \"1094\": \"no show\",\n",
    "    \"1107\": \"psychotropic meds + serious fall\",\n",
    "    \"1312\": \"no show\",\n",
    "    \"1338\": \"no show\",\n",
    "}\n",
    "exclude_ids = set(exclusions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c756aabd-9756-4770-bf4f-0d7f983051e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop these participants from details + raw instrument data\n",
    "# details_unique, stroop_df, emo_df should already exist and have normalized string IDs\n",
    "# Remove from details\n",
    "details_unique = details_unique[~details_unique[\"participant_id\"].isin(exclude_ids)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "50e75584-23f5-4d01-b35c-5509a65102c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also remove from instrument data (in case any slipped in)\n",
    "stroop_df = stroop_df[~stroop_df[\"participant_id\"].isin(exclude_ids)].copy()\n",
    "emo_df    = emo_df[~emo_df[\"participant_id\"].isin(exclude_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5910f747-7693-4b31-aeb9-78c3f446e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-merge demographics to the instruments\n",
    "stroop_merged = stroop_df.merge(details_unique, on=\"participant_id\", how=\"left\", validate=\"m:1\")\n",
    "emo_merged    = emo_df.merge(details_unique,    on=\"participant_id\", how=\"left\", validate=\"m:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a216f2c2-c2b1-4f50-a7da-08166f73ce79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining details participants: 328\n",
      "Unmatched Stroop (missing sex): 0\n",
      "Unmatched Emo (missing sex):    0\n",
      "Excluded IDs still in details_unique: []\n",
      "Excluded IDs still in stroop_df: []\n",
      "Excluded IDs still in emo_df: []\n",
      "Excluded IDs still in stroop_merged: []\n",
      "Excluded IDs still in emo_merged: []\n"
     ]
    }
   ],
   "source": [
    "#checking the data \n",
    "print(\"Remaining details participants:\", details_unique[\"participant_id\"].nunique())\n",
    "print(\"Unmatched Stroop (missing sex):\", stroop_merged[\"sex\"].isna().sum())\n",
    "print(\"Unmatched Emo (missing sex):   \", emo_merged[\"sex\"].isna().sum())\n",
    "\n",
    "# Verify none of the excluded IDs remain anywhere\n",
    "def any_left(df, name):\n",
    "    left = sorted(set(df[\"participant_id\"]) & exclude_ids)\n",
    "    print(f\"Excluded IDs still in {name}: {left}\")\n",
    "\n",
    "any_left(details_unique, \"details_unique\")\n",
    "any_left(stroop_df,     \"stroop_df\")\n",
    "any_left(emo_df,        \"emo_df\")\n",
    "any_left(stroop_merged, \"stroop_merged\")\n",
    "any_left(emo_merged,    \"emo_merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "35bac8a3-8df4-49cd-91a7-c4ec0554da18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Dataset    N                                        Sex counts  \\\n",
      "0  Classical Stroop   88               {'f': 58, 'm': 29, 'non-binary': 1}   \n",
      "1  Emotional Stroop   88               {'f': 58, 'm': 29, 'non-binary': 1}   \n",
      "2  All Participants  328  {'f': 223, 'm': 94, '<na>': 10, 'non-binary': 1}   \n",
      "\n",
      "  Age (mean ± SD) Age range  \n",
      "0   48.25 ± 13.73   25 – 77  \n",
      "1   48.25 ± 13.73   25 – 77  \n",
      "2   47.63 ± 16.43   18 – 82  \n"
     ]
    }
   ],
   "source": [
    "# re-build the demographic table\n",
    "# ensure numeric age\n",
    "for _df in (details_unique, stroop_merged, emo_merged):\n",
    "    _df[\"age\"] = pd.to_numeric(_df[\"age\"], errors=\"coerce\")\n",
    "\n",
    "stroop_demo   = summarize_demo(stroop_merged,   \"Classical Stroop\")\n",
    "emo_demo      = summarize_demo(emo_merged,      \"Emotional Stroop\")\n",
    "combined_demo = summarize_demo(details_unique,  \"All Participants\")\n",
    "\n",
    "import pandas as pd\n",
    "demo_table = pd.DataFrame([stroop_demo, emo_demo, combined_demo])\n",
    "print(demo_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3d69042b-e43c-4cd9-89f4-6abb9cc4dc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Got answer from the participant 1048, age 65 years old. SO now I have to re-merge the data and update the participants age. \n",
    "details_unique.loc[details_unique[\"participant_id\"] == \"1048\", \"age\"] = 65\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9aacdcdb-176d-471c-9ba9-ad9dc20334b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-emerge so stroop and emotional stroop have the same data update of the participant\n",
    "stroop_merged = stroop_df.merge(details_unique, on=\"participant_id\", how=\"left\", validate=\"m:1\")\n",
    "emo_merged    = emo_df.merge(details_unique,    on=\"participant_id\", how=\"left\", validate=\"m:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428397d2-c617-4769-bd18-6b347b27f08a",
   "metadata": {},
   "source": [
    "update the raw file with the new participants update age. BUT first! find the missing sex participants and try to fill in sex before touching the raw datafile and merging everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dfa5221f-5f49-482b-9560-6a3e3cc76f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participants with missing sex: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1103</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>1143</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>1225</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>1233</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>1238</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>1239</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>1275</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>1288</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>1329</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>1347</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    participant_id   sex   age\n",
       "102           1103  <NA>  66.0\n",
       "142           1143  <NA>  62.0\n",
       "224           1225  <NA>  23.0\n",
       "232           1233  <NA>  34.0\n",
       "237           1238  <NA>  58.0\n",
       "238           1239  <NA>  51.0\n",
       "274           1275  <NA>  33.0\n",
       "287           1288  <NA>  30.0\n",
       "328           1329  <NA>  70.0\n",
       "346           1347  <NA>  56.0"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find all missing sex-participants\n",
    "# normalize sex (so \"<na>\" and weird spellings don't slip through)\n",
    "sex_norm = (details_unique[\"sex\"].astype(str).str.strip().str.lower()\n",
    "            .replace({\"nan\": pd.NA, \"<na>\": pd.NA, \"female\":\"f\", \"male\":\"m\"}))\n",
    "\n",
    "missing_sex = details_unique.loc[sex_norm.isna(), [\"participant_id\",\"sex\",\"age\"]].sort_values(\"participant_id\")\n",
    "print(f\"Participants with missing sex: {len(missing_sex)}\")\n",
    "missing_sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "821ae266-38f8-4973-9625-e625f769f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sex information of participants, manually checking:\n",
    "    # \"1103\": female,\n",
    "    # \"1143\": no show,\n",
    "    # \"1225\": excluded because of age 23,\n",
    "    # \"1233\": female, coded as 1223,\n",
    "    # \"1238\": no show,\n",
    "    # \"1239\": 51, but tested 2 years ago so not included in this sample,\n",
    "    # \"1275\": male,\n",
    "    # \"1247\": female, is 1192 so we can substitue these two,\n",
    "    # \"1288\": no show,\n",
    "    # \"1329\": male,\n",
    "    # \"1347\": female,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4fafde8c-1792-4183-9b97-4c85fc597e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- apply sex corrections ---\n",
    "sex_corrections = {\n",
    "    \"1103\": \"f\",\n",
    "    \"1233\": \"f\",  \n",
    "    \"1275\": \"m\",\n",
    "    \"1329\": \"m\",\n",
    "    \"1347\": \"f\",\n",
    "    \"1275\": \"m\",\n",
    "    \"1247\": \"f\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "316333e3-6ced-438f-9c0b-eca52644676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid, sex_val in sex_corrections.items():\n",
    "    details_unique.loc[details_unique[\"participant_id\"] == pid, \"sex\"] = sex_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "cc1137ae-202e-4321-9a30-3f815c243b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize sex to f/m/NaN\n",
    "details_unique[\"sex\"] = (details_unique[\"sex\"].astype(str).str.strip().str.lower()\n",
    "                         .replace({\"female\":\"f\", \"male\":\"m\", \"nan\": pd.NA, \"<na>\": pd.NA}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f2b842c4-2c25-41ed-8912-3355c0129dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- exclusions (do NOT exclude 1247) ---\n",
    "extra_exclusions = {\"1143\", \"1225\", \"1238\", \"1239\", \"1288\"}\n",
    "details_unique = details_unique[~details_unique[\"participant_id\"].isin(extra_exclusions)].copy()\n",
    "stroop_df      = stroop_df[~stroop_df[\"participant_id\"].isin(extra_exclusions)].copy()\n",
    "emo_df         = emo_df[~emo_df[\"participant_id\"].isin(extra_exclusions)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "49dc5c6c-adc9-48b9-a9ae-9233222019f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-emerge demographocs to stroop and emo\n",
    "stroop_merged = stroop_df.merge(details_unique, on=\"participant_id\", how=\"left\", validate=\"m:1\")\n",
    "emo_merged    = emo_df.merge(details_unique,    on=\"participant_id\", how=\"left\", validate=\"m:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c039a8fe-6ddf-4ea4-bd4d-1fb40f211a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Dataset    N                            Sex counts  \\\n",
      "0  Classical Stroop   88   {'f': 58, 'm': 29, 'non-binary': 1}   \n",
      "1  Emotional Stroop   88   {'f': 58, 'm': 29, 'non-binary': 1}   \n",
      "2  All Participants  323  {'f': 226, 'm': 96, 'non-binary': 1}   \n",
      "\n",
      "  Age (mean ± SD) Age range  \n",
      "0   48.25 ± 13.73   25 – 77  \n",
      "1   48.25 ± 13.73   25 – 77  \n",
      "2   47.67 ± 16.44   18 – 82  \n"
     ]
    }
   ],
   "source": [
    "#re-build the three summaries again (emo, stroop, rawfile\n",
    "# ensure numeric age\n",
    "for _df in (details_unique, stroop_merged, emo_merged):\n",
    "    _df[\"age\"] = pd.to_numeric(_df[\"age\"], errors=\"coerce\")\n",
    "\n",
    "def summarize_demo(df, label):\n",
    "    cols = [c for c in [\"participant_id\",\"sex\",\"age\"] if c in df.columns]\n",
    "    dfu = df[cols].drop_duplicates(subset=\"participant_id\").copy()\n",
    "    dfu[\"sex\"] = dfu[\"sex\"].astype(str).str.strip().str.lower().replace({\"nan\": pd.NA})\n",
    "    n = len(dfu)\n",
    "    sex_counts = dfu[\"sex\"].value_counts(dropna=False).to_dict()\n",
    "    age_mean = dfu[\"age\"].mean(); age_sd = dfu[\"age\"].std()\n",
    "    age_min  = dfu[\"age\"].min();  age_max = dfu[\"age\"].max()\n",
    "    return {\n",
    "        \"Dataset\": label,\n",
    "        \"N\": n,\n",
    "        \"Sex counts\": sex_counts,\n",
    "        \"Age (mean ± SD)\": f\"{age_mean:.2f} ± {age_sd:.2f}\",\n",
    "        \"Age range\": f\"{int(age_min)} – {int(age_max)}\" if pd.notna(age_min) else \"—\",\n",
    "    }\n",
    "\n",
    "stroop_demo   = summarize_demo(stroop_merged,   \"Classical Stroop\")\n",
    "emo_demo      = summarize_demo(emo_merged,      \"Emotional Stroop\")\n",
    "combined_demo = summarize_demo(details_unique,  \"All Participants\")\n",
    "\n",
    "demo_table = pd.DataFrame([stroop_demo, emo_demo, combined_demo])\n",
    "print(demo_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "761a3080-8165-4451-b25c-8c7c56dbd1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing sex (details): 0\n",
      "Unmatched Stroop (sex NaN): 0\n",
      "Unmatched Emo (sex NaN):    0\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing sex (details):\", details_unique[\"sex\"].isna().sum())\n",
    "print(\"Unmatched Stroop (sex NaN):\", stroop_merged[\"sex\"].isna().sum())\n",
    "print(\"Unmatched Emo (sex NaN):   \", emo_merged[\"sex\"].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bb0d9b-0957-48a8-b562-d07b7b59bd80",
   "metadata": {},
   "source": [
    "finding a non-binary participant. Which is correct according to the dataset a participant did selflabel as binary. But we will have to exclude this participant due to missing number on this specific group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6d43701d-7b80-4c25-8c21-7d39240e6144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>1216</td>\n",
       "      <td>non-binary</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    participant_id         sex   age\n",
       "215           1216  non-binary  35.0"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#locate the non-binary\n",
    "mask_nb = details_unique[\"sex\"].astype(str).str.strip().str.lower() == \"non-binary\"\n",
    "details_unique.loc[mask_nb, [\"participant_id\",\"sex\",\"age\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da19736b-a361-4ddb-89f7-7a3e40b23db4",
   "metadata": {},
   "source": [
    "This information is correct with our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2f3eb562-309e-4f14-bf96-74a1577b1098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# participant to exclude\n",
    "exclude_nonbinary = [\"1216\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "6430c58a-3ad3-4e9c-bd16-170b85b4c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# remove from cleaned demographics\n",
    "details_unique = details_unique[~details_unique[\"participant_id\"].isin(exclude_nonbinary)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ded4798e-1acb-4bb2-8686-75d6799cc227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove from raw (if you want consistency)\n",
    "if \"participant_id\" not in details_df.columns:\n",
    "    def normalize_id(s):\n",
    "        return (s.astype(str).str.replace(r\"\\.0$\",\"\",regex=True).str.strip())\n",
    "    details_df[\"participant_id\"] = normalize_id(details_df[\"participant_ID\"])\n",
    "\n",
    "details_df = details_df[~details_df[\"participant_id\"].isin(exclude_nonbinary)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "815a3349-76a4-4e7d-9330-d718c952c84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove from instrument data\n",
    "stroop_df = stroop_df[~stroop_df[\"participant_id\"].isin(exclude_nonbinary)].copy()\n",
    "emo_df    = emo_df[~emo_df[\"participant_id\"].isin(exclude_nonbinary)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "06d48771-50ff-42d3-9c26-92024028d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroop_merged = stroop_df.merge(details_unique, on=\"participant_id\", how=\"left\", validate=\"m:1\")\n",
    "emo_merged    = emo_df.merge(details_unique,    on=\"participant_id\", how=\"left\", validate=\"m:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "dab12fe7-fb25-4b4e-ad4e-9096ff76fb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participants left in details: 322\n",
      "Any 1216 left in details? False\n",
      "Any 1216 left in Stroop? False\n",
      "Any 1216 left in Emo? False\n"
     ]
    }
   ],
   "source": [
    "print(\"Participants left in details:\", details_unique[\"participant_id\"].nunique())\n",
    "print(\"Any 1216 left in details?\", \"1216\" in details_unique[\"participant_id\"].values)\n",
    "print(\"Any 1216 left in Stroop?\", \"1216\" in stroop_df[\"participant_id\"].values)\n",
    "print(\"Any 1216 left in Emo?\", \"1216\" in emo_df[\"participant_id\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a3c24-ba12-4c5f-a70f-9890a13f9443",
   "metadata": {},
   "source": [
    "Those lines are the checks we wrote to confirm that participant 1216 was successfully excluded. \n",
    "Any 1216 left in details? \n",
    "\n",
    "False → means 1216 is not present in details_unique.\n",
    "\n",
    "Any 1216 left in Stroop? False → means 1216 is not present in your Stroop dataset.\n",
    "\n",
    "Any 1216 left in Emo? False → means 1216 is not present in your Emotional Stroop dataset.\n",
    "\n",
    "So all three False values are good news: they confirm that participant 1216 was fully removed from every dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "fa584bb4-d6f5-48fd-9d49-9e79013e5c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Ensure age is numeric and sex normalized (f/m/NaN only) ---\n",
    "for _df in (details_unique, stroop_merged, emo_merged):\n",
    "    _df[\"age\"] = pd.to_numeric(_df[\"age\"], errors=\"coerce\")\n",
    "    _df[\"sex\"] = (_df[\"sex\"].astype(str).str.strip().str.lower()\n",
    "                  .replace({\"female\": \"f\", \"male\": \"m\", \"nan\": np.nan, \"<na>\": np.nan}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a170c61b-eee0-445d-af1f-43fe8d65c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) Helper to summarize demographics ---\n",
    "def summarize_demo(df, label):\n",
    "    cols = [c for c in [\"participant_id\",\"sex\",\"age\"] if c in df.columns]\n",
    "    dfu = df[cols].drop_duplicates(subset=\"participant_id\").copy()\n",
    "\n",
    "    n = len(dfu)\n",
    "    sex_counts = dfu[\"sex\"].value_counts(dropna=False).to_dict()\n",
    "    age_mean = dfu[\"age\"].mean(); age_sd = dfu[\"age\"].std()\n",
    "    age_min  = dfu[\"age\"].min();  age_max = dfu[\"age\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "37711f1f-60ca-4b77-9173-9d9b249241e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_demo(df, label):\n",
    "    cols = [c for c in [\"participant_id\", \"sex\", \"age\"] if c in df.columns]\n",
    "    dfu = df[cols].drop_duplicates(subset=\"participant_id\").copy()\n",
    "\n",
    "    n = len(dfu)\n",
    "    sex_counts = dfu[\"sex\"].value_counts(dropna=False).to_dict()\n",
    "    age_mean = dfu[\"age\"].mean()\n",
    "    age_sd   = dfu[\"age\"].std()\n",
    "    age_min  = dfu[\"age\"].min()\n",
    "    age_max  = dfu[\"age\"].max()\n",
    "\n",
    "    # Pretty-print sex counts: collapse NaN label to '<na>' if present\n",
    "    if any(pd.isna(k) for k in sex_counts.keys()):\n",
    "        sex_counts = {\n",
    "            (\"f\" if k == \"f\" else \"m\" if k == \"m\" else \"<na>\"): v\n",
    "            for k, v in sex_counts.items()\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"Dataset\": label,\n",
    "        \"N\": n,\n",
    "        \"Sex counts\": sex_counts,\n",
    "        \"Age (mean ± SD)\": f\"{age_mean:.2f} ± {age_sd:.2f}\" if n else \"—\",\n",
    "        \"Age range\": f\"{int(age_min)} – {int(age_max)}\" if pd.notna(age_min) else \"—\",\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "18a59e88-45e1-4e75-a85f-e71aae2df207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Dataset    N           Sex counts Age (mean ± SD) Age range\n",
      "0  Classical Stroop   87   {'f': 58, 'm': 29}   48.40 ± 13.74   25 – 77\n",
      "1  Emotional Stroop   87   {'f': 58, 'm': 29}   48.40 ± 13.74   25 – 77\n",
      "2  All Participants  322  {'f': 226, 'm': 96}   47.71 ± 16.45   18 – 82\n"
     ]
    }
   ],
   "source": [
    "# --- 3) Build summaries ---\n",
    "stroop_demo   = summarize_demo(stroop_merged,   \"Classical Stroop\")\n",
    "emo_demo      = summarize_demo(emo_merged,      \"Emotional Stroop\")\n",
    "combined_demo = summarize_demo(details_unique,  \"All Participants\")\n",
    "\n",
    "demo_table = pd.DataFrame([stroop_demo, emo_demo, combined_demo])\n",
    "print(demo_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369fbb56-fcfa-4bb9-a08c-2beab4a02681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c1fa59-fd62-4552-b878-042714bea412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
